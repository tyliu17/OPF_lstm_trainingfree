{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JKLovDeXoCCP"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import math\n",
    "from datetime import datetime\n",
    "root=''\n",
    "# try:\n",
    "#   from google.colab import drive\n",
    "#   drive.mount('/content/drive')\n",
    "#   root='./drive/MyDrive/gnn/data/'\n",
    "# except:\n",
    "#   pass\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# print(torch.cuda.get_device_name(0))\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Hj9_eLfoWQY3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([186, 118]) torch.Size([186, 1]) tensor(72)\n",
      "1654.8 -332.4 949.67 0.94\n",
      "voltage range(scaled): 3.9999999999999925 16.000000000000004\n",
      "price range old: 11.547 949.67\n",
      "voltage range old: 3.9999999999999925 16.000000000000004\n",
      "price range new: 11.547 949.67\n",
      "voltage range new: 3.9999999999999925 16.000000000000004\n",
      "(118, 6, 10000) (118, 2, 10000)\n"
     ]
    }
   ],
   "source": [
    "root = 'C:/Users/USER/Desktop/GNN_OPF_electricity_market-main/GNN_OPF_electricity_market-main/GNN_OPF_electricity_market-main/data_gen/118_data/'\n",
    "# C:\\Users\\USER\\Desktop\\GNN_OPF_electricity_market-main\\GNN_OPF_electricity_market-main\\GNN_OPF_electricity_market-main\\data_gen\\118_data\\118dc_quad_ISF.txt\n",
    "filename = root + '118dc_quad_ISF.txt'\n",
    "\n",
    "S_isf=pd.read_table(filename,sep=',',header=None).to_numpy() # ISF matrix\n",
    "filename=root+'118ac_fmax.txt'\n",
    "f_max=pd.read_table(filename,sep=',',header=None).to_numpy() # flow limit\n",
    "n_line = np.size(S_isf,0)\n",
    "S = torch.from_numpy(S_isf).to(device) # ISF\n",
    "f_max = torch.from_numpy(f_max).to(device) # flow limit\n",
    "print(S.shape,f_max.shape,torch.min(f_max))\n",
    "\n",
    "x=np.load(root+'ac118_p10_x_v.npy')\n",
    "y=np.load(root+'ac118_p10_y_v.npy')\n",
    "W=np.load(root+'ac118_p10_w.npy')\n",
    "print(np.max(x),np.min(x),np.max(y),np.min(y))\n",
    "\n",
    "# scaling on voltage\n",
    "vy_deviation = 0.9\n",
    "vy_scale = 100\n",
    "y[:,1,:] = (y[:,1,:] - vy_deviation) * vy_scale\n",
    "print('voltage range(scaled):',np.min(y[:,1,:]),np.max(y[:,1,:]))\n",
    "\n",
    "# scaling on price\n",
    "pi_deviation = 0\n",
    "y[:,0,:] = y[:,0,:] + pi_deviation\n",
    "# filter out extreme points in price\n",
    "y_sort_arg = np.argsort(np.amax(np.abs(y[:,0,:]),axis=0)) # max extreme\n",
    "y_sort_arg1 = np.argsort(np.amin(y[:,0,:],axis=0),axis=0) # min extreme\n",
    "\n",
    "del_idx0 = []\n",
    "del_num = 0\n",
    "for i in range(del_num):\n",
    "  del_idx0.append(y_sort_arg[-i])\n",
    "  del_idx0.append(y_sort_arg1[i])\n",
    "# print(del_idx0)\n",
    "del_idx = [] # keep only non-repeated\n",
    "[del_idx.append(x) for x in del_idx0 if x not in del_idx]\n",
    "del_idx = np.sort(del_idx)\n",
    "# delete extreme points\n",
    "print('price range old:',np.min(y[:,0,:]),np.max(y[:,0,:]))\n",
    "print('voltage range old:',np.min(y[:,1,:]),np.max(y[:,1,:]))\n",
    "# x = np.delete(x, del_idx, axis=2)\n",
    "# y = np.delete(y, del_idx, axis=2)\n",
    "print('price range new:',np.min(y[:,0,:]),np.max(y[:,0,:]))\n",
    "print('voltage range new:',np.min(y[:,1,:]),np.max(y[:,1,:]))\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "YcR42RBbdZqZ"
   },
   "outputs": [],
   "source": [
    "n_sample=y.shape[-1]\n",
    "n_bus=y.shape[0]\n",
    "x_total=x.transpose((1,0,2)).reshape(-1,x.shape[-1])\n",
    "y_total=y.transpose((1,0,2)).reshape(-1,y.shape[-1])\n",
    "x_train,x_test,y_train,y_test=train_test_split(x_total.T,y_total.T,test_size=0.2)\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        self.x=torch.from_numpy(x).float()\n",
    "        self.y=torch.from_numpy(y).float()\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "          idx=idx.tolist()\n",
    "        return self.x[idx],self.y[idx]\n",
    "params={'batch_size': 512,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 0}\n",
    "train=Dataset(x_train,y_train)\n",
    "train_set=torch.utils.data.DataLoader(train,**params)\n",
    "val=Dataset(x_test,y_test)\n",
    "val_set=torch.utils.data.DataLoader(val,**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "JN9gN9BnCNim"
   },
   "outputs": [],
   "source": [
    "# fig2 = plt.figure(figsize=(8,4))\n",
    "# flat_list = list(np.concatenate(y[:,n_sample:]).flat)\n",
    "# flat_list3 = list(np.concatenate(y[:,:n_sample]).flat)\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.hist(flat_list,bins = 100,label = 'voltage')\n",
    "# plt.subplot(1,2,2)\n",
    "# # plt.hist(flat_list3,range=[-2000, 2000],bins = 100,label = 'price')\n",
    "# plt.hist(flat_list3,bins = 100,label = 'price')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "iOyWpG_4yCtz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params: 3909576\n"
     ]
    }
   ],
   "source": [
    "class dnn(torch.nn.Module):\n",
    "  def __init__(self,shape):\n",
    "    super(dnn,self).__init__()\n",
    "    layers=[]\n",
    "    for idx in range(len(shape)-2):\n",
    "      layers.extend([\n",
    "        nn.Linear(shape[idx],shape[idx+1]),\n",
    "        nn.BatchNorm1d(shape[idx+1]),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "      ])\n",
    "    layers+=[nn.Linear(shape[-2],shape[-1])]\n",
    "    self.features=nn.Sequential(*layers)\n",
    "    for temp in self.features:\n",
    "      if type(temp)==nn.Linear:\n",
    "        torch.nn.init.normal_(temp.weight,mean=0,std=1)\n",
    "  def forward(self,x): return self.features(x)\n",
    "net=dnn([n_bus*6,n_bus*10,n_bus*10,n_bus*10,n_bus*2]).to(device)\n",
    "print('number of params: %d'%(sum(temp.numel() for temp in net.parameters() if temp.requires_grad)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GEQ-MDKOTqW1"
   },
   "outputs": [],
   "source": [
    "# threshold function for p_g\n",
    "class my_gen_pred_binary(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(my_gen_pred_binary,self).__init__()\n",
    "  def forward(self,x,thresh):\n",
    "    right_thresh=thresh.clone().detach().requires_grad_(True).double()\n",
    "    left_thresh=torch.tensor(0).double()\n",
    "    x=x.double()\n",
    "    output = torch.sigmoid(left_thresh - x)\n",
    "    output = torch.mul(output,left_thresh - x) + x\n",
    "    output = torch.sigmoid(output - right_thresh)\n",
    "    output = torch.mul(output,output - right_thresh) + right_thresh\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9mYCMszHaosA"
   },
   "outputs": [],
   "source": [
    "## params needed for S calculation\n",
    "# line parameters\n",
    "filename1 = root+'ieee118_lineloc.txt'\n",
    "filename2 = root+'ieee118_lineparams.txt'\n",
    "filename3 = root+'ieee118_Bmat.txt'\n",
    "# incidence info\n",
    "line_loc = pd.read_table(filename1,sep=',',header=None).to_numpy()\n",
    "# r, x, shunt, S_max\n",
    "line_params = pd.read_table(filename2,sep=',',header=None).to_numpy()\n",
    "B_mat=pd.read_table(filename3,sep=',',header=None).to_numpy()\n",
    "B_r = np.delete(B_mat,68,axis=0)\n",
    "B_r = np.delete(B_r,68,axis=1)\n",
    "Br_inv = np.linalg.inv(B_r)\n",
    "\n",
    "R_line = line_params[:,0].copy()\n",
    "X_line = line_params[:,1].copy()\n",
    "\n",
    "B_shunt = line_params[:,2].copy()\n",
    "\n",
    "Z_line = R_line + 1j * X_line \n",
    "Y_line = 1 / Z_line\n",
    "\n",
    "G_line = np.real(Y_line)\n",
    "B_line = np.imag(Y_line)\n",
    "\n",
    "# transformer indicator\n",
    "a = (R_line > 0).astype(int)\n",
    "\n",
    "# params to tensor and GPU\n",
    "G_line_tensor = torch.from_numpy(G_line).to(device) # conductance\n",
    "B_line_tensor = torch.from_numpy(B_line).to(device) # susceptance\n",
    "B_shunt_tensor = torch.from_numpy(B_shunt/2).to(device) # conductance\n",
    "Br_inv_tensor = torch.from_numpy(Br_inv).to(device) # reduced Bbus matrix\n",
    "a_tensor = torch.from_numpy(a).double().to(device) # line/transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Go4bwoEmoi0D"
   },
   "outputs": [],
   "source": [
    "class loss_func:\n",
    "    def __init__(self,s_max,G_line,B_line,B_shunt,Br_inv,a,line_loc):\n",
    "      self.s=s_max\n",
    "      self.g=G_line\n",
    "      self.b=B_line\n",
    "      self.c=B_shunt\n",
    "      self.r=Br_inv\n",
    "      self.a=a\n",
    "      self.mse=nn.MSELoss() # MSE loss\n",
    "      self.lmda1=torch.tensor(10).to(device) # V MSE \n",
    "      self.lmda2=torch.tensor(1).to(device) # pi MSE \n",
    "      self.lmda3=torch.tensor(0.1).to(device) # v l_inf\n",
    "      self.lmda4=torch.tensor(0.1).to(device) # s feasibility\n",
    "      self.lmda5=torch.tensor(0.01).to(device) # pi l_inf\n",
    "      self.line_loc=line_loc\n",
    "      self.binary_cell=my_gen_pred_binary()\n",
    "    def calc(self,pred,label,x,feas):\n",
    "      mse_p=self.mse(pred[:,:n_bus],label[:,:n_bus])\n",
    "      mse_v=self.mse(pred[:,n_bus:],label[:,n_bus:])\n",
    "      linf_p=(pred[:,:n_bus]-label[:,:n_bus]).norm(p=float('inf'))\n",
    "      linf_v=(pred[:,n_bus:]-label[:,n_bus:]).norm(p=float('inf'))\n",
    "      if feas==False:\n",
    "        return self.lmda1*mse_v+self.lmda2*mse_p+self.lmda3*linf_v+self.lmda5*linf_p\n",
    "      label_pred=pred[:,:n_bus]\n",
    "      p_max=x[:,:n_bus*1]-x[:,n_bus*1:n_bus*2]\n",
    "      quadratic_b=x[:,n_bus*4:n_bus*5]\n",
    "      quadratic_a=x[:,n_bus*5:n_bus*6]\n",
    "      quadratic_center=(label_pred-quadratic_b)/(quadratic_a+1e-5)/2\n",
    "      p_inj=self.binary_cell(quadratic_center,p_max)\n",
    "      bus_inj=p_inj+x[:,n_bus:n_bus*2]\n",
    "      p_inj_r=torch.cat((bus_inj[:,:68],bus_inj[:,69:]),1)/100\n",
    "      theta0=torch.matmul(self.r,p_inj_r.T)\n",
    "      ref_ang=torch.zeros(1,theta0.shape[1]).to(device)\n",
    "      theta=torch.cat([theta0[:68,:],ref_ang,theta0[68:,:]],0)\n",
    "      v_pred=(pred[:,n_bus:].transpose(0,1))*0.01+0.9\n",
    "      \n",
    "      # s penalty\n",
    "      theta1=theta[self.line_loc[:,0]-1,:]\n",
    "      theta2=theta[self.line_loc[:,1]-1,:]\n",
    "      V1=(v_pred[self.line_loc[:,0]-1,:]).double()\n",
    "      V2=(v_pred[self.line_loc[:,1]-1,:]).double()\n",
    "      f_p=(self.a*self.g*(V1*V1).T)-self.a*((V1*V2).T)*(self.g*torch.cos(theta1-theta2).T+self.b*torch.sin(theta1-theta2).T)\n",
    "      f_p=f_p.T\n",
    "      f_q=-self.a*(V1.T)*(self.a*V1.T)*(self.b+self.c/2)+self.a*((V1*V2).T)*(self.b*torch.cos(theta1-theta2).T-self.g*torch.sin(theta1-theta2).T)\n",
    "      f_q=f_q.T\n",
    "      s_pred=torch.sqrt(f_p*f_p+f_q*f_q+1e-5)*100\n",
    "      s_penalty=torch.sigmoid(s_pred-self.s)+torch.sigmoid(-s_pred-self.s)\n",
    "      s_total=torch.sum(s_penalty)\n",
    "\n",
    "      # sji penalty\n",
    "      theta1=theta[self.line_loc[:,1]-1,:]\n",
    "      theta2=theta[self.line_loc[:,0]-1,:]\n",
    "      V1=(v_pred[self.line_loc[:,1]-1,:]).double()\n",
    "      V2=(v_pred[self.line_loc[:,0]-1,:]).double() \n",
    "      fji_p=(self.a*self.g*(V1*V1).T)-self.a*((V1*V2).T)*(self.g*torch.cos(theta1-theta2).T+self.b*torch.sin(theta1-theta2).T)\n",
    "      fji_p=fji_p.T\n",
    "      fji_q=-self.a*(V1.T)*(self.a*V1.T)*(self.b+self.c/2)+self.a*((V1*V2).T)*(self.b*torch.cos(theta1-theta2).T-self.g*torch.sin(theta1-theta2).T)\n",
    "      fji_q=fji_q.T\n",
    "      sji_pred=torch.sqrt(fji_p*fji_p+fji_q*fji_q+1e-5)*100\n",
    "      sji_penalty=torch.sigmoid(sji_pred-self.s)+torch.sigmoid(-sji_pred-self.s)\n",
    "      sji_total=torch.sum(sji_penalty)\n",
    "\n",
    "      return self.lmda1*mse_v+self.lmda2*mse_p+self.lmda3*linf_v+self.lmda5*linf_p+self.lmda4*s_total+self.lmda4*sji_total\n",
    "my_loss=loss_func(f_max,G_line_tensor,B_line_tensor,B_shunt_tensor,Br_inv_tensor,a_tensor,line_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "iu0PKvcgF2vN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cold start\n",
      "Epoch 0 | Training loss: 426.5989\n",
      "Epoch 0 | Testing loss: 409.2309\n",
      "Epoch 1 | Training loss: 409.3180\n",
      "Epoch 2 | Training loss: 393.8501\n",
      "Epoch 3 | Training loss: 379.5366\n",
      "Epoch 4 | Training loss: 365.5437\n",
      "Epoch 5 | Training loss: 351.3892\n",
      "Epoch 5 | Testing loss: 363.6674\n",
      "Epoch 6 | Training loss: 336.8743\n",
      "Epoch 7 | Training loss: 322.2565\n",
      "Epoch 8 | Training loss: 306.9702\n",
      "Epoch 9 | Training loss: 291.4635\n",
      "Epoch 10 | Training loss: 275.5420\n",
      "Epoch 10 | Testing loss: 301.6728\n",
      "Epoch 11 | Training loss: 259.3258\n",
      "Epoch 12 | Training loss: 243.0300\n",
      "Epoch 13 | Training loss: 226.0951\n",
      "Epoch 14 | Training loss: 209.6775\n",
      "Epoch 15 | Training loss: 193.3762\n",
      "Epoch 15 | Testing loss: 217.0584\n",
      "Epoch 16 | Training loss: 177.5415\n",
      "Epoch 17 | Training loss: 162.1342\n",
      "Epoch 18 | Training loss: 147.5277\n",
      "Epoch 19 | Training loss: 133.8492\n",
      "Epoch 20 | Training loss: 121.2226\n",
      "Epoch 20 | Testing loss: 130.4328\n",
      "Epoch 21 | Training loss: 109.4553\n",
      "Epoch 22 | Training loss: 98.4522\n",
      "Epoch 23 | Training loss: 88.9631\n",
      "Epoch 24 | Training loss: 80.4453\n",
      "Epoch 25 | Training loss: 73.0751\n",
      "Epoch 25 | Testing loss: 66.6518\n",
      "Epoch 26 | Training loss: 66.6681\n",
      "Epoch 27 | Training loss: 61.1010\n",
      "Epoch 28 | Training loss: 56.3788\n",
      "Epoch 29 | Training loss: 52.3107\n",
      "Epoch 30 | Training loss: 48.9380\n",
      "Epoch 30 | Testing loss: 33.3516\n",
      "Epoch 31 | Training loss: 46.1591\n",
      "Epoch 32 | Training loss: 43.7105\n",
      "Epoch 33 | Training loss: 41.5553\n",
      "Epoch 34 | Training loss: 39.8093\n",
      "Epoch 35 | Training loss: 38.2650\n",
      "Epoch 35 | Testing loss: 18.1741\n",
      "Epoch 36 | Training loss: 36.7458\n",
      "Epoch 37 | Training loss: 35.3681\n",
      "Epoch 38 | Training loss: 34.2925\n",
      "Epoch 39 | Training loss: 33.2873\n",
      "Epoch 40 | Training loss: 32.2186\n",
      "Epoch 40 | Testing loss: 11.7402\n",
      "Epoch 41 | Training loss: 31.3287\n",
      "Epoch 42 | Training loss: 30.4280\n",
      "Epoch 43 | Training loss: 29.6137\n",
      "Epoch 44 | Training loss: 28.8450\n",
      "Epoch 45 | Training loss: 28.1645\n",
      "Epoch 45 | Testing loss: 8.5550\n",
      "Epoch 46 | Training loss: 27.3996\n",
      "Epoch 47 | Training loss: 26.7177\n",
      "Epoch 48 | Training loss: 26.1239\n",
      "Epoch 49 | Training loss: 25.4708\n",
      "Epoch 50 | Training loss: 24.9380\n",
      "Epoch 50 | Testing loss: 6.9178\n",
      "Epoch 51 | Training loss: 24.3081\n",
      "Epoch 52 | Training loss: 23.7069\n",
      "Epoch 53 | Training loss: 23.1616\n",
      "Epoch 54 | Training loss: 22.6421\n",
      "Epoch 55 | Training loss: 22.0571\n",
      "Epoch 55 | Testing loss: 5.7573\n",
      "Epoch 56 | Training loss: 21.8024\n",
      "Epoch 57 | Training loss: 21.2714\n",
      "Epoch 58 | Training loss: 20.8542\n",
      "Epoch 59 | Training loss: 20.3500\n",
      "Epoch 60 | Training loss: 19.9762\n",
      "Epoch 60 | Testing loss: 4.9846\n",
      "Epoch 61 | Training loss: 19.5053\n",
      "Epoch 62 | Training loss: 19.2244\n",
      "Epoch 63 | Training loss: 18.8370\n",
      "Epoch 64 | Training loss: 18.4650\n",
      "Epoch 65 | Training loss: 18.0523\n",
      "Epoch 65 | Testing loss: 4.4201\n",
      "Epoch 66 | Training loss: 17.7614\n",
      "Epoch 67 | Training loss: 17.4737\n",
      "Epoch 68 | Training loss: 17.0776\n",
      "Epoch 69 | Training loss: 16.8855\n",
      "Epoch 70 | Training loss: 16.5179\n",
      "Epoch 70 | Testing loss: 4.0865\n",
      "Epoch 71 | Training loss: 16.2323\n",
      "Epoch 72 | Training loss: 15.9072\n",
      "Epoch 73 | Training loss: 15.7144\n",
      "Epoch 74 | Training loss: 15.3835\n",
      "Epoch 75 | Training loss: 15.1251\n",
      "Epoch 75 | Testing loss: 3.6019\n",
      "Epoch 76 | Training loss: 14.8850\n",
      "Epoch 77 | Training loss: 14.6886\n",
      "Epoch 78 | Training loss: 14.4216\n",
      "Epoch 79 | Training loss: 14.1456\n",
      "Epoch 80 | Training loss: 13.9364\n",
      "Epoch 80 | Testing loss: 3.3866\n",
      "Epoch 81 | Training loss: 13.7401\n",
      "Epoch 82 | Training loss: 13.5690\n",
      "Epoch 83 | Training loss: 13.3867\n",
      "Epoch 84 | Training loss: 13.1189\n",
      "Epoch 85 | Training loss: 12.9339\n",
      "Epoch 85 | Testing loss: 3.0999\n",
      "Epoch 86 | Training loss: 12.7253\n",
      "Epoch 87 | Training loss: 12.5184\n",
      "Epoch 88 | Training loss: 12.3290\n",
      "Epoch 89 | Training loss: 12.1918\n",
      "Epoch 90 | Training loss: 11.9834\n",
      "Epoch 90 | Testing loss: 2.9230\n",
      "Epoch 91 | Training loss: 11.8282\n",
      "Epoch 92 | Training loss: 11.6778\n",
      "Epoch 93 | Training loss: 11.5212\n",
      "Epoch 94 | Training loss: 11.3094\n",
      "Epoch 95 | Training loss: 11.1915\n",
      "Epoch 95 | Testing loss: 2.6593\n",
      "Epoch 96 | Training loss: 11.0074\n",
      "Epoch 97 | Training loss: 10.9273\n",
      "Epoch 98 | Training loss: 10.6990\n",
      "Epoch 99 | Training loss: 10.5864\n",
      "Epoch 100 | Training loss: 10.3903\n",
      "Epoch 100 | Testing loss: 2.5297\n",
      "Epoch 101 | Training loss: 10.2907\n",
      "Epoch 102 | Training loss: 10.2114\n",
      "Epoch 103 | Training loss: 10.0270\n",
      "Epoch 104 | Training loss: 9.8811\n",
      "Epoch 105 | Training loss: 9.8135\n",
      "Epoch 105 | Testing loss: 2.3670\n",
      "Epoch 106 | Training loss: 9.6437\n",
      "Epoch 107 | Training loss: 9.5344\n",
      "Epoch 108 | Training loss: 9.3964\n",
      "Epoch 109 | Training loss: 9.3461\n",
      "Epoch 110 | Training loss: 9.1892\n",
      "Epoch 110 | Testing loss: 2.2666\n",
      "Epoch 111 | Training loss: 9.0640\n",
      "Epoch 112 | Training loss: 8.9735\n",
      "Epoch 113 | Training loss: 8.8512\n",
      "Epoch 114 | Training loss: 8.7886\n",
      "Epoch 115 | Training loss: 8.6903\n",
      "Epoch 115 | Testing loss: 2.1915\n",
      "Epoch 116 | Training loss: 8.6226\n",
      "Epoch 117 | Training loss: 8.4594\n",
      "Epoch 118 | Training loss: 8.3317\n",
      "Epoch 119 | Training loss: 8.2889\n",
      "Epoch 120 | Training loss: 8.2379\n",
      "Epoch 120 | Testing loss: 2.0499\n",
      "Epoch 121 | Training loss: 8.0926\n",
      "Epoch 122 | Training loss: 8.0119\n",
      "Epoch 123 | Training loss: 7.9701\n",
      "Epoch 124 | Training loss: 7.8199\n",
      "Epoch 125 | Training loss: 7.7962\n",
      "Epoch 125 | Testing loss: 2.0104\n",
      "Epoch 126 | Training loss: 7.7205\n",
      "Epoch 127 | Training loss: 7.5873\n",
      "Epoch 128 | Training loss: 7.5475\n",
      "Epoch 129 | Training loss: 7.4698\n",
      "Epoch 130 | Training loss: 7.3988\n",
      "Epoch 130 | Testing loss: 1.9597\n",
      "Epoch 131 | Training loss: 7.3184\n",
      "Epoch 132 | Training loss: 7.2099\n",
      "Epoch 133 | Training loss: 7.2169\n",
      "Epoch 134 | Training loss: 7.0787\n",
      "Epoch 135 | Training loss: 7.0087\n",
      "Epoch 135 | Testing loss: 1.9487\n",
      "Epoch 136 | Training loss: 7.0051\n",
      "Epoch 137 | Training loss: 6.8666\n",
      "Epoch 138 | Training loss: 6.8268\n",
      "Epoch 139 | Training loss: 6.7382\n",
      "Epoch 140 | Training loss: 6.6745\n",
      "Epoch 140 | Testing loss: 1.8867\n",
      "Epoch 141 | Training loss: 6.6403\n",
      "Epoch 142 | Training loss: 6.5837\n",
      "Epoch 143 | Training loss: 6.5159\n",
      "Epoch 144 | Training loss: 6.4503\n",
      "Epoch 145 | Training loss: 6.4118\n",
      "Epoch 145 | Testing loss: 1.8443\n",
      "Epoch 146 | Training loss: 6.3804\n",
      "Epoch 147 | Training loss: 6.3103\n",
      "Epoch 148 | Training loss: 6.2126\n",
      "Epoch 149 | Training loss: 6.1527\n",
      "Epoch 150 | Training loss: 6.1107\n",
      "Epoch 150 | Testing loss: 1.8044\n",
      "Epoch 151 | Training loss: 6.0564\n",
      "Epoch 152 | Training loss: 5.9594\n",
      "Epoch 153 | Training loss: 5.9709\n",
      "Epoch 154 | Training loss: 5.9319\n",
      "Epoch 155 | Training loss: 5.8404\n",
      "Epoch 155 | Testing loss: 1.7805\n",
      "Epoch 156 | Training loss: 5.8162\n",
      "Epoch 157 | Training loss: 5.8059\n",
      "Epoch 158 | Training loss: 5.7514\n",
      "Epoch 159 | Training loss: 5.6999\n",
      "Epoch 160 | Training loss: 5.6053\n",
      "Epoch 160 | Testing loss: 1.7544\n",
      "Epoch 161 | Training loss: 5.6256\n",
      "Epoch 162 | Training loss: 5.5572\n",
      "Epoch 163 | Training loss: 5.5220\n",
      "Epoch 164 | Training loss: 5.4866\n",
      "Epoch 165 | Training loss: 5.4522\n",
      "Epoch 165 | Testing loss: 1.7311\n",
      "Epoch 166 | Training loss: 5.4219\n",
      "Epoch 167 | Training loss: 5.3136\n",
      "Epoch 168 | Training loss: 5.3373\n",
      "Epoch 169 | Training loss: 5.2383\n",
      "Epoch 170 | Training loss: 5.2674\n",
      "Epoch 170 | Testing loss: 1.7024\n",
      "Epoch 171 | Training loss: 5.2287\n",
      "Epoch 172 | Training loss: 5.1207\n",
      "Epoch 173 | Training loss: 5.1474\n",
      "Epoch 174 | Training loss: 5.0927\n",
      "Epoch 175 | Training loss: 5.0658\n",
      "Epoch 175 | Testing loss: 1.6633\n",
      "Epoch 176 | Training loss: 5.0050\n",
      "Epoch 177 | Training loss: 4.9659\n",
      "Epoch 178 | Training loss: 4.9980\n",
      "Epoch 179 | Training loss: 4.9071\n",
      "Epoch 180 | Training loss: 4.9175\n",
      "Epoch 180 | Testing loss: 1.6462\n",
      "Epoch 181 | Training loss: 4.8399\n",
      "Epoch 182 | Training loss: 4.7806\n",
      "Epoch 183 | Training loss: 4.7996\n",
      "Epoch 184 | Training loss: 4.7163\n",
      "Epoch 185 | Training loss: 4.7246\n",
      "Epoch 185 | Testing loss: 1.6360\n",
      "Epoch 186 | Training loss: 4.7436\n",
      "Epoch 187 | Training loss: 4.6404\n",
      "Epoch 188 | Training loss: 4.6100\n",
      "Epoch 189 | Training loss: 4.6147\n",
      "Epoch 190 | Training loss: 4.5779\n",
      "Epoch 190 | Testing loss: 1.6264\n",
      "Epoch 191 | Training loss: 4.5836\n",
      "Epoch 192 | Training loss: 4.5892\n",
      "Epoch 193 | Training loss: 4.5179\n",
      "Epoch 194 | Training loss: 4.4989\n",
      "Epoch 195 | Training loss: 4.4799\n",
      "Epoch 195 | Testing loss: 1.6016\n",
      "Epoch 196 | Training loss: 4.4137\n",
      "Epoch 197 | Training loss: 4.4310\n",
      "Epoch 198 | Training loss: 4.4065\n",
      "Epoch 199 | Training loss: 4.3434\n",
      "Epoch 200 | Training loss: 4.3359\n",
      "Epoch 200 | Testing loss: 1.6115\n",
      "Epoch 201 | Training loss: 4.3618\n",
      "Epoch 202 | Training loss: 4.2546\n",
      "Epoch 203 | Training loss: 4.2721\n",
      "Epoch 204 | Training loss: 4.2562\n",
      "Epoch 205 | Training loss: 4.2501\n",
      "Epoch 205 | Testing loss: 1.5853\n",
      "Epoch 206 | Training loss: 4.1913\n",
      "Epoch 207 | Training loss: 4.1679\n",
      "Epoch 208 | Training loss: 4.1330\n",
      "Epoch 209 | Training loss: 4.0960\n",
      "Epoch 210 | Training loss: 4.1083\n",
      "Epoch 210 | Testing loss: 1.5744\n",
      "Epoch 211 | Training loss: 4.0836\n",
      "Epoch 212 | Training loss: 4.0876\n",
      "Epoch 213 | Training loss: 4.0663\n",
      "Epoch 214 | Training loss: 3.9707\n",
      "Epoch 215 | Training loss: 4.0162\n",
      "Epoch 215 | Testing loss: 1.5521\n",
      "Epoch 216 | Training loss: 3.9712\n",
      "Epoch 217 | Training loss: 3.9589\n",
      "Epoch 218 | Training loss: 3.9274\n",
      "Epoch 219 | Training loss: 3.9078\n",
      "Epoch 220 | Training loss: 3.9371\n",
      "Epoch 220 | Testing loss: 1.5424\n",
      "Epoch 221 | Training loss: 3.8857\n",
      "Epoch 222 | Training loss: 3.8767\n",
      "Epoch 223 | Training loss: 3.8514\n",
      "Epoch 224 | Training loss: 3.8095\n",
      "Epoch 225 | Training loss: 3.7982\n",
      "Epoch 225 | Testing loss: 1.5288\n",
      "Epoch 226 | Training loss: 3.7977\n",
      "Epoch 227 | Training loss: 3.7670\n",
      "Epoch 228 | Training loss: 3.7615\n",
      "Epoch 229 | Training loss: 3.7536\n",
      "Epoch 230 | Training loss: 3.6941\n",
      "Epoch 230 | Testing loss: 1.5225\n",
      "Epoch 231 | Training loss: 3.7876\n",
      "Epoch 232 | Training loss: 3.7362\n",
      "Epoch 233 | Training loss: 3.6374\n",
      "Epoch 234 | Training loss: 3.6898\n",
      "Epoch 235 | Training loss: 3.6449\n",
      "Epoch 235 | Testing loss: 1.5141\n",
      "Epoch 236 | Training loss: 3.6206\n",
      "Epoch 237 | Training loss: 3.5834\n",
      "Epoch 238 | Training loss: 3.5710\n",
      "Epoch 239 | Training loss: 3.5447\n",
      "Epoch 240 | Training loss: 3.5859\n",
      "Epoch 240 | Testing loss: 1.4982\n",
      "Epoch 241 | Training loss: 3.5534\n",
      "Epoch 242 | Training loss: 3.5233\n",
      "Epoch 243 | Training loss: 3.5309\n",
      "Epoch 244 | Training loss: 3.5238\n",
      "Epoch 245 | Training loss: 3.4580\n",
      "Epoch 245 | Testing loss: 1.5026\n",
      "Epoch 246 | Training loss: 3.4677\n",
      "Epoch 247 | Training loss: 3.4953\n",
      "Epoch 248 | Training loss: 3.4827\n",
      "Epoch 249 | Training loss: 3.3991\n",
      "Epoch 250 | Training loss: 3.3798\n",
      "Epoch 250 | Testing loss: 1.4744\n",
      "Epoch 251 | Training loss: 3.4033\n",
      "Epoch 252 | Training loss: 3.3770\n",
      "Epoch 253 | Training loss: 3.3971\n",
      "Epoch 254 | Training loss: 3.3567\n",
      "Epoch 255 | Training loss: 3.3419\n",
      "Epoch 255 | Testing loss: 1.4579\n",
      "Epoch 256 | Training loss: 3.2946\n",
      "Epoch 257 | Training loss: 3.2794\n",
      "Epoch 258 | Training loss: 3.3135\n",
      "Epoch 259 | Training loss: 3.3454\n",
      "Epoch 260 | Training loss: 3.2774\n",
      "Epoch 260 | Testing loss: 1.4626\n",
      "Epoch 261 | Training loss: 3.2733\n",
      "Epoch 262 | Training loss: 3.2522\n",
      "Epoch 263 | Training loss: 3.2747\n",
      "Epoch 264 | Training loss: 3.2317\n",
      "Epoch 265 | Training loss: 3.1908\n",
      "Epoch 265 | Testing loss: 1.4708\n",
      "Epoch 266 | Training loss: 3.2046\n",
      "Epoch 267 | Training loss: 3.2038\n",
      "Epoch 268 | Training loss: 3.2066\n",
      "Epoch 269 | Training loss: 3.1132\n",
      "Epoch 270 | Training loss: 3.1563\n",
      "Epoch 270 | Testing loss: 1.4510\n",
      "Epoch 271 | Training loss: 3.1343\n",
      "Epoch 272 | Training loss: 3.1374\n",
      "Epoch 273 | Training loss: 3.1472\n",
      "Epoch 274 | Training loss: 3.0932\n",
      "Epoch 275 | Training loss: 3.0883\n",
      "Epoch 275 | Testing loss: 1.4267\n",
      "Epoch 276 | Training loss: 3.0934\n",
      "Epoch 277 | Training loss: 3.1281\n",
      "Epoch 278 | Training loss: 3.0660\n",
      "Epoch 279 | Training loss: 3.0316\n",
      "Epoch 280 | Training loss: 3.0511\n",
      "Epoch 280 | Testing loss: 1.4280\n",
      "Epoch 281 | Training loss: 3.0178\n",
      "Epoch 282 | Training loss: 2.9958\n",
      "Epoch 283 | Training loss: 3.0223\n",
      "Epoch 284 | Training loss: 2.9486\n",
      "Epoch 285 | Training loss: 2.9648\n",
      "Epoch 285 | Testing loss: 1.4374\n",
      "Epoch 286 | Training loss: 3.0093\n",
      "Epoch 287 | Training loss: 2.9370\n",
      "Epoch 288 | Training loss: 2.9384\n",
      "Epoch 289 | Training loss: 2.9498\n",
      "Epoch 290 | Training loss: 2.9308\n",
      "Epoch 290 | Testing loss: 1.4289\n",
      "Epoch 291 | Training loss: 2.9070\n",
      "Epoch 292 | Training loss: 2.9294\n",
      "Epoch 293 | Training loss: 2.9392\n",
      "Epoch 294 | Training loss: 2.8761\n",
      "Epoch 295 | Training loss: 2.8826\n",
      "Epoch 295 | Testing loss: 1.4094\n",
      "Epoch 296 | Training loss: 2.8576\n",
      "Epoch 297 | Training loss: 2.8809\n",
      "Epoch 298 | Training loss: 2.8669\n",
      "Epoch 299 | Training loss: 2.8324\n",
      "Training time:123.2486s\n"
     ]
    }
   ],
   "source": [
    "path=root+'data_118_quad/gnn_trained_ac118.pickle'\n",
    "try: \n",
    "  net.load_state_dict(torch.load(path))\n",
    "  print('params loaded')\n",
    "except: \n",
    "  print('cold start')\n",
    "\n",
    "optimizer=torch.optim.Adam(net.parameters())\n",
    "train_loss=[]\n",
    "val_loss=[]\n",
    "\n",
    "## Training\n",
    "t0=time.time()\n",
    "max_epochs=300\n",
    "eval_epoch=5\n",
    "\n",
    "# earlystopping\n",
    "tolerance=5\n",
    "min_delta=1e-3\n",
    "previous=0\n",
    "\n",
    "# add feasibility \n",
    "feas=False\n",
    "for epoch in range(max_epochs):\n",
    "  # training loop\n",
    "  total_loss=0.0\n",
    "  for local_batch,local_label in train_set:\n",
    "    optimizer.zero_grad() # clear the past gradient\n",
    "    local_batch,local_label=local_batch.to(device),local_label.to(device)\n",
    "    logits=net(local_batch)\n",
    "    loss=my_loss.calc(logits,local_label,local_batch,feas)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    total_loss+=loss.item()\n",
    "  avg_loss=total_loss/len(train_set.dataset)\n",
    "  train_loss.append(avg_loss)\n",
    "  print(\"Epoch %d | Training loss: %.4f\"%(epoch,avg_loss))\n",
    "  # eval\n",
    "  if epoch%eval_epoch==0:\n",
    "    net.eval()\n",
    "    total_loss=0.0\n",
    "    for local_batch,local_label in val_set:\n",
    "      local_batch,local_label=local_batch.to(device),local_label.to(device)\n",
    "      logits=net(local_batch)\n",
    "      loss=my_loss.calc(logits,local_label,local_batch,feas)\n",
    "      total_loss+=loss.item()\n",
    "    avg_loss=total_loss/len(val_set.dataset)\n",
    "    val_loss.append([epoch, avg_loss])\n",
    "    print(\"Epoch %d | Testing loss: %.4f\"%(epoch,avg_loss))\n",
    "    if epoch:\n",
    "      if previous-avg_loss<min_delta: tolerance-=1\n",
    "      if tolerance==0: pass\n",
    "    previous=avg_loss\n",
    "    net.train()\n",
    "t1=time.time()\n",
    "print(\"Training time:%.4fs\"%(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "AvUTphUSzqzA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Desktop\\GNN_OPF_electricity_market-main\\GNN_OPF_electricity_market-main\\GNN_OPF_electricity_market-main\\data_gen\\118_data\\FCNN_model\\dnn_02181135.pickle\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "timestamp=datetime.now().strftime('%m%d%H%M')\n",
    "path = 'C:\\\\Users\\\\USER\\\\Desktop\\\\GNN_OPF_electricity_market-main\\\\GNN_OPF_electricity_market-main\\\\GNN_OPF_electricity_market-main\\\\data_gen\\\\118_data\\\\FCNN_model\\\\'\n",
    "\n",
    "path = path+'dnn_%s.pickle'%(timestamp)\n",
    "if feas==False: path.replace('feas','')\n",
    "print(path)\n",
    "torch.save(net.state_dict(),path)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "L3M4jmjkscK_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_len = len(val_loss)\n",
    "print(val_len)\n",
    "val_plt = np.zeros((2,val_len))\n",
    "for i in range(val_len):\n",
    "  val_plt[0,i] = val_loss[i][0]\n",
    "  val_plt[1,i] = val_loss[i][1]\n",
    "\n",
    "plt.figure()\n",
    "plot_idx = np.arange(np.size(train_loss))\n",
    "plt.plot(plot_idx[5:-1],train_loss[5:-1],lw=2,label='training loss')\n",
    "plt.plot(val_plt[0,1:],val_plt[1,1:],lw=2,label='validation loss')\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "# \n",
    "timestamp=datetime.now().strftime('%m%d%H%M')\n",
    "output_path = r'C:\\Users\\USER\\Desktop\\GNN_OPF_electricity_market-main\\output_loss_fig'\n",
    "output_path = output_path + '/loss_fig_%s.png'%(timestamp)\n",
    "plt.savefig(output_path, dpi=300, bbox_inches='tight')  #  PNG  300 dpi\n",
    "\n",
    "\n",
    "plt.show(block=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9uGUm4rsco3"
   },
   "source": [
    "# Evaluate the model w/ validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "4u6001UQ2pN3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset size: torch.Size([2000, 708])\n",
      "Number of validation set:  2000\n",
      "torch.Size([2000, 236])\n"
     ]
    }
   ],
   "source": [
    "n_test = np.size(x_test,0)\n",
    "x_test_feed = torch.from_numpy(x_test).float()\n",
    "x_test_feed = x_test_feed#.transpose(1,2)\n",
    "x_test_feed = x_test_feed.to(device)\n",
    "print('Validation dataset size:',x_test_feed.shape)\n",
    "print('Number of validation set: ',n_test)\n",
    "y_pred = net(x_test_feed)\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnNbSGYoXS3J"
   },
   "source": [
    "* Visualization\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKZQaqwJkn3P"
   },
   "source": [
    " - Visualize errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "7-JCo0KmXwm3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 236) (2000, 236)\n"
     ]
    }
   ],
   "source": [
    "y_pred1 = y_pred.cpu().detach()\n",
    "y_pred1 = torch.squeeze(y_pred1,1).numpy()#.transpose()\n",
    "print(y_test.shape,y_pred1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "NOeXUzrd9h9y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 2, 2000) (2000, 236)\n"
     ]
    }
   ],
   "source": [
    "# x=np.reshape(x,(x.shape[0]*x.shape[1],x.shape[2])) # reshape by samples not dim1\n",
    "# y=np.reshape(y,(y.shape[0]*y.shape[1],y.shape[2]))\n",
    "# print(x_pre.shape,y_pre.shape)\n",
    "\n",
    "y_pred_temp = y_pred1.copy().transpose()\n",
    "# y_pred2=np.reshape(y_pred2,(y_pre.shape[0],y_pre.shape[1],n_test))\n",
    "y_pred2=np.zeros([y.shape[0],y.shape[1],n_test])\n",
    "y_pred2[:,0,:]=y_pred_temp[:n_bus,:]\n",
    "y_pred2[:,1,:]=y_pred_temp[n_bus:,:]\n",
    "print(y_pred2.shape,y_pred1.shape)\n",
    "\n",
    "y_test_temp = y_test.copy().transpose()\n",
    "# y_pred2=np.reshape(y_pred2,(y_pre.shape[0],y_pre.shape[1],n_test))\n",
    "y_test2=np.zeros([y.shape[0],y.shape[1],n_test])\n",
    "y_test2[:,0,:]=y_test_temp[:n_bus,:]\n",
    "y_test2[:,1,:]=y_test_temp[n_bus:,:]\n",
    "y_pred1 = y_pred2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "2eVE-t3nl0Cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 2, 2000) (118, 2, 2000)\n"
     ]
    }
   ],
   "source": [
    "# recover the original p.u. scale\n",
    "# vy_deviation) * vy_scale\n",
    "y_pred1[:,1,:] = y_pred1[:,1,:] / vy_scale + vy_deviation\n",
    "y_test2[:,1,:] = y_test2[:,1,:] / vy_scale + vy_deviation\n",
    "print(y_test2.shape,y_pred1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "FUVl5LXeknC4"
   },
   "outputs": [],
   "source": [
    "n_test = np.size(y_test2,2)\n",
    "err_L2 = np.zeros(n_test)\n",
    "err_Linf = np.zeros(n_test)\n",
    "for i in range(n_test):\n",
    "  err_L2[i] = np.linalg.norm(y_test2[:,0,i] - y_pred1[:,0,i]) / np.linalg.norm(y_test2[:,0,i])\n",
    "  err_Linf[i] = np.max(np.abs(y_test2[:,0,i] - y_pred1[:,0,i])) / np.max(np.abs(y_test2[:,0,i]))\n",
    "\n",
    "err_L2_v = np.zeros(n_test)\n",
    "err_Linf_v = np.zeros(n_test)\n",
    "for i in range(n_test):\n",
    "  err_L2_v[i] = np.linalg.norm(y_test2[:,1,i] - y_pred1[:,1,i]) / np.linalg.norm(y_test2[:,1,i])\n",
    "  err_Linf_v[i] = np.max(np.abs(y_test2[:,1,i] - y_pred1[:,1,i])) / np.max(np.abs(y_test2[:,1,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "79xFCVXklkLb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price L2 mean: 0.07909630204568156 L_inf mean: 0.12169735309667963\n",
      "Voltage L2 mean: 0.02178199541803404 L_inf mean: 0.06319676368322462\n"
     ]
    }
   ],
   "source": [
    "err_L2_mean = np.mean(err_L2)\n",
    "err_Linf_mean = np.mean(err_Linf)\n",
    "print('Price L2 mean:', err_L2_mean,'L_inf mean:', err_Linf_mean )\n",
    "err_L2_mean_v = np.mean(err_L2_v)\n",
    "err_Linf_mean_v = np.mean(err_Linf_v)\n",
    "print('Voltage L2 mean:', err_L2_mean_v,'L_inf mean:', err_Linf_mean_v )\n",
    "\n",
    "# fig2 = plt.figure(figsize=(16, 16))\n",
    "# plt.subplot(2, 2, 1)\n",
    "# # plt.hist(np.abs(ga),bins = 10)\n",
    "# plt.plot(err_L2,'bo',markersize=0.5,label = 'L2 error')\n",
    "# plt.plot(err_Linf,'r^',markersize=0.5,label = 'Linf error')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('sample index')\n",
    "# plt.ylabel('error')\n",
    "# plt.title('Normalized sample error')\n",
    "# plt.grid(True)\n",
    "# # error histogram\n",
    "# plt.subplot(2, 2, 2)\n",
    "# plt.hist(err_L2, bins = 50, facecolor='b', alpha=0.75,label = 'L2 error')\n",
    "# plt.hist(err_Linf, bins = 50, facecolor='g', alpha=0.75,label = 'Linf error')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('sample error value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('Histogram of L_2 error')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)\n",
    "\n",
    "# plt.subplot(2, 2, 3)\n",
    "# # plt.hist(np.abs(ga),bins = 10)\n",
    "# plt.plot(err_L2_v,'bo',markersize=0.5,label = 'L2 error')\n",
    "# plt.plot(err_Linf_v,'r^',markersize=0.5,label = 'Linf error')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('sample index')\n",
    "# plt.ylabel('error')\n",
    "# plt.title('Normalized sample error')\n",
    "# plt.grid(True)\n",
    "# # error histogram\n",
    "# plt.subplot(2, 2, 4)\n",
    "# plt.hist(err_L2_v, bins = 50, facecolor='b', alpha=0.75,label = 'L2 error')\n",
    "# plt.hist(err_Linf_v, bins = 50, facecolor='g', alpha=0.75,label = 'Linf error')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('sample error value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('Histogram of L_2 error')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "6sUddh-uGg_p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 2000) (118, 2000)\n",
      "true range: 1.06 0.94\n",
      "predicted range 1.163633041381836 0.8564827013015748\n"
     ]
    }
   ],
   "source": [
    "print(y_pred1[:,1,:n_test].shape,y_test2[:,1,:n_test].shape)\n",
    "print('true range:',np.max(y_test2[:,1,:n_test]),np.min(y_test2[:,1,:n_test]))\n",
    "print('predicted range',np.max(y_pred1[:,1,:n_test]),np.min(y_pred1[:,1,:n_test]))\n",
    "\n",
    "# fig3 = plt.figure(figsize=(16, 8))\n",
    "# flat_list1 = list(np.concatenate(y_test2[:,1,:n_test]).flat)\n",
    "# flat_list2 = list(np.concatenate(y_pred1[:,1,:n_test]).flat)\n",
    "# plt.hist(flat_list1,bins = 100,label = 'true')\n",
    "\n",
    "# plt.hist(flat_list2,bins = 100,label = 'pred')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "jhfImaPsjKYq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 6, 10000) 10000\n"
     ]
    }
   ],
   "source": [
    "print(x.shape,n_sample)\n",
    "\n",
    "x_new = np.zeros([x.shape[0],x.shape[1],n_sample])\n",
    "for i in range(x.shape[1]):\n",
    "  x_new[:,i,:] = x_total[n_bus*i:n_bus*(i+1),:]\n",
    "\n",
    "y_new = np.zeros([y.shape[0],y.shape[1],n_sample])\n",
    "for i in range(y.shape[1]):\n",
    "  y_new[:,i,:] = y_total[n_bus*i:n_bus*(i+1),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9x7neMNj7n3"
   },
   "source": [
    "# Predict generation using $\\pi$\n",
    "* Using predicted $\\pi$ and find the active constraints in $p_G(i)$\n",
    "* For inactive $p_G(i)$ consider other methods like power flow balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Kr29K04j2KTN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 10000)\n",
      "<class 'numpy.ndarray'> 118 [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117]\n"
     ]
    }
   ],
   "source": [
    "gen_limit0 = x_new[:,4,:].copy() # lin cost\n",
    "print(gen_limit0.shape)\n",
    "\n",
    "gen_idx = []\n",
    "gen_idx = np.arange(n_bus)\n",
    "# for i in range(n_bus):\n",
    "#   if gen_limit0[i,0] > 0:\n",
    "#     gen_idx.append(i)\n",
    "print(type(gen_idx),len(gen_idx),gen_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "gHmx9lMDXzpM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 2, 10000) (236, 10000)\n"
     ]
    }
   ],
   "source": [
    "n_sample=x_total.shape[-1]\n",
    "x_feed = torch.from_numpy(x_total.T).float()\n",
    "y_pred1=net(x_feed.to(device)).cpu().detach().numpy().T\n",
    "y_pred_temp = y_pred1.copy()\n",
    "y_pred2=np.zeros([y.shape[0],y.shape[1],n_sample])\n",
    "y_pred2[:,0,:]=y_pred_temp[:n_bus,:]\n",
    "y_pred2[:,1,:]=y_pred_temp[n_bus:,:]\n",
    "print(y_pred2.shape,y_pred1.shape)\n",
    "y_pred1 = y_pred2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "GPEHF2L91bGv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00011212158203122158\n",
      "0.784000000000006\n",
      "0.03788146984760038\n",
      "0.24221453285700786\n"
     ]
    }
   ],
   "source": [
    "gen_cost0 = x_new[:,4,:].copy()\n",
    "lmp_data = y_new[:,0,:].copy()\n",
    "quadratic_a = x_new[:,5,:].copy()\n",
    "profit_pred = y_pred1[:,0,:] - gen_cost0\n",
    "print(np.min(np.abs(profit_pred)))\n",
    "profit_true = lmp_data - gen_cost0\n",
    "print(np.min(np.abs(profit_true)))\n",
    "profit_pred=(y_pred1[:,0,:]-gen_cost0)/(quadratic_a+1e-10)/2\n",
    "profit_true=(lmp_data-gen_cost0)/(quadratic_a+1e-10)/2\n",
    "print(np.min(np.abs(profit_pred)))\n",
    "print(np.min(np.abs(profit_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "uiHWtU5OLc1x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 10000) (118, 10000)\n",
      "0.07667388058490922\n"
     ]
    }
   ],
   "source": [
    "print(profit_pred.shape,profit_true.shape)\n",
    "profit_err = profit_true - profit_pred\n",
    "profit_err_l2 = np.zeros([n_sample,1])\n",
    "\n",
    "for i in range(n_sample):\n",
    "  profit_err_l2[i] = np.linalg.norm(profit_err[:,i])/np.linalg.norm(profit_true[:,i])\n",
    "print(np.mean(profit_err_l2))\n",
    "\n",
    "# fig5 = plt.figure(figsize=(16, 8))\n",
    "# # error histogram\n",
    "# plt.hist(profit_err_l2, bins = 50, facecolor='b', alpha=0.75,label = 'L2 error')\n",
    "# # plt.hist(err_linf_new, bins = 50, facecolor='g', alpha=0.75,label = 'Linf error')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('sample error value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('Histogram of L_2 error')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ZbHchRQd_g8-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1180000,)\n",
      "-3493.877666636092 -2.9391182643149665\n"
     ]
    }
   ],
   "source": [
    "p_pred_sort = np.reshape(profit_pred,n_bus*n_sample)\n",
    "p_true_sort = np.reshape(profit_true,n_bus*n_sample)\n",
    "print(p_pred_sort.shape)\n",
    "print(np.min(p_pred_sort),np.min(p_true_sort))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "TYsSdNGp-OLP"
   },
   "outputs": [],
   "source": [
    "# fig2 = plt.figure(figsize=(8, 8))\n",
    "# plt.hist(p_pred_sort, bins = 50, facecolor='b', alpha=0.75,label = 'pred. profit')\n",
    "# plt.hist(p_true_sort, bins = 50, facecolor='g', alpha=0.75,label = 'true profit')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('profit histogram')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "1J9j5tT9p_f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1827430.8504865684 2764601.835122853\n",
      "1827430.8504865684 43260060.56350001 1827430.8504865684\n"
     ]
    }
   ],
   "source": [
    "# x = [load, gen_cost, gen_lim]\n",
    "binary_thres_true = 1e-5\n",
    "binary_thres = x_new[:,0,:].copy() # upper\n",
    "binary_thres_lo = x_new[:,1,:].copy() # lower\n",
    "gen_pred_binary_full = np.zeros((n_bus,n_sample))\n",
    "gen_true_binary_full = np.zeros((n_bus,n_sample))\n",
    "\n",
    "for i in range(n_sample):\n",
    "  for j in range(len(gen_idx)):\n",
    "    # predicted generator limit\n",
    "    if profit_pred[gen_idx[j],i] > binary_thres[gen_idx[j],i]:\n",
    "      gen_pred_binary_full[gen_idx[j],i] = binary_thres[gen_idx[j],i]\n",
    "    elif profit_pred[gen_idx[j],i] < binary_thres_lo[gen_idx[j],i]:\n",
    "      gen_pred_binary_full[gen_idx[j],i] = binary_thres_lo[gen_idx[j],i]\n",
    "    else:\n",
    "      gen_pred_binary_full[gen_idx[j],i] = profit_pred[gen_idx[j],i]\n",
    "    # true generator limit\n",
    "    if profit_true[gen_idx[j],i] > binary_thres[gen_idx[j],i]:\n",
    "      gen_true_binary_full[gen_idx[j],i] = binary_thres[gen_idx[j],i]\n",
    "    elif profit_true[gen_idx[j],i] < binary_thres_lo[gen_idx[j],i]:\n",
    "      gen_true_binary_full[gen_idx[j],i] = binary_thres_lo[gen_idx[j],i]\n",
    "    else:\n",
    "      gen_true_binary_full[gen_idx[j],i] = profit_true[gen_idx[j],i]\n",
    "\n",
    "gen_inj=gen_pred_binary_full\n",
    "gen_inj_true=gen_true_binary_full\n",
    "# nodal injection\n",
    "load0 = -x_new[:,1,:].copy() # load file\n",
    "p_inj = gen_inj #- load0\n",
    "p_inj_true = gen_inj_true #- load0\n",
    "print(np.sum(p_inj),np.sum(gen_inj_true))\n",
    "print(np.sum(p_inj),np.sum(load0),np.sum(gen_inj))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WAgqdRPjAONm"
   },
   "source": [
    "## injection accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "J46pLAw2AQor"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 10000) (118, 10000)\n",
      "mean p_inj l2 err: 0.1735892943061762\n"
     ]
    }
   ],
   "source": [
    "print(p_inj_true.shape,p_inj.shape)\n",
    "p_inj_true_sort = np.reshape(p_inj_true,n_bus*n_sample)\n",
    "p_inj_sort = np.reshape(p_inj,n_bus*n_sample)\n",
    "p_err = np.zeros(n_sample)\n",
    "for i in range(n_sample):\n",
    "  p_err[i] = np.linalg.norm(p_inj_true[:,i]-p_inj[:,i]) / np.linalg.norm(p_inj_true[:,i])\n",
    "print('mean p_inj l2 err:',np.mean(p_err))\n",
    "# fig3 = plt.figure(figsize=(16, 8))\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.hist(p_inj_sort, bins = 50, facecolor='b', alpha=0.75,label = 'pred. injection')\n",
    "# plt.hist(p_inj_true_sort, bins = 50, facecolor='g', alpha=0.75,label = 'true injection')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('injection histogram')\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.hist(p_err, bins = 50, facecolor='b', alpha=0.75,label = 'injection err')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('error histogram')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)s_binary\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DaN7u4xeGIom"
   },
   "source": [
    "* Calculate flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "YT4sgn_n79MI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(186, 1) (186, 10000) (186, 10000)\n",
      "19053 13334\n",
      "0.010243548387096775 0.007168817204301075\n",
      "186 10000 (186, 10000)\n"
     ]
    }
   ],
   "source": [
    "filename=root+'118ac_fmax.txt'\n",
    "f_max1=pd.read_table(filename,sep=',',header=None).to_numpy() # flow limit\n",
    "\n",
    "n_line = np.size(S_isf,0)\n",
    "flow_est = np.zeros((n_line,n_sample))\n",
    "flow_est0 = np.zeros((n_line,n_sample))\n",
    "\n",
    "f_binary = np.zeros((n_line,n_sample))\n",
    "f_binary0 = np.zeros((n_line,n_sample))\n",
    "\n",
    "# for i in range(n_sample):\n",
    "flow_est = np.dot(S_isf,p_inj)\n",
    "flow_est0 = np.dot(S_isf,p_inj_true)\n",
    "# f_max\n",
    "# f_max_numpy = f_max.cpu().detach().numpy()\n",
    "f_max_numpy = f_max1.copy()\n",
    "f_binary = (np.abs(flow_est)-f_max_numpy > 0)\n",
    "f_binary0 = (np.abs(flow_est0)-f_max_numpy > 0)\n",
    "\n",
    "print(f_max_numpy.shape,flow_est.shape,flow_est0.shape)\n",
    "f_tot_sample = n_line * n_sample\n",
    "print(np.sum(f_binary),np.sum(f_binary0))\n",
    "print(np.sum(f_binary)/f_tot_sample,np.sum(f_binary0)/f_tot_sample)\n",
    "print(n_line,n_sample,flow_est.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "VyvDpKhyQj0M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184.09685937712908 39.64401431411352\n",
      "1.2273123958475272 0.2642934287607568\n"
     ]
    }
   ],
   "source": [
    "# soft threshold\n",
    "f_err_est = np.abs(flow_est)-f_max_numpy\n",
    "f_err_true = np.abs(flow_est0)-f_max_numpy\n",
    "\n",
    "f_err_est = np.maximum(np.abs(flow_est)-f_max_numpy,0) # identify violations\n",
    "f_err_true = np.maximum(np.abs(flow_est0)-f_max_numpy,0)\n",
    "\n",
    "print(np.max(f_err_est),np.max(f_err_true))\n",
    "print(np.max(f_err_est/f_max_numpy),np.max(f_err_true/f_max_numpy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "7iuX7tN2a2Cp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14700 10451\n",
      "0.007903225806451614 0.005618817204301075\n"
     ]
    }
   ],
   "source": [
    "f_binary_soft = (np.abs(flow_est)-f_max_numpy > 0.1*(f_max_numpy))\n",
    "f_binary0_soft = (np.abs(flow_est0)-f_max_numpy > 0.1*(f_max_numpy))\n",
    "print(np.sum(f_binary_soft),np.sum(f_binary0_soft))\n",
    "print(np.sum(f_binary_soft)/f_tot_sample,np.sum(f_binary0_soft)/f_tot_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "SYl9pxnOUUQF"
   },
   "outputs": [],
   "source": [
    "f_pred_sort = np.reshape(f_err_est/f_max_numpy,n_line*n_sample)\n",
    "f_true_sort = np.reshape(f_err_true/f_max_numpy,n_line*n_sample)\n",
    "\n",
    "# fig2 = plt.figure(figsize=(16, 8))\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.hist(f_pred_sort, bins = 10, facecolor='b', alpha=0.75,label = 'pred. f')\n",
    "# plt.hist(f_true_sort, bins = 10, facecolor='g', alpha=0.75,label = 'true f')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('percentage')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('flow violation level histogram')\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.hist(f_true_sort, bins = 10, facecolor='g', alpha=0.75,label = 'true f')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('percentage')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('flow violation level histogram')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "YglgTpWVRLri"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sample pred: 10\n",
      "max line pred: 7212\n",
      "max sample true: 3\n",
      "max line true: 10000\n"
     ]
    }
   ],
   "source": [
    "f_line = np.sum(f_binary,0)\n",
    "f_samp = np.sum(f_binary,1)\n",
    "print('max sample pred:',np.max(f_line))\n",
    "print('max line pred:',np.max(f_samp))\n",
    "\n",
    "f_line0  = np.sum(f_binary0,0)\n",
    "f_samp0 = np.sum(f_binary0,1)\n",
    "print('max sample true:',np.max(f_line0))\n",
    "print('max line true:',np.max(f_samp0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZQnarHmPl35"
   },
   "source": [
    "# Check objective optimality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "BZjhp-CgQaDz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12389221586989191\n"
     ]
    }
   ],
   "source": [
    "gen_cost_pred = np.zeros((n_bus,n_sample))\n",
    "gen_cost_true = np.zeros((n_bus,n_sample))\n",
    "objective_err = np.zeros(n_sample)\n",
    "\n",
    "gen_cost_pred = np.multiply(np.multiply(p_inj,p_inj),quadratic_a) + np.multiply(p_inj,gen_cost0)\n",
    "gen_cost_true = np.multiply(np.multiply(p_inj_true,p_inj_true),quadratic_a) + np.multiply(p_inj_true,gen_cost0)\n",
    "\n",
    "objective_err = np.sum(np.abs(gen_cost_true-gen_cost_pred),axis=0) / np.sum(gen_cost_true,axis=0)\n",
    "print(np.mean(objective_err))\n",
    "\n",
    "# fig6 = plt.figure(figsize=(16, 8))\n",
    "# # error histogram\n",
    "# plt.hist(objective_err, bins = 50, facecolor='b', alpha=0.75,label = 'L2 error')\n",
    "# # plt.hist(err_linf_new, bins = 50, facecolor='g', alpha=0.75,label = 'Linf error')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('sample error value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('Histogram of L_2 error')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdrsAWpYo0-w"
   },
   "source": [
    "## injection accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "mArjSN-So0-x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 10000) (118, 10000)\n",
      "mean p_inj l2 err: 0.1735892943061762\n"
     ]
    }
   ],
   "source": [
    "print(p_inj_true.shape,p_inj.shape)\n",
    "\n",
    "p_inj_true_sort = np.reshape(p_inj_true,n_bus*n_sample)\n",
    "p_inj_sort = np.reshape(p_inj,n_bus*n_sample)\n",
    "\n",
    "p_err = np.zeros(n_sample)\n",
    "for i in range(n_sample):\n",
    "  p_err[i] = np.linalg.norm(p_inj_true[:,i]-p_inj[:,i]) / np.linalg.norm(p_inj_true[:,i])\n",
    "\n",
    "print('mean p_inj l2 err:',np.mean(p_err))\n",
    "# fig3 = plt.figure(figsize=(16, 8))\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.hist(p_inj_sort, bins = 50, facecolor='b', alpha=0.75,label = 'pred. injection')\n",
    "# plt.hist(p_inj_true_sort, bins = 50, facecolor='g', alpha=0.75,label = 'true injection')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('injection histogram')s_binary\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.hist(p_err, bins = 50, facecolor='b', alpha=0.75,label = 'injection err')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('error histogram')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujU84tOSpqsy"
   },
   "source": [
    "# Test AC feasibility\n",
    "* P in actual value, V in p.u.\n",
    "* Use P to recover $\\theta$, or solve $\\theta$ and Q for PF\n",
    "$$ Q_m = V_m \\sum_{n=1}^N V_n \\left(G_{mn}\\sin\\theta_{mn} - B_{mn}\\cos\\theta_{mn} \\right) $$\n",
    "calculate $Q_{mn}$ directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "Of6mEXF4puDN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 118) (118, 118)\n",
      "(117, 117) (118, 10000) (118, 10000)\n",
      "(118, 10000) (118, 10000)\n"
     ]
    }
   ],
   "source": [
    "# Bbus and B_r inverse\n",
    "filename1 = root+'ieee118_Bbus.txt'\n",
    "Bbus=pd.read_table(filename1,sep=',',header=None).to_numpy()\n",
    "B_r = np.delete(Bbus,68,axis=0)\n",
    "B_r = np.delete(B_r,68,axis=1)\n",
    "Br_inv = np.linalg.inv(B_r)\n",
    "\n",
    "# Y = G + jB\n",
    "filename1 = root+'ieee118_Gmat.txt'\n",
    "G_mat=pd.read_table(filename1,sep=',',header=None).to_numpy()\n",
    "filename1 = root+'ieee118_Bmat.txt'\n",
    "B_mat=pd.read_table(filename1,sep=',',header=None).to_numpy()\n",
    "print(G_mat.shape,B_mat.shape)\n",
    "\n",
    "# line parameters\n",
    "filename1 = root+'ieee118_lineloc.txt'\n",
    "line_loc = pd.read_table(filename1,sep=',',header=None).to_numpy()\n",
    "\n",
    "# load line params\n",
    "filename1 = root+'ieee118_lineparams.txt'\n",
    "line_params = pd.read_table(filename1,sep=',',header=None).to_numpy()\n",
    "R_line = line_params[:,0].copy()\n",
    "X_line = line_params[:,1].copy()\n",
    "B_shunt = line_params[:,2].copy()\n",
    "Z_line = R_line + 1j * X_line \n",
    "Y_line = 1 / Z_line\n",
    "G_line = np.real(Y_line)\n",
    "B_line = np.imag(Y_line)\n",
    "# P_inj w/out reference bus in p.u.\n",
    "p_inj_r = np.delete(p_inj,68,axis=0) / 100\n",
    "p_inj_true_r = np.delete(p_inj_true,68,axis=0) / 100\n",
    "p_inj_pu = p_inj / 100\n",
    "p_inj_true_pu = p_inj_true / 100\n",
    "print(Br_inv.shape,p_inj.shape,p_inj_true.shape)#p_inj_true\n",
    "\n",
    "theta0 = np.matmul(Br_inv,p_inj_r)\n",
    "theta_true0 = np.matmul(Br_inv,p_inj_true_r)\n",
    "theta = np.insert(theta0,68,0,axis = 0)\n",
    "theta_true = np.insert(theta_true0,68,0,axis = 0)\n",
    "print(theta.shape,theta_true.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.469106615249783 -0.9936982985230585\n",
      "2.7803011534120627 -9.166735486002148\n"
     ]
    }
   ],
   "source": [
    "print(np.max(theta),np.min(theta))\n",
    "math.sin(math.pi/6)\n",
    "print(G_line[0],B_line[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(186, 10000)\n",
      "1.176731777191162 0.8440495252609254 (118, 10000)\n"
     ]
    }
   ],
   "source": [
    "# Calculate real and reactive flow\n",
    "f_p = np.zeros((n_line,n_sample))\n",
    "f_q = np.zeros((n_line,n_sample))\n",
    "fji_p = np.zeros((n_line,n_sample))\n",
    "fji_q = np.zeros((n_line,n_sample))\n",
    "print(f_q.shape)\n",
    "\n",
    "v_pred = y_pred1[:,1,:].copy()\n",
    "v_pred = v_pred / vy_scale + vy_deviation\n",
    "print(np.max(v_pred),np.min(v_pred),v_pred.shape)\n",
    "\n",
    "theta1 = theta[line_loc[:,0]-1,:]\n",
    "theta2 = theta[line_loc[:,1]-1,:]\n",
    "V1 = v_pred[line_loc[:,0]-1,:]\n",
    "V2 = v_pred[line_loc[:,1]-1,:] \n",
    "f_p=(a*G_line*(V1*V1).T)-a*((V1*V2).T)*(G_line*np.cos(theta1-theta2).T+B_line*np.sin(theta1-theta2).T)\n",
    "f_p=f_p.T\n",
    "f_q=-a*(V1.T)*(a*V1.T)*(B_line+B_shunt/2)+a*((V1*V2).T)*(B_line*np.cos(theta1-theta2).T-G_line*np.sin(theta1-theta2).T)\n",
    "f_q=f_q.T\n",
    "\n",
    "theta1 = theta[line_loc[:,1]-1,:]\n",
    "theta2 = theta[line_loc[:,0]-1,:]\n",
    "V1 = v_pred[line_loc[:,1]-1,:]\n",
    "V2 = v_pred[line_loc[:,0]-1,:]\n",
    "fji_p=(a*G_line*(V1*V1).T)-a*((V1*V2).T)*(G_line*np.cos(theta1-theta2).T+B_line*np.sin(theta1-theta2).T)\n",
    "fji_p=fji_p.T\n",
    "fji_q=-a*(V1.T)*(a*V1.T)*(B_line+B_shunt/2)+a*((V1*V2).T)*(B_line*np.cos(theta1-theta2).T-G_line*np.sin(theta1-theta2).T)\n",
    "fji_q=fji_q.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "gKfcrbTSVMeO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.120182277895026 -24.210320291056092\n",
      "33.16230739236516 151.0\n"
     ]
    }
   ],
   "source": [
    "s_pred = np.sqrt(f_p*f_p+f_q*f_q)*100\n",
    "sji_pred = np.sqrt(fji_p*fji_p+fji_q*fji_q)*100\n",
    "print(np.max(f_q),np.min(f_q))\n",
    "flow_est.shape\n",
    "print(np.mean(s_pred[0,:]),np.mean(f_max_numpy[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "YO4__LJ2brLu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101871\n",
      "hard violation rate: 0.05476935483870967\n",
      "80041\n",
      "0.04303279569892473\n"
     ]
    }
   ],
   "source": [
    "sij_binary = (np.abs(s_pred)-f_max_numpy[:n_line] > 0)\n",
    "sji_binary = (np.abs(sji_pred)-f_max_numpy[:n_line] > 0)\n",
    "s_binary = np.maximum(sij_binary,sji_binary)\n",
    "print(np.sum(s_binary))#,np.sum(f_binary0))\n",
    "print('hard violation rate:',np.sum(s_binary)/n_sample/n_line)#,np.sum(f_binary0)/f_tot_sample)\n",
    "s_binary_soft = (np.abs(s_pred)-f_max_numpy[:n_line] > 0.1*(f_max_numpy[:n_line]))\n",
    "print(np.sum(s_binary_soft))#,np.sum(f_binary0_soft))\n",
    "print(np.sum(s_binary_soft)/n_sample/n_line)#,np.sum(f_binary0_soft)/f_tot_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "Ty0WpBdfJwMR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S violation level:\n",
      "hard: 0.05476935483870968\n",
      "mean: 0.040065006287387746\n",
      "median: 0.0\n",
      "max: 9.260294844503802\n",
      "std: 0.26865347569495823\n",
      "p99: 1.2757338349259104\n",
      "f violation level:\n",
      "hard: 0.010243548387096775 0.007168817204301075\n",
      "mean: 0.003381439621359906\n",
      "median: 0.0\n",
      "max: 1.2273123958475272\n",
      "std: 0.041987147063885184\n",
      "p99: 0.007303807730418323\n"
     ]
    }
   ],
   "source": [
    "# violation level\n",
    "sij_violation = np.abs(s_pred)-f_max_numpy[:n_line] #/ f_max_numpy\n",
    "sij_violation_level = np.maximum(sij_violation,0)\n",
    "sji_violation = np.abs(sji_pred)-f_max_numpy[:n_line] #/ f_max_numpy\n",
    "sji_violation_level = np.maximum(sji_violation,0)\n",
    "s_violation_level = np.maximum(sij_violation_level,sji_violation_level)\n",
    "s_violation_level = np.divide(s_violation_level,f_max_numpy[:n_line])\n",
    "s_vio_lvl = np.reshape(s_violation_level,n_line*n_sample)\n",
    "\n",
    "print('S violation level:')\n",
    "print('hard:',np.sum(s_binary)/f_tot_sample)\n",
    "print('mean:',np.mean(s_vio_lvl))\n",
    "print('median:',np.median(s_vio_lvl))\n",
    "print('max:',np.max(s_vio_lvl))\n",
    "print('std:',np.std(s_vio_lvl))\n",
    "print('p99:',np.percentile(s_vio_lvl,99))\n",
    "\n",
    "f_violation = np.abs(flow_est)-f_max_numpy #/ f_max_numpy\n",
    "f_violation_level = np.maximum(f_violation,0)\n",
    "f_violation_level = np.divide(f_violation_level,f_max_numpy)\n",
    "f_vio_lvl = np.reshape(f_violation_level,n_line*n_sample)\n",
    "\n",
    "print('f violation level:')\n",
    "print('hard:',np.sum(f_binary)/f_tot_sample,np.sum(f_binary0)/f_tot_sample)\n",
    "print('mean:',np.mean(f_vio_lvl))\n",
    "print('median:',np.median(f_vio_lvl))\n",
    "print('max:',np.max(f_vio_lvl))\n",
    "print('std:',np.std(f_vio_lvl))\n",
    "print('p99:',np.percentile(f_vio_lvl,99))\n",
    "\n",
    "# fig4 = plt.figure(figsize=(6,4))\n",
    "# plt.hist(s_vio_lvl, bins = 50, facecolor='b', alpha=0.75,label = 's violation')\n",
    "# plt.hist(f_vio_lvl, bins = 50, facecolor='r', alpha=0.75,label = 'f violation')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('violation level')\n",
    "# plt.ylabel('frequency')\n",
    "# # plt.title('injection histogram')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "kWXEpj-ryjbJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price L2 mean: 0.07909630204568156 L_inf mean: 0.12169735309667963\n",
      "std: 0.03223798267442152\n",
      "Voltage L2 mean: 0.02178199541803404 L_inf mean: 0.06319676368322462\n",
      "std: 0.0023174730332581684\n"
     ]
    }
   ],
   "source": [
    "# err_L2_mean = np.mean(err_L2)\n",
    "# err_Linf_mean = np.mean(err_Linf)\n",
    "print('Price L2 mean:', err_L2_mean,'L_inf mean:', err_Linf_mean )\n",
    "print('std:',np.std(err_L2))\n",
    "# err_L2_mean_v = np.mean(err_L2_v)\n",
    "# err_Linf_mean_v = np.mean(err_Linf_v)\n",
    "print('Voltage L2 mean:', err_L2_mean_v,'L_inf mean:', err_Linf_mean_v )\n",
    "print('std:',np.std(err_L2_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " C:\\Users\\USER\\Desktop\\GNN_OPF_electricity_market-main\\output_loss_fig/output_02181136.txt\n"
     ]
    }
   ],
   "source": [
    "import os   \n",
    "# \n",
    "output_content = f\"\"\"\n",
    "'number of params: {sum(p.numel() for p in net.parameters())}\n",
    "Price L2 mean: {err_L2_mean} \n",
    "Price L_inf mean: {err_Linf_mean}\n",
    "Price std: {np.std(err_L2)}\n",
    "\n",
    "Voltage L2 mean: {err_L2_mean_v} \n",
    "Voltage L_inf mean: {err_Linf_mean_v}\n",
    "Voltage std: {np.std(err_L2_v)}\n",
    "\"\"\"\n",
    "\n",
    "# \n",
    "output_dir = r'C:\\Users\\USER\\Desktop\\GNN_OPF_electricity_market-main\\output_loss_fig'\n",
    "timestamp = datetime.now().strftime('%m%d%H%M')\n",
    "output_file = f'{output_dir}/output_{timestamp}.txt'\n",
    "\n",
    "# \n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#  txt \n",
    "with open(output_file, 'w') as file:\n",
    "    file.write(output_content)\n",
    "\n",
    "print(f\" {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "118ac_feasdnn0417.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "AC_OPF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
