{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JKLovDeXoCCP"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import math\n",
    "from datetime import datetime\n",
    "root=''\n",
    "# try:\n",
    "#   from google.colab import drive\n",
    "#   drive.mount('/content/drive')\n",
    "#   root='./drive/MyDrive/gnn/data/'\n",
    "# except:\n",
    "#   pass\n",
    "# device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# print(torch.cuda.get_device_name(0))\n",
    "# torch.cuda.empty_cache()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Hj9_eLfoWQY3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([186, 118]) torch.Size([186, 1]) tensor(72)\n",
      "1654.8 -332.4 949.67 0.94\n",
      "voltage range(scaled): 3.9999999999999925 16.000000000000004\n",
      "price range old: 11.547 949.67\n",
      "voltage range old: 3.9999999999999925 16.000000000000004\n",
      "price range new: 11.547 949.67\n",
      "voltage range new: 3.9999999999999925 16.000000000000004\n",
      "(118, 6, 10000) (118, 2, 10000)\n"
     ]
    }
   ],
   "source": [
    "root = 'C:/Users/USER/Desktop/GNN_OPF_electricity_market-main/GNN_OPF_electricity_market-main/GNN_OPF_electricity_market-main/data_gen/118_data/'\n",
    "# C:\\Users\\USER\\Desktop\\GNN_OPF_electricity_market-main\\GNN_OPF_electricity_market-main\\GNN_OPF_electricity_market-main\\data_gen\\118_data\\118dc_quad_ISF.txt\n",
    "filename = root + '118dc_quad_ISF.txt'\n",
    "S_isf=pd.read_table(filename,sep=',',header=None).to_numpy() # ISF matrix\n",
    "filename=root+'118ac_fmax.txt'\n",
    "f_max=pd.read_table(filename,sep=',',header=None).to_numpy() # flow limit\n",
    "n_line = np.size(S_isf,0)\n",
    "S = torch.from_numpy(S_isf).to(device) # ISF\n",
    "f_max = torch.from_numpy(f_max).to(device) # flow limit\n",
    "print(S.shape,f_max.shape,torch.min(f_max))\n",
    "\n",
    "x=np.load(root+'ac118_p10_x_v.npy')\n",
    "y=np.load(root+'ac118_p10_y_v.npy')\n",
    "W=np.load(root+'ac118_p10_w.npy')\n",
    "print(np.max(x),np.min(x),np.max(y),np.min(y))\n",
    "\n",
    "# scaling on voltage\n",
    "vy_deviation = 0.9\n",
    "vy_scale = 100\n",
    "y[:,1,:] = (y[:,1,:] - vy_deviation) * vy_scale\n",
    "print('voltage range(scaled):',np.min(y[:,1,:]),np.max(y[:,1,:]))\n",
    "\n",
    "# scaling on price\n",
    "pi_deviation = 0\n",
    "y[:,0,:] = y[:,0,:] + pi_deviation\n",
    "# filter out extreme points in price\n",
    "y_sort_arg = np.argsort(np.amax(np.abs(y[:,0,:]),axis=0)) # max extreme\n",
    "y_sort_arg1 = np.argsort(np.amin(y[:,0,:],axis=0),axis=0) # min extreme\n",
    "\n",
    "del_idx0 = []\n",
    "del_num = 0\n",
    "for i in range(del_num):\n",
    "  del_idx0.append(y_sort_arg[-i])\n",
    "  del_idx0.append(y_sort_arg1[i])\n",
    "# print(del_idx0)\n",
    "del_idx = [] # keep only non-repeated\n",
    "[del_idx.append(x) for x in del_idx0 if x not in del_idx]\n",
    "del_idx = np.sort(del_idx)\n",
    "# delete extreme points\n",
    "print('price range old:',np.min(y[:,0,:]),np.max(y[:,0,:]))\n",
    "print('voltage range old:',np.min(y[:,1,:]),np.max(y[:,1,:]))\n",
    "# x = np.delete(x, del_idx, axis=2)\n",
    "# y = np.delete(y, del_idx, axis=2)\n",
    "print('price range new:',np.min(y[:,0,:]),np.max(y[:,0,:]))\n",
    "print('voltage range new:',np.min(y[:,1,:]),np.max(y[:,1,:]))\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "YcR42RBbdZqZ"
   },
   "outputs": [],
   "source": [
    "n_sample=y.shape[-1]\n",
    "n_bus=y.shape[0]\n",
    "x_total=x.transpose((1,0,2)).reshape(-1,x.shape[-1])\n",
    "y_total=y.transpose((1,0,2)).reshape(-1,y.shape[-1])\n",
    "x_train,x_test,y_train,y_test=train_test_split(x_total.T,y_total.T,test_size=0.2)\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        self.x=torch.from_numpy(x).float()\n",
    "        self.y=torch.from_numpy(y).float()\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "          idx=idx.tolist()\n",
    "        return self.x[idx],self.y[idx]\n",
    "batch_size=512\n",
    "#512\n",
    "params={'batch_size': batch_size,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 0}\n",
    "train=Dataset(x_train,y_train)\n",
    "train_set=torch.utils.data.DataLoader(train,**params)\n",
    "val=Dataset(x_test,y_test)\n",
    "val_set=torch.utils.data.DataLoader(val,**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 708])\n",
      "Train Mean: 6.413394927978516\n",
      "Train Std: 3.698127031326294\n",
      "Validation Mean: 6.445342063903809\n",
      "Validation Std: 3.674724578857422\n"
     ]
    }
   ],
   "source": [
    "# 資料分析\n",
    "# 從資料集中建立 DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 取第一個 batch 進行分析\n",
    "train_batch_x, _ = next(iter(train_loader))\n",
    "val_batch_x, _ = next(iter(val_loader))\n",
    "\n",
    "# 計算均值和標準差\n",
    "print(train_batch_x.shape)\n",
    "train_mean, train_std = train_batch_x.mean(dim=0), train_batch_x.std(dim=0)\n",
    "val_mean, val_std = val_batch_x.mean(dim=0), val_batch_x.std(dim=0)\n",
    "\n",
    "mean_value_train = train_mean.mean().item()\n",
    "print('Train Mean:', mean_value_train)\n",
    "std_value_train = train_std.mean().item()\n",
    "print('Train Std:', std_value_train)\n",
    "mean_value_val = val_mean.mean().item()\n",
    "print('Validation Mean:', mean_value_val)\n",
    "std_value_val = val_std.mean().item()\n",
    "print('Validation Std:', std_value_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "JN9gN9BnCNim"
   },
   "outputs": [],
   "source": [
    "# fig2 = plt.figure(figsize=(8,4))\n",
    "# flat_list = list(np.concatenate(y[:,n_sample:]).flat)\n",
    "# flat_list3 = list(np.concatenate(y[:,:n_sample]).flat)\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.hist(flat_list,bins = 100,label = 'voltage')\n",
    "# plt.subplot(1,2,2)\n",
    "# # plt.hist(flat_list3,range=[-2000, 2000],bins = 100,label = 'price')\n",
    "# plt.hist(flat_list3,bins = 100,label = 'price')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "iOyWpG_4yCtz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params: 3909576\n",
      "dnn(\n",
      "  (features): Sequential(\n",
      "    (0): Linear(in_features=708, out_features=1180, bias=True)\n",
      "    (1): BatchNorm1d(1180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=1180, out_features=1180, bias=True)\n",
      "    (5): BatchNorm1d(1180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.5, inplace=False)\n",
      "    (8): Linear(in_features=1180, out_features=1180, bias=True)\n",
      "    (9): BatchNorm1d(1180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Dropout(p=0.5, inplace=False)\n",
      "    (12): Linear(in_features=1180, out_features=236, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class dnn(torch.nn.Module):\n",
    "  def __init__(self,shape):\n",
    "    super(dnn,self).__init__()\n",
    "    layers=[]\n",
    "    for idx in range(len(shape)-2):\n",
    "      layers.extend([\n",
    "        nn.Linear(shape[idx],shape[idx+1]),\n",
    "        nn.BatchNorm1d(shape[idx+1]),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "      ])\n",
    "    layers+=[nn.Linear(shape[-2],shape[-1])]\n",
    "    self.features=nn.Sequential(*layers)\n",
    "    for temp in self.features:\n",
    "      if type(temp)==nn.Linear:\n",
    "        torch.nn.init.normal_(temp.weight,mean=0,std=1)\n",
    "  def forward(self,x): return self.features(x)\n",
    "net=dnn([n_bus*6,n_bus*10,n_bus*10,n_bus*10,n_bus*2]).to(device)\n",
    "\n",
    "\n",
    "print('number of params: %d'%(sum(temp.numel() for temp in net.parameters() if temp.requires_grad)))\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GEQ-MDKOTqW1"
   },
   "outputs": [],
   "source": [
    "# threshold function for p_g\n",
    "class my_gen_pred_binary(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(my_gen_pred_binary,self).__init__()\n",
    "  def forward(self,x,thresh):\n",
    "    right_thresh=thresh.clone().detach().requires_grad_(True).double()\n",
    "    left_thresh=torch.tensor(0).double()\n",
    "    x=x.double()\n",
    "    output = torch.sigmoid(left_thresh - x)\n",
    "    output = torch.mul(output,left_thresh - x) + x\n",
    "    output = torch.sigmoid(output - right_thresh)\n",
    "    output = torch.mul(output,output - right_thresh) + right_thresh\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9mYCMszHaosA"
   },
   "outputs": [],
   "source": [
    "## params needed for S calculation\n",
    "# line parameters\n",
    "filename1 = root+'ieee118_lineloc.txt'\n",
    "filename2 = root+'ieee118_lineparams.txt'\n",
    "filename3 = root+'ieee118_Bmat.txt'\n",
    "# incidence info\n",
    "line_loc = pd.read_table(filename1,sep=',',header=None).to_numpy()\n",
    "# r, x, shunt, S_max\n",
    "line_params = pd.read_table(filename2,sep=',',header=None).to_numpy()\n",
    "B_mat=pd.read_table(filename3,sep=',',header=None).to_numpy()\n",
    "B_r = np.delete(B_mat,68,axis=0)\n",
    "B_r = np.delete(B_r,68,axis=1)\n",
    "Br_inv = np.linalg.inv(B_r)\n",
    "\n",
    "R_line = line_params[:,0].copy()\n",
    "X_line = line_params[:,1].copy()\n",
    "\n",
    "B_shunt = line_params[:,2].copy()\n",
    "\n",
    "Z_line = R_line + 1j * X_line \n",
    "Y_line = 1 / Z_line\n",
    "\n",
    "G_line = np.real(Y_line)\n",
    "B_line = np.imag(Y_line)\n",
    "\n",
    "# transformer indicator\n",
    "a = (R_line > 0).astype(int)\n",
    "\n",
    "# params to tensor and GPU\n",
    "G_line_tensor = torch.from_numpy(G_line).to(device) # conductance\n",
    "B_line_tensor = torch.from_numpy(B_line).to(device) # susceptance\n",
    "B_shunt_tensor = torch.from_numpy(B_shunt/2).to(device) # conductance\n",
    "Br_inv_tensor = torch.from_numpy(Br_inv).to(device) # reduced Bbus matrix\n",
    "a_tensor = torch.from_numpy(a).double().to(device) # line/transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Go4bwoEmoi0D"
   },
   "outputs": [],
   "source": [
    "class loss_func:\n",
    "    def __init__(self,s_max,G_line,B_line,B_shunt,Br_inv,a,line_loc):\n",
    "      self.s=s_max\n",
    "      self.g=G_line\n",
    "      self.b=B_line\n",
    "      self.c=B_shunt\n",
    "      self.r=Br_inv\n",
    "      self.a=a\n",
    "      self.mse=nn.MSELoss() # MSE loss\n",
    "      self.lmda1=torch.tensor(10).to(device) # V MSE \n",
    "      self.lmda2=torch.tensor(1).to(device) # pi MSE \n",
    "      self.lmda3=torch.tensor(0.1).to(device) # v l_inf\n",
    "      self.lmda4=torch.tensor(0.02).to(device) # s feasibility\n",
    "      self.lmda5=torch.tensor(0.01).to(device) # pi l_inf\n",
    "      self.line_loc=line_loc\n",
    "      self.binary_cell=my_gen_pred_binary()\n",
    "    def calc(self,pred,label,x,feas):\n",
    "      mse_p=self.mse(pred[:,:n_bus],label[:,:n_bus])\n",
    "      mse_v=self.mse(pred[:,n_bus:],label[:,n_bus:])\n",
    "      linf_p=(pred[:,:n_bus]-label[:,:n_bus]).norm(p=float('inf'))\n",
    "      linf_v=(pred[:,n_bus:]-label[:,n_bus:]).norm(p=float('inf'))\n",
    "      if feas==False:\n",
    "        return self.lmda1*mse_v+self.lmda2*mse_p+self.lmda3*linf_v+self.lmda5*linf_p\n",
    "      label_pred=pred[:,:n_bus]\n",
    "      p_max=x[:,:n_bus*1]-x[:,n_bus*1:n_bus*2]\n",
    "      quadratic_b=x[:,n_bus*4:n_bus*5]\n",
    "      quadratic_a=x[:,n_bus*5:n_bus*6]\n",
    "      quadratic_center=(label_pred-quadratic_b)/(quadratic_a+1e-5)/2\n",
    "      p_inj=self.binary_cell(quadratic_center,p_max)\n",
    "      bus_inj=p_inj+x[:,n_bus:n_bus*2]\n",
    "      p_inj_r=torch.cat((bus_inj[:,:68],bus_inj[:,69:]),1)/100\n",
    "      theta0=torch.matmul(self.r,p_inj_r.T)\n",
    "      ref_ang=torch.zeros(1,theta0.shape[1]).to(device)\n",
    "      theta=torch.cat([theta0[:68,:],ref_ang,theta0[68:,:]],0)\n",
    "      v_pred=(pred[:,n_bus:].transpose(0,1))*0.01+0.9\n",
    "      \n",
    "      # s penalty\n",
    "      theta1=theta[self.line_loc[:,0]-1,:]\n",
    "      theta2=theta[self.line_loc[:,1]-1,:]\n",
    "      V1=(v_pred[self.line_loc[:,0]-1,:]).double()\n",
    "      V2=(v_pred[self.line_loc[:,1]-1,:]).double()\n",
    "      f_p=(self.a*self.g*(V1*V1).T)-self.a*((V1*V2).T)*(self.g*torch.cos(theta1-theta2).T+self.b*torch.sin(theta1-theta2).T)\n",
    "      f_p=f_p.T\n",
    "      f_q=-self.a*(V1.T)*(self.a*V1.T)*(self.b+self.c/2)+self.a*((V1*V2).T)*(self.b*torch.cos(theta1-theta2).T-self.g*torch.sin(theta1-theta2).T)\n",
    "      f_q=f_q.T\n",
    "      s_pred=torch.sqrt(f_p*f_p+f_q*f_q+1e-5)*100\n",
    "      s_penalty=torch.sigmoid(s_pred-self.s)+torch.sigmoid(-s_pred-self.s)\n",
    "      s_total=torch.sum(s_penalty)\n",
    "\n",
    "      # sji penalty\n",
    "      theta1=theta[self.line_loc[:,1]-1,:]\n",
    "      theta2=theta[self.line_loc[:,0]-1,:]\n",
    "      V1=(v_pred[self.line_loc[:,1]-1,:]).double()\n",
    "      V2=(v_pred[self.line_loc[:,0]-1,:]).double() \n",
    "      fji_p=(self.a*self.g*(V1*V1).T)-self.a*((V1*V2).T)*(self.g*torch.cos(theta1-theta2).T+self.b*torch.sin(theta1-theta2).T)\n",
    "      fji_p=fji_p.T\n",
    "      fji_q=-self.a*(V1.T)*(self.a*V1.T)*(self.b+self.c/2)+self.a*((V1*V2).T)*(self.b*torch.cos(theta1-theta2).T-self.g*torch.sin(theta1-theta2).T)\n",
    "      fji_q=fji_q.T\n",
    "      sji_pred=torch.sqrt(fji_p*fji_p+fji_q*fji_q+1e-5)*100\n",
    "      sji_penalty=torch.sigmoid(sji_pred-self.s)+torch.sigmoid(-sji_pred-self.s)\n",
    "      sji_total=torch.sum(sji_penalty)\n",
    "\n",
    "      return self.lmda1*mse_v+self.lmda2*mse_p+self.lmda3*linf_v+self.lmda5*linf_p+self.lmda4*s_total+self.lmda4*sji_total\n",
    "my_loss=loss_func(f_max,G_line_tensor,B_line_tensor,B_shunt_tensor,Br_inv_tensor,a_tensor,line_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "iu0PKvcgF2vN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cold start\n",
      "Epoch 0 | Training loss: 430.5431\n",
      "Epoch 0 | Testing loss: 405.9904\n",
      "Epoch 1 | Training loss: 413.7313\n",
      "Epoch 2 | Training loss: 398.4850\n",
      "Epoch 3 | Training loss: 384.1770\n",
      "Epoch 4 | Training loss: 370.3811\n",
      "Epoch 5 | Training loss: 356.2094\n",
      "Epoch 5 | Testing loss: 367.4792\n",
      "Epoch 6 | Training loss: 342.0273\n",
      "Epoch 7 | Training loss: 327.4825\n",
      "Epoch 8 | Training loss: 312.5230\n",
      "Epoch 9 | Training loss: 297.3223\n",
      "Epoch 10 | Training loss: 281.5134\n",
      "Epoch 10 | Testing loss: 308.3185\n",
      "Epoch 11 | Training loss: 265.9556\n",
      "Epoch 12 | Training loss: 249.8336\n",
      "Epoch 13 | Training loss: 233.4604\n",
      "Epoch 14 | Training loss: 217.3406\n",
      "Epoch 15 | Training loss: 201.5443\n",
      "Epoch 15 | Testing loss: 229.1147\n",
      "Epoch 16 | Training loss: 185.9538\n",
      "Epoch 17 | Training loss: 170.8666\n",
      "Epoch 18 | Training loss: 156.6466\n",
      "Epoch 19 | Training loss: 142.8390\n",
      "Epoch 20 | Training loss: 130.1908\n",
      "Epoch 20 | Testing loss: 146.6984\n",
      "Epoch 21 | Training loss: 118.4056\n",
      "Epoch 22 | Training loss: 107.7041\n",
      "Epoch 23 | Training loss: 98.1912\n",
      "Epoch 24 | Training loss: 89.5679\n",
      "Epoch 25 | Training loss: 81.9314\n",
      "Epoch 25 | Testing loss: 81.1654\n",
      "Epoch 26 | Training loss: 75.4183\n",
      "Epoch 27 | Training loss: 69.8221\n",
      "Epoch 28 | Training loss: 65.0093\n",
      "Epoch 29 | Training loss: 60.8525\n",
      "Epoch 30 | Training loss: 57.3208\n",
      "Epoch 30 | Testing loss: 43.7852\n",
      "Epoch 31 | Training loss: 54.4217\n",
      "Epoch 32 | Training loss: 51.9003\n",
      "Epoch 33 | Training loss: 49.7433\n",
      "Epoch 34 | Training loss: 47.7872\n",
      "Epoch 35 | Training loss: 46.2724\n",
      "Epoch 35 | Testing loss: 26.0473\n",
      "Epoch 36 | Training loss: 44.8084\n",
      "Epoch 37 | Training loss: 43.4731\n",
      "Epoch 38 | Training loss: 42.2349\n",
      "Epoch 39 | Training loss: 41.2201\n",
      "Epoch 40 | Training loss: 40.0705\n",
      "Epoch 40 | Testing loss: 18.1951\n",
      "Epoch 41 | Training loss: 39.1261\n",
      "Epoch 42 | Training loss: 38.2409\n",
      "Epoch 43 | Training loss: 37.3784\n",
      "Epoch 44 | Training loss: 36.6465\n",
      "Epoch 45 | Training loss: 35.8113\n",
      "Epoch 45 | Testing loss: 14.3846\n",
      "Epoch 46 | Training loss: 35.1820\n",
      "Epoch 47 | Training loss: 34.3398\n",
      "Epoch 48 | Training loss: 33.7608\n",
      "Epoch 49 | Training loss: 33.0852\n",
      "Epoch 50 | Training loss: 32.4218\n",
      "Epoch 50 | Testing loss: 11.8285\n",
      "Epoch 51 | Training loss: 31.8617\n",
      "Epoch 52 | Training loss: 31.2469\n",
      "Epoch 53 | Training loss: 30.6782\n",
      "Epoch 54 | Training loss: 30.0222\n",
      "Epoch 55 | Training loss: 29.5474\n",
      "Epoch 55 | Testing loss: 10.1258\n",
      "Epoch 56 | Training loss: 29.0396\n",
      "Epoch 57 | Training loss: 28.4633\n",
      "Epoch 58 | Training loss: 28.0898\n",
      "Epoch 59 | Training loss: 27.5732\n",
      "Epoch 60 | Training loss: 27.1056\n",
      "Epoch 60 | Testing loss: 8.9931\n",
      "Epoch 61 | Training loss: 26.6585\n",
      "Epoch 62 | Training loss: 26.2459\n",
      "Epoch 63 | Training loss: 25.7889\n",
      "Epoch 64 | Training loss: 25.4152\n",
      "Epoch 65 | Training loss: 25.0324\n",
      "Epoch 65 | Testing loss: 7.8155\n",
      "Epoch 66 | Training loss: 24.6624\n",
      "Epoch 67 | Training loss: 24.2984\n",
      "Epoch 68 | Training loss: 23.8586\n",
      "Epoch 69 | Training loss: 23.4547\n",
      "Epoch 70 | Training loss: 23.0555\n",
      "Epoch 70 | Testing loss: 7.0197\n",
      "Epoch 71 | Training loss: 22.7832\n",
      "Epoch 72 | Training loss: 22.4544\n",
      "Epoch 73 | Training loss: 22.0839\n",
      "Epoch 74 | Training loss: 21.8832\n",
      "Epoch 75 | Training loss: 21.5103\n",
      "Epoch 75 | Testing loss: 6.5204\n",
      "Epoch 76 | Training loss: 21.2622\n",
      "Epoch 77 | Training loss: 20.9159\n",
      "Epoch 78 | Training loss: 20.7258\n",
      "Epoch 79 | Training loss: 20.3727\n",
      "Epoch 80 | Training loss: 20.1539\n",
      "Epoch 80 | Testing loss: 5.9437\n",
      "Epoch 81 | Training loss: 19.7920\n",
      "Epoch 82 | Training loss: 19.6501\n",
      "Epoch 83 | Training loss: 19.2688\n",
      "Epoch 84 | Training loss: 19.0041\n",
      "Epoch 85 | Training loss: 18.8547\n",
      "Epoch 85 | Testing loss: 5.5514\n",
      "Epoch 86 | Training loss: 18.5706\n",
      "Epoch 87 | Training loss: 18.3315\n",
      "Epoch 88 | Training loss: 18.1075\n",
      "Epoch 89 | Training loss: 17.8960\n",
      "Epoch 90 | Training loss: 17.6592\n",
      "Epoch 90 | Testing loss: 5.1989\n",
      "Epoch 91 | Training loss: 17.4877\n",
      "Epoch 92 | Training loss: 17.2246\n",
      "Epoch 93 | Training loss: 16.9940\n",
      "Epoch 94 | Training loss: 16.8425\n",
      "Epoch 95 | Training loss: 16.6343\n",
      "Epoch 95 | Testing loss: 4.8483\n",
      "Epoch 96 | Training loss: 16.4379\n",
      "Epoch 97 | Training loss: 16.2831\n",
      "Epoch 98 | Training loss: 16.0413\n",
      "Epoch 99 | Training loss: 15.8679\n",
      "Epoch 100 | Training loss: 15.7042\n",
      "Epoch 100 | Testing loss: 4.5569\n",
      "Epoch 101 | Training loss: 15.5243\n",
      "Epoch 102 | Training loss: 15.3675\n",
      "Epoch 103 | Training loss: 15.1200\n",
      "Epoch 104 | Training loss: 14.9972\n",
      "Epoch 105 | Training loss: 14.8401\n",
      "Epoch 105 | Testing loss: 4.4095\n",
      "Epoch 106 | Training loss: 14.6222\n",
      "Epoch 107 | Training loss: 14.5269\n",
      "Epoch 108 | Training loss: 14.2940\n",
      "Epoch 109 | Training loss: 14.2157\n",
      "Epoch 110 | Training loss: 14.0346\n",
      "Epoch 110 | Testing loss: 4.1598\n",
      "Epoch 111 | Training loss: 13.8846\n",
      "Epoch 112 | Training loss: 13.7431\n",
      "Epoch 113 | Training loss: 13.5177\n",
      "Epoch 114 | Training loss: 13.4247\n",
      "Epoch 115 | Training loss: 13.3033\n",
      "Epoch 115 | Testing loss: 3.9718\n",
      "Epoch 116 | Training loss: 13.1781\n",
      "Epoch 117 | Training loss: 12.9846\n",
      "Epoch 118 | Training loss: 12.8752\n",
      "Epoch 119 | Training loss: 12.7951\n",
      "Epoch 120 | Training loss: 12.6153\n",
      "Epoch 120 | Testing loss: 3.7727\n",
      "Epoch 121 | Training loss: 12.4814\n",
      "Epoch 122 | Training loss: 12.3443\n",
      "Epoch 123 | Training loss: 12.1832\n",
      "Epoch 124 | Training loss: 12.1209\n",
      "Epoch 125 | Training loss: 11.9785\n",
      "Epoch 125 | Testing loss: 3.6561\n",
      "Epoch 126 | Training loss: 11.9071\n",
      "Epoch 127 | Training loss: 11.7342\n",
      "Epoch 128 | Training loss: 11.6158\n",
      "Epoch 129 | Training loss: 11.5153\n",
      "Epoch 130 | Training loss: 11.3872\n",
      "Epoch 130 | Testing loss: 3.5136\n",
      "Epoch 131 | Training loss: 11.3065\n",
      "Epoch 132 | Training loss: 11.1702\n",
      "Epoch 133 | Training loss: 11.1521\n",
      "Epoch 134 | Training loss: 10.9637\n",
      "Epoch 135 | Training loss: 10.8380\n",
      "Epoch 135 | Testing loss: 3.4079\n",
      "Epoch 136 | Training loss: 10.7645\n",
      "Epoch 137 | Training loss: 10.6657\n",
      "Epoch 138 | Training loss: 10.5741\n",
      "Epoch 139 | Training loss: 10.4753\n",
      "Epoch 140 | Training loss: 10.3631\n",
      "Epoch 140 | Testing loss: 3.3134\n",
      "Epoch 141 | Training loss: 10.2746\n",
      "Epoch 142 | Training loss: 10.1645\n",
      "Epoch 143 | Training loss: 10.0693\n",
      "Epoch 144 | Training loss: 9.9594\n",
      "Epoch 145 | Training loss: 9.9258\n",
      "Epoch 145 | Testing loss: 3.1747\n",
      "Epoch 146 | Training loss: 9.8416\n",
      "Epoch 147 | Training loss: 9.7305\n",
      "Epoch 148 | Training loss: 9.6289\n",
      "Epoch 149 | Training loss: 9.6029\n",
      "Epoch 150 | Training loss: 9.4865\n",
      "Epoch 150 | Testing loss: 3.0614\n",
      "Epoch 151 | Training loss: 9.3838\n",
      "Epoch 152 | Training loss: 9.3448\n",
      "Epoch 153 | Training loss: 9.2300\n",
      "Epoch 154 | Training loss: 9.1122\n",
      "Epoch 155 | Training loss: 9.0615\n",
      "Epoch 155 | Testing loss: 3.0027\n",
      "Epoch 156 | Training loss: 8.9901\n",
      "Epoch 157 | Training loss: 8.8484\n",
      "Epoch 158 | Training loss: 8.8102\n",
      "Epoch 159 | Training loss: 8.7590\n",
      "Epoch 160 | Training loss: 8.6447\n",
      "Epoch 160 | Testing loss: 2.8579\n",
      "Epoch 161 | Training loss: 8.6255\n",
      "Epoch 162 | Training loss: 8.5199\n",
      "Epoch 163 | Training loss: 8.4543\n",
      "Epoch 164 | Training loss: 8.3951\n",
      "Epoch 165 | Training loss: 8.3290\n",
      "Epoch 165 | Testing loss: 2.8037\n",
      "Epoch 166 | Training loss: 8.2795\n",
      "Epoch 167 | Training loss: 8.1011\n",
      "Epoch 168 | Training loss: 8.1202\n",
      "Epoch 169 | Training loss: 8.0655\n",
      "Epoch 170 | Training loss: 7.9667\n",
      "Epoch 170 | Testing loss: 2.7414\n",
      "Epoch 171 | Training loss: 7.8830\n",
      "Epoch 172 | Training loss: 7.8332\n",
      "Epoch 173 | Training loss: 7.7652\n",
      "Epoch 174 | Training loss: 7.7368\n",
      "Epoch 175 | Training loss: 7.6348\n",
      "Epoch 175 | Testing loss: 2.7142\n",
      "Epoch 176 | Training loss: 7.6113\n",
      "Epoch 177 | Training loss: 7.4767\n",
      "Epoch 178 | Training loss: 7.5041\n",
      "Epoch 179 | Training loss: 7.4075\n",
      "Epoch 180 | Training loss: 7.3392\n",
      "Epoch 180 | Testing loss: 2.6160\n",
      "Epoch 181 | Training loss: 7.3433\n",
      "Epoch 182 | Training loss: 7.2224\n",
      "Epoch 183 | Training loss: 7.2067\n",
      "Epoch 184 | Training loss: 7.0890\n",
      "Epoch 185 | Training loss: 7.0309\n",
      "Epoch 185 | Testing loss: 2.5542\n",
      "Epoch 186 | Training loss: 7.0207\n",
      "Epoch 187 | Training loss: 6.9652\n",
      "Epoch 188 | Training loss: 6.8831\n",
      "Epoch 189 | Training loss: 6.8581\n",
      "Epoch 190 | Training loss: 6.8089\n",
      "Epoch 190 | Testing loss: 2.5365\n",
      "Epoch 191 | Training loss: 6.7337\n",
      "Epoch 192 | Training loss: 6.7318\n",
      "Epoch 193 | Training loss: 6.6416\n",
      "Epoch 194 | Training loss: 6.6001\n",
      "Epoch 195 | Training loss: 6.5631\n",
      "Epoch 195 | Testing loss: 2.5138\n",
      "Epoch 196 | Training loss: 6.5046\n",
      "Epoch 197 | Training loss: 6.4441\n",
      "Epoch 198 | Training loss: 6.4330\n",
      "Epoch 199 | Training loss: 6.3909\n",
      "Epoch 200 | Training loss: 6.3351\n",
      "Epoch 200 | Testing loss: 2.4058\n",
      "Epoch 201 | Training loss: 6.2831\n",
      "Epoch 202 | Training loss: 6.2267\n",
      "Epoch 203 | Training loss: 6.1741\n",
      "Epoch 204 | Training loss: 6.1073\n",
      "Epoch 205 | Training loss: 6.1252\n",
      "Epoch 205 | Testing loss: 2.3390\n",
      "Epoch 206 | Training loss: 6.1115\n",
      "Epoch 207 | Training loss: 6.0634\n",
      "Epoch 208 | Training loss: 6.0337\n",
      "Epoch 209 | Training loss: 5.9584\n",
      "Epoch 210 | Training loss: 5.9662\n",
      "Epoch 210 | Testing loss: 2.3465\n",
      "Epoch 211 | Training loss: 5.9293\n",
      "Epoch 212 | Training loss: 5.8372\n",
      "Epoch 213 | Training loss: 5.7381\n",
      "Epoch 214 | Training loss: 5.7254\n",
      "Epoch 215 | Training loss: 5.7043\n",
      "Epoch 215 | Testing loss: 2.3110\n",
      "Epoch 216 | Training loss: 5.6689\n",
      "Epoch 217 | Training loss: 5.6582\n",
      "Epoch 218 | Training loss: 5.6395\n",
      "Epoch 219 | Training loss: 5.6441\n",
      "Epoch 220 | Training loss: 5.5378\n",
      "Epoch 220 | Testing loss: 2.2355\n",
      "Epoch 221 | Training loss: 5.4765\n",
      "Epoch 222 | Training loss: 5.4221\n",
      "Epoch 223 | Training loss: 5.5045\n",
      "Epoch 224 | Training loss: 5.4321\n",
      "Epoch 225 | Training loss: 5.3247\n",
      "Epoch 225 | Testing loss: 2.1817\n",
      "Epoch 226 | Training loss: 5.3028\n",
      "Epoch 227 | Training loss: 5.3386\n",
      "Epoch 228 | Training loss: 5.2111\n",
      "Epoch 229 | Training loss: 5.2874\n",
      "Epoch 230 | Training loss: 5.2452\n",
      "Epoch 230 | Testing loss: 2.2282\n",
      "Epoch 231 | Training loss: 5.1800\n",
      "Epoch 232 | Training loss: 5.1683\n",
      "Epoch 233 | Training loss: 5.1455\n",
      "Epoch 234 | Training loss: 5.0638\n",
      "Epoch 235 | Training loss: 5.0309\n",
      "Epoch 235 | Testing loss: 2.2112\n",
      "Epoch 236 | Training loss: 5.0616\n",
      "Epoch 237 | Training loss: 5.0346\n",
      "Epoch 238 | Training loss: 5.0425\n",
      "Epoch 239 | Training loss: 4.9455\n",
      "Epoch 240 | Training loss: 4.9635\n",
      "Epoch 240 | Testing loss: 2.1675\n",
      "Epoch 241 | Training loss: 4.9158\n",
      "Epoch 242 | Training loss: 4.9113\n",
      "Epoch 243 | Training loss: 4.8962\n",
      "Epoch 244 | Training loss: 4.8476\n",
      "Epoch 245 | Training loss: 4.8346\n",
      "Epoch 245 | Testing loss: 2.1599\n",
      "Epoch 246 | Training loss: 4.8042\n",
      "Epoch 247 | Training loss: 4.7936\n",
      "Epoch 248 | Training loss: 4.7548\n",
      "Epoch 249 | Training loss: 4.7591\n",
      "Epoch 250 | Training loss: 4.7248\n",
      "Epoch 250 | Testing loss: 2.1399\n",
      "Epoch 251 | Training loss: 4.7280\n",
      "Epoch 252 | Training loss: 4.6859\n",
      "Epoch 253 | Training loss: 4.6799\n",
      "Epoch 254 | Training loss: 4.6388\n",
      "Epoch 255 | Training loss: 4.5974\n",
      "Epoch 255 | Testing loss: 2.0810\n",
      "Epoch 256 | Training loss: 4.5597\n",
      "Epoch 257 | Training loss: 4.6075\n",
      "Epoch 258 | Training loss: 4.5313\n",
      "Epoch 259 | Training loss: 4.5417\n",
      "Epoch 260 | Training loss: 4.5549\n",
      "Epoch 260 | Testing loss: 2.1043\n",
      "Epoch 261 | Training loss: 4.5032\n",
      "Epoch 262 | Training loss: 4.4759\n",
      "Epoch 263 | Training loss: 4.4236\n",
      "Epoch 264 | Training loss: 4.4751\n",
      "Epoch 265 | Training loss: 4.4089\n",
      "Epoch 265 | Testing loss: 2.0836\n",
      "Epoch 266 | Training loss: 4.4004\n",
      "Epoch 267 | Training loss: 4.4017\n",
      "Epoch 268 | Training loss: 4.3739\n",
      "Epoch 269 | Training loss: 4.3710\n",
      "Epoch 270 | Training loss: 4.2987\n",
      "Epoch 270 | Testing loss: 2.0555\n",
      "Epoch 271 | Training loss: 4.2748\n",
      "Epoch 272 | Training loss: 4.3644\n",
      "Epoch 273 | Training loss: 4.2717\n",
      "Epoch 274 | Training loss: 4.3119\n",
      "Epoch 275 | Training loss: 4.2609\n",
      "Epoch 275 | Testing loss: 1.9942\n",
      "Epoch 276 | Training loss: 4.2133\n",
      "Epoch 277 | Training loss: 4.2260\n",
      "Epoch 278 | Training loss: 4.2328\n",
      "Epoch 279 | Training loss: 4.1897\n",
      "Epoch 280 | Training loss: 4.1405\n",
      "Epoch 280 | Testing loss: 2.0146\n",
      "Epoch 281 | Training loss: 4.2074\n",
      "Epoch 282 | Training loss: 4.1189\n",
      "Epoch 283 | Training loss: 4.1170\n",
      "Epoch 284 | Training loss: 4.1608\n",
      "Epoch 285 | Training loss: 4.0900\n",
      "Epoch 285 | Testing loss: 2.0047\n",
      "Epoch 286 | Training loss: 4.0965\n",
      "Epoch 287 | Training loss: 4.0757\n",
      "Epoch 288 | Training loss: 4.0963\n",
      "Epoch 289 | Training loss: 4.0089\n",
      "Epoch 290 | Training loss: 4.0359\n",
      "Epoch 290 | Testing loss: 2.0037\n",
      "Training time:227.7762s\n"
     ]
    }
   ],
   "source": [
    "path=root+'data_118_quad/gnn_trained_ac118.pickle'\n",
    "try: \n",
    "  net.load_state_dict(torch.load(path))\n",
    "  print('params loaded')\n",
    "except: \n",
    "  print('cold start')\n",
    "\n",
    "# 0.001\n",
    "lr=0.001\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr)  # 調整 lr 值\n",
    "\n",
    "\n",
    "train_loss=[]\n",
    "val_loss=[]\n",
    "\n",
    "## Training\n",
    "t0=time.time()\n",
    "max_epochs=300\n",
    "eval_epoch=5\n",
    "\n",
    "# earlystopping\n",
    "tolerance=5\n",
    "min_delta=1e-3\n",
    "previous=0\n",
    "\n",
    "# add feasibility \n",
    "feas=True\n",
    "for epoch in range(max_epochs):\n",
    "  # training loop\n",
    "  total_loss=0.0\n",
    "  for local_batch,local_label in train_set:\n",
    "    optimizer.zero_grad() # clear the past gradient\n",
    "    local_batch,local_label=local_batch.to(device),local_label.to(device)\n",
    "    logits=net(local_batch)\n",
    "    loss=my_loss.calc(logits,local_label,local_batch,feas)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    total_loss+=loss.item()\n",
    "  avg_loss=total_loss/len(train_set.dataset)\n",
    "  train_loss.append(avg_loss)\n",
    "  print(\"Epoch %d | Training loss: %.4f\"%(epoch,avg_loss))\n",
    "  # eval\n",
    "  if epoch%eval_epoch==0:\n",
    "    net.eval()\n",
    "    total_loss=0.0\n",
    "    for local_batch,local_label in val_set:\n",
    "      local_batch,local_label=local_batch.to(device),local_label.to(device)\n",
    "      logits=net(local_batch)\n",
    "      loss=my_loss.calc(logits,local_label,local_batch,feas)\n",
    "      total_loss+=loss.item()\n",
    "    avg_loss=total_loss/len(val_set.dataset)\n",
    "    val_loss.append([epoch, avg_loss])\n",
    "    print(\"Epoch %d | Testing loss: %.4f\"%(epoch,avg_loss))\n",
    "    if epoch:\n",
    "      if previous-avg_loss<min_delta: tolerance-=1\n",
    "      if tolerance==0: \n",
    "        break\n",
    "        # pass\n",
    "    previous=avg_loss\n",
    "    net.train()\n",
    "    final_epoch = epoch\n",
    "t1=time.time()\n",
    "print(\"Training time:%.4fs\"%(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "AvUTphUSzqzA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Desktop\\GNN_OPF_electricity_market-main\\GNN_OPF_electricity_market-main\\GNN_OPF_electricity_market-main\\data_gen\\118_data\\FCNN_model\\dnn_02151308.pickle\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "timestamp=datetime.now().strftime('%m%d%H%M')\n",
    "path = 'C:\\\\Users\\\\USER\\\\Desktop\\\\GNN_OPF_electricity_market-main\\\\GNN_OPF_electricity_market-main\\\\GNN_OPF_electricity_market-main\\\\data_gen\\\\118_data\\\\FCNN_model\\\\'\n",
    "\n",
    "path = path+'dnn_%s.pickle'%(timestamp)\n",
    "if feas==False: path.replace('feas','')\n",
    "print(path)\n",
    "torch.save(net.state_dict(),path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "L3M4jmjkscK_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "current_lr = optimizer.param_groups[0]['lr']  # 取得當前學習率\n",
    "\n",
    "val_len = len(val_loss)\n",
    "print(val_len)\n",
    "val_plt = np.zeros((2,val_len))\n",
    "for i in range(val_len):\n",
    "  val_plt[0,i] = val_loss[i][0]\n",
    "  val_plt[1,i] = val_loss[i][1]\n",
    "\n",
    "plt.figure()\n",
    "plot_idx = np.arange(np.size(train_loss))\n",
    "plt.plot(plot_idx[5:-1],train_loss[5:-1],lw=2,label='training loss')\n",
    "plt.plot(val_plt[0,1:],val_plt[1,1:],lw=2,label='validation loss')\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "\n",
    "# 在圖的右上角標示學習率\n",
    "plt.text(0.95, 0.95, f'Learning Rate: {current_lr:.6f}\\nbatch size: {batch_size}', \n",
    "         transform=plt.gca().transAxes, fontsize=10, ha='right', va='top', color='red')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 儲存圖片到指定資料夾（需提供路徑）\n",
    "timestamp=datetime.now().strftime('%m%d%H%M')\n",
    "output_path = r'C:\\Users\\USER\\Desktop\\GNN_OPF_electricity_market-main\\output_loss_fig'\n",
    "output_path = output_path + '/118ac_loss_fig_FR_%s_%s.png' % (str(current_lr), timestamp)\n",
    "plt.savefig(output_path, dpi=300, bbox_inches='tight')  # 儲存為 PNG 格式，解析度 300 dpi\n",
    "\n",
    "\n",
    "plt.show(block=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9uGUm4rsco3"
   },
   "source": [
    "# Evaluate the model w/ validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "4u6001UQ2pN3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset size: torch.Size([2000, 708])\n",
      "Number of validation set:  2000\n",
      "torch.Size([2000, 236])\n"
     ]
    }
   ],
   "source": [
    "n_test = np.size(x_test,0)\n",
    "x_test_feed = torch.from_numpy(x_test).float()\n",
    "x_test_feed = x_test_feed#.transpose(1,2)\n",
    "x_test_feed = x_test_feed.to(device)\n",
    "print('Validation dataset size:',x_test_feed.shape)\n",
    "print('Number of validation set: ',n_test)\n",
    "y_pred = net(x_test_feed)\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnNbSGYoXS3J"
   },
   "source": [
    "* Visualization\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKZQaqwJkn3P"
   },
   "source": [
    " - Visualize errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "7-JCo0KmXwm3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 236) (2000, 236)\n"
     ]
    }
   ],
   "source": [
    "y_pred1 = y_pred.cpu().detach()\n",
    "y_pred1 = torch.squeeze(y_pred1,1).numpy()#.transpose()\n",
    "print(y_test.shape,y_pred1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "NOeXUzrd9h9y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 2, 2000) (2000, 236)\n"
     ]
    }
   ],
   "source": [
    "# x=np.reshape(x,(x.shape[0]*x.shape[1],x.shape[2])) # reshape by samples not dim1\n",
    "# y=np.reshape(y,(y.shape[0]*y.shape[1],y.shape[2]))\n",
    "# print(x_pre.shape,y_pre.shape)\n",
    "\n",
    "y_pred_temp = y_pred1.copy().transpose()\n",
    "# y_pred2=np.reshape(y_pred2,(y_pre.shape[0],y_pre.shape[1],n_test))\n",
    "y_pred2=np.zeros([y.shape[0],y.shape[1],n_test])\n",
    "y_pred2[:,0,:]=y_pred_temp[:n_bus,:]\n",
    "y_pred2[:,1,:]=y_pred_temp[n_bus:,:]\n",
    "print(y_pred2.shape,y_pred1.shape)\n",
    "\n",
    "y_test_temp = y_test.copy().transpose()\n",
    "# y_pred2=np.reshape(y_pred2,(y_pre.shape[0],y_pre.shape[1],n_test))\n",
    "y_test2=np.zeros([y.shape[0],y.shape[1],n_test])\n",
    "y_test2[:,0,:]=y_test_temp[:n_bus,:]\n",
    "y_test2[:,1,:]=y_test_temp[n_bus:,:]\n",
    "y_pred1 = y_pred2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "2eVE-t3nl0Cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 2, 2000) (118, 2, 2000)\n"
     ]
    }
   ],
   "source": [
    "# recover the original p.u. scale\n",
    "# vy_deviation) * vy_scale\n",
    "y_pred1[:,1,:] = y_pred1[:,1,:] / vy_scale + vy_deviation\n",
    "y_test2[:,1,:] = y_test2[:,1,:] / vy_scale + vy_deviation\n",
    "print(y_test2.shape,y_pred1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "FUVl5LXeknC4"
   },
   "outputs": [],
   "source": [
    "n_test = np.size(y_test2,2)\n",
    "err_L2 = np.zeros(n_test)\n",
    "err_Linf = np.zeros(n_test)\n",
    "for i in range(n_test):\n",
    "  err_L2[i] = np.linalg.norm(y_test2[:,0,i] - y_pred1[:,0,i]) / np.linalg.norm(y_test2[:,0,i])\n",
    "  err_Linf[i] = np.max(np.abs(y_test2[:,0,i] - y_pred1[:,0,i])) / np.max(np.abs(y_test2[:,0,i]))\n",
    "\n",
    "err_L2_v = np.zeros(n_test)\n",
    "err_Linf_v = np.zeros(n_test)\n",
    "for i in range(n_test):\n",
    "  err_L2_v[i] = np.linalg.norm(y_test2[:,1,i] - y_pred1[:,1,i]) / np.linalg.norm(y_test2[:,1,i])\n",
    "  err_Linf_v[i] = np.max(np.abs(y_test2[:,1,i] - y_pred1[:,1,i])) / np.max(np.abs(y_test2[:,1,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "79xFCVXklkLb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price L2 mean: 0.05347990957965441 L_inf mean: 0.08817550115114071\n",
      "Voltage L2 mean: 0.01127635148947856 L_inf mean: 0.04627788321325917\n"
     ]
    }
   ],
   "source": [
    "err_L2_mean = np.mean(err_L2)\n",
    "err_Linf_mean = np.mean(err_Linf)\n",
    "print('Price L2 mean:', err_L2_mean,'L_inf mean:', err_Linf_mean )\n",
    "err_L2_mean_v = np.mean(err_L2_v)\n",
    "err_Linf_mean_v = np.mean(err_Linf_v)\n",
    "print('Voltage L2 mean:', err_L2_mean_v,'L_inf mean:', err_Linf_mean_v )\n",
    "\n",
    "# fig2 = plt.figure(figsize=(16, 16))\n",
    "# plt.subplot(2, 2, 1)\n",
    "# # plt.hist(np.abs(ga),bins = 10)\n",
    "# plt.plot(err_L2,'bo',markersize=0.5,label = 'L2 error')\n",
    "# plt.plot(err_Linf,'r^',markersize=0.5,label = 'Linf error')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('sample index')\n",
    "# plt.ylabel('error')\n",
    "# plt.title('Normalized sample error')\n",
    "# plt.grid(True)\n",
    "# # error histogram\n",
    "# plt.subplot(2, 2, 2)\n",
    "# plt.hist(err_L2, bins = 50, facecolor='b', alpha=0.75,label = 'L2 error')\n",
    "# plt.hist(err_Linf, bins = 50, facecolor='g', alpha=0.75,label = 'Linf error')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('sample error value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('Histogram of L_2 error')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)\n",
    "\n",
    "# plt.subplot(2, 2, 3)\n",
    "# # plt.hist(np.abs(ga),bins = 10)\n",
    "# plt.plot(err_L2_v,'bo',markersize=0.5,label = 'L2 error')\n",
    "# plt.plot(err_Linf_v,'r^',markersize=0.5,label = 'Linf error')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('sample index')\n",
    "# plt.ylabel('error')\n",
    "# plt.title('Normalized sample error')\n",
    "# plt.grid(True)\n",
    "# # error histogram\n",
    "# plt.subplot(2, 2, 4)\n",
    "# plt.hist(err_L2_v, bins = 50, facecolor='b', alpha=0.75,label = 'L2 error')\n",
    "# plt.hist(err_Linf_v, bins = 50, facecolor='g', alpha=0.75,label = 'Linf error')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('sample error value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('Histogram of L_2 error')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "6sUddh-uGg_p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 2000) (118, 2000)\n",
      "true range: 1.06 0.94\n",
      "predicted range 1.076542911529541 0.9265808868408203\n"
     ]
    }
   ],
   "source": [
    "print(y_pred1[:,1,:n_test].shape,y_test2[:,1,:n_test].shape)\n",
    "print('true range:',np.max(y_test2[:,1,:n_test]),np.min(y_test2[:,1,:n_test]))\n",
    "print('predicted range',np.max(y_pred1[:,1,:n_test]),np.min(y_pred1[:,1,:n_test]))\n",
    "\n",
    "# fig3 = plt.figure(figsize=(16, 8))\n",
    "# flat_list1 = list(np.concatenate(y_test2[:,1,:n_test]).flat)\n",
    "# flat_list2 = list(np.concatenate(y_pred1[:,1,:n_test]).flat)\n",
    "# plt.hist(flat_list1,bins = 100,label = 'true')\n",
    "\n",
    "# plt.hist(flat_list2,bins = 100,label = 'pred')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "jhfImaPsjKYq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 6, 10000) 10000\n"
     ]
    }
   ],
   "source": [
    "print(x.shape,n_sample)\n",
    "\n",
    "x_new = np.zeros([x.shape[0],x.shape[1],n_sample])\n",
    "for i in range(x.shape[1]):\n",
    "  x_new[:,i,:] = x_total[n_bus*i:n_bus*(i+1),:]\n",
    "\n",
    "y_new = np.zeros([y.shape[0],y.shape[1],n_sample])\n",
    "for i in range(y.shape[1]):\n",
    "  y_new[:,i,:] = y_total[n_bus*i:n_bus*(i+1),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9x7neMNj7n3"
   },
   "source": [
    "# Predict generation using $\\pi$\n",
    "* Using predicted $\\pi$ and find the active constraints in $p_G(i)$\n",
    "* For inactive $p_G(i)$ consider other methods like power flow balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Kr29K04j2KTN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 10000)\n",
      "<class 'numpy.ndarray'> 118 [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117]\n"
     ]
    }
   ],
   "source": [
    "gen_limit0 = x_new[:,4,:].copy() # lin cost\n",
    "print(gen_limit0.shape)\n",
    "\n",
    "gen_idx = []\n",
    "gen_idx = np.arange(n_bus)\n",
    "# for i in range(n_bus):\n",
    "#   if gen_limit0[i,0] > 0:\n",
    "#     gen_idx.append(i)\n",
    "print(type(gen_idx),len(gen_idx),gen_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "gHmx9lMDXzpM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 2, 10000) (236, 10000)\n"
     ]
    }
   ],
   "source": [
    "n_sample=x_total.shape[-1]\n",
    "x_feed = torch.from_numpy(x_total.T).float()\n",
    "y_pred1=net(x_feed.to(device)).cpu().detach().numpy().T\n",
    "y_pred_temp = y_pred1.copy()\n",
    "y_pred2=np.zeros([y.shape[0],y.shape[1],n_sample])\n",
    "y_pred2[:,0,:]=y_pred_temp[:n_bus,:]\n",
    "y_pred2[:,1,:]=y_pred_temp[n_bus:,:]\n",
    "print(y_pred2.shape,y_pred1.shape)\n",
    "y_pred1 = y_pred2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "GPEHF2L91bGv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.600266952514648\n",
      "0.784000000000006\n",
      "5.6458735363288355\n",
      "0.24221453285700786\n"
     ]
    }
   ],
   "source": [
    "gen_cost0 = x_new[:,4,:].copy()\n",
    "lmp_data = y_new[:,0,:].copy()\n",
    "quadratic_a = x_new[:,5,:].copy()\n",
    "profit_pred = y_pred1[:,0,:] - gen_cost0\n",
    "print(np.min(np.abs(profit_pred)))\n",
    "profit_true = lmp_data - gen_cost0\n",
    "print(np.min(np.abs(profit_true)))\n",
    "profit_pred=(y_pred1[:,0,:]-gen_cost0)/(quadratic_a+1e-10)/2\n",
    "profit_true=(lmp_data-gen_cost0)/(quadratic_a+1e-10)/2\n",
    "print(np.min(np.abs(profit_pred)))\n",
    "print(np.min(np.abs(profit_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "uiHWtU5OLc1x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 10000) (118, 10000)\n",
      "0.050201678026626964\n"
     ]
    }
   ],
   "source": [
    "print(profit_pred.shape,profit_true.shape)\n",
    "profit_err = profit_true - profit_pred\n",
    "profit_err_l2 = np.zeros([n_sample,1])\n",
    "\n",
    "for i in range(n_sample):\n",
    "  profit_err_l2[i] = np.linalg.norm(profit_err[:,i])/np.linalg.norm(profit_true[:,i])\n",
    "print(np.mean(profit_err_l2))\n",
    "\n",
    "# fig5 = plt.figure(figsize=(16, 8))\n",
    "# # error histogram\n",
    "# plt.hist(profit_err_l2, bins = 50, facecolor='b', alpha=0.75,label = 'L2 error')\n",
    "# # plt.hist(err_linf_new, bins = 50, facecolor='g', alpha=0.75,label = 'Linf error')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('sample error value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('Histogram of L_2 error')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ZbHchRQd_g8-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1180000,)\n",
      "5.6458735363288355 -2.9391182643149665\n"
     ]
    }
   ],
   "source": [
    "p_pred_sort = np.reshape(profit_pred,n_bus*n_sample)\n",
    "p_true_sort = np.reshape(profit_true,n_bus*n_sample)\n",
    "print(p_pred_sort.shape)\n",
    "print(np.min(p_pred_sort),np.min(p_true_sort))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "TYsSdNGp-OLP"
   },
   "outputs": [],
   "source": [
    "# fig2 = plt.figure(figsize=(8, 8))\n",
    "# plt.hist(p_pred_sort, bins = 50, facecolor='b', alpha=0.75,label = 'pred. profit')\n",
    "# plt.hist(p_true_sort, bins = 50, facecolor='g', alpha=0.75,label = 'true profit')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('profit histogram')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "1J9j5tT9p_f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3623238.3943584417 2764601.835122853\n",
      "3623238.3943584417 43260060.56350001 3623238.3943584417\n"
     ]
    }
   ],
   "source": [
    "# x = [load, gen_cost, gen_lim]\n",
    "binary_thres_true = 1e-5\n",
    "binary_thres = x_new[:,0,:].copy() # upper\n",
    "binary_thres_lo = x_new[:,1,:].copy() # lower\n",
    "gen_pred_binary_full = np.zeros((n_bus,n_sample))\n",
    "gen_true_binary_full = np.zeros((n_bus,n_sample))\n",
    "\n",
    "for i in range(n_sample):\n",
    "  for j in range(len(gen_idx)):\n",
    "    # predicted generator limit\n",
    "    if profit_pred[gen_idx[j],i] > binary_thres[gen_idx[j],i]:\n",
    "      gen_pred_binary_full[gen_idx[j],i] = binary_thres[gen_idx[j],i]\n",
    "    elif profit_pred[gen_idx[j],i] < binary_thres_lo[gen_idx[j],i]:\n",
    "      gen_pred_binary_full[gen_idx[j],i] = binary_thres_lo[gen_idx[j],i]\n",
    "    else:\n",
    "      gen_pred_binary_full[gen_idx[j],i] = profit_pred[gen_idx[j],i]\n",
    "    # true generator limit\n",
    "    if profit_true[gen_idx[j],i] > binary_thres[gen_idx[j],i]:\n",
    "      gen_true_binary_full[gen_idx[j],i] = binary_thres[gen_idx[j],i]\n",
    "    elif profit_true[gen_idx[j],i] < binary_thres_lo[gen_idx[j],i]:\n",
    "      gen_true_binary_full[gen_idx[j],i] = binary_thres_lo[gen_idx[j],i]\n",
    "    else:\n",
    "      gen_true_binary_full[gen_idx[j],i] = profit_true[gen_idx[j],i]\n",
    "\n",
    "gen_inj=gen_pred_binary_full\n",
    "gen_inj_true=gen_true_binary_full\n",
    "# nodal injection\n",
    "load0 = -x_new[:,1,:].copy() # load file\n",
    "p_inj = gen_inj #- load0\n",
    "p_inj_true = gen_inj_true #- load0\n",
    "print(np.sum(p_inj),np.sum(gen_inj_true))\n",
    "print(np.sum(p_inj),np.sum(load0),np.sum(gen_inj))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WAgqdRPjAONm"
   },
   "source": [
    "## injection accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "J46pLAw2AQor"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 10000) (118, 10000)\n",
      "mean p_inj l2 err: 0.08509241366837896\n"
     ]
    }
   ],
   "source": [
    "print(p_inj_true.shape,p_inj.shape)\n",
    "p_inj_true_sort = np.reshape(p_inj_true,n_bus*n_sample)\n",
    "p_inj_sort = np.reshape(p_inj,n_bus*n_sample)\n",
    "p_err = np.zeros(n_sample)\n",
    "for i in range(n_sample):\n",
    "  p_err[i] = np.linalg.norm(p_inj_true[:,i]-p_inj[:,i]) / np.linalg.norm(p_inj_true[:,i])\n",
    "print('mean p_inj l2 err:',np.mean(p_err))\n",
    "# fig3 = plt.figure(figsize=(16, 8))\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.hist(p_inj_sort, bins = 50, facecolor='b', alpha=0.75,label = 'pred. injection')\n",
    "# plt.hist(p_inj_true_sort, bins = 50, facecolor='g', alpha=0.75,label = 'true injection')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('injection histogram')\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.hist(p_err, bins = 50, facecolor='b', alpha=0.75,label = 'injection err')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('error histogram')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)s_binary\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DaN7u4xeGIom"
   },
   "source": [
    "* Calculate flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "YT4sgn_n79MI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(186, 1) (186, 10000) (186, 10000)\n",
      "16302 13334\n",
      "0.008764516129032259 0.007168817204301075\n",
      "186 10000 (186, 10000)\n"
     ]
    }
   ],
   "source": [
    "filename=root+'118ac_fmax.txt'\n",
    "f_max1=pd.read_table(filename,sep=',',header=None).to_numpy() # flow limit\n",
    "\n",
    "n_line = np.size(S_isf,0)\n",
    "flow_est = np.zeros((n_line,n_sample))\n",
    "flow_est0 = np.zeros((n_line,n_sample))\n",
    "\n",
    "f_binary = np.zeros((n_line,n_sample))\n",
    "f_binary0 = np.zeros((n_line,n_sample))\n",
    "\n",
    "# for i in range(n_sample):\n",
    "flow_est = np.dot(S_isf,p_inj)\n",
    "flow_est0 = np.dot(S_isf,p_inj_true)\n",
    "# f_max\n",
    "# f_max_numpy = f_max.cpu().detach().numpy()\n",
    "f_max_numpy = f_max1.copy()\n",
    "f_binary = (np.abs(flow_est)-f_max_numpy > 0)\n",
    "f_binary0 = (np.abs(flow_est0)-f_max_numpy > 0)\n",
    "\n",
    "print(f_max_numpy.shape,flow_est.shape,flow_est0.shape)\n",
    "f_tot_sample = n_line * n_sample\n",
    "print(np.sum(f_binary),np.sum(f_binary0))\n",
    "print(np.sum(f_binary)/f_tot_sample,np.sum(f_binary0)/f_tot_sample)\n",
    "print(n_line,n_sample,flow_est.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "VyvDpKhyQj0M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142.69317394649192 39.64401431411352\n",
      "0.9512878263099461 0.2642934287607568\n"
     ]
    }
   ],
   "source": [
    "# soft threshold\n",
    "f_err_est = np.abs(flow_est)-f_max_numpy\n",
    "f_err_true = np.abs(flow_est0)-f_max_numpy\n",
    "\n",
    "f_err_est = np.maximum(np.abs(flow_est)-f_max_numpy,0) # identify violations\n",
    "f_err_true = np.maximum(np.abs(flow_est0)-f_max_numpy,0)\n",
    "\n",
    "print(np.max(f_err_est),np.max(f_err_true))\n",
    "print(np.max(f_err_est/f_max_numpy),np.max(f_err_true/f_max_numpy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "7iuX7tN2a2Cp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12813 10451\n",
      "0.006888709677419355 0.005618817204301075\n"
     ]
    }
   ],
   "source": [
    "f_binary_soft = (np.abs(flow_est)-f_max_numpy > 0.1*(f_max_numpy))\n",
    "f_binary0_soft = (np.abs(flow_est0)-f_max_numpy > 0.1*(f_max_numpy))\n",
    "print(np.sum(f_binary_soft),np.sum(f_binary0_soft))\n",
    "print(np.sum(f_binary_soft)/f_tot_sample,np.sum(f_binary0_soft)/f_tot_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "SYl9pxnOUUQF"
   },
   "outputs": [],
   "source": [
    "f_pred_sort = np.reshape(f_err_est/f_max_numpy,n_line*n_sample)\n",
    "f_true_sort = np.reshape(f_err_true/f_max_numpy,n_line*n_sample)\n",
    "\n",
    "# fig2 = plt.figure(figsize=(16, 8))\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.hist(f_pred_sort, bins = 10, facecolor='b', alpha=0.75,label = 'pred. f')\n",
    "# plt.hist(f_true_sort, bins = 10, facecolor='g', alpha=0.75,label = 'true f')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('percentage')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('flow violation level histogram')\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.hist(f_true_sort, bins = 10, facecolor='g', alpha=0.75,label = 'true f')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('percentage')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('flow violation level histogram')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "YglgTpWVRLri"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sample pred: 5\n",
      "max line pred: 9999\n",
      "max sample true: 3\n",
      "max line true: 10000\n"
     ]
    }
   ],
   "source": [
    "f_line = np.sum(f_binary,0)\n",
    "f_samp = np.sum(f_binary,1)\n",
    "print('max sample pred:',np.max(f_line))\n",
    "print('max line pred:',np.max(f_samp))\n",
    "\n",
    "f_line0  = np.sum(f_binary0,0)\n",
    "f_samp0 = np.sum(f_binary0,1)\n",
    "print('max sample true:',np.max(f_line0))\n",
    "print('max line true:',np.max(f_samp0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZQnarHmPl35"
   },
   "source": [
    "# Check objective optimality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "BZjhp-CgQaDz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0867215603639917\n"
     ]
    }
   ],
   "source": [
    "gen_cost_pred = np.zeros((n_bus,n_sample))\n",
    "gen_cost_true = np.zeros((n_bus,n_sample))\n",
    "objective_err = np.zeros(n_sample)\n",
    "\n",
    "gen_cost_pred = np.multiply(np.multiply(p_inj,p_inj),quadratic_a) + np.multiply(p_inj,gen_cost0)\n",
    "gen_cost_true = np.multiply(np.multiply(p_inj_true,p_inj_true),quadratic_a) + np.multiply(p_inj_true,gen_cost0)\n",
    "\n",
    "objective_err = np.sum(np.abs(gen_cost_true-gen_cost_pred),axis=0) / np.sum(gen_cost_true,axis=0)\n",
    "print(np.mean(objective_err))\n",
    "\n",
    "# fig6 = plt.figure(figsize=(16, 8))\n",
    "# # error histogram\n",
    "# plt.hist(objective_err, bins = 50, facecolor='b', alpha=0.75,label = 'L2 error')\n",
    "# # plt.hist(err_linf_new, bins = 50, facecolor='g', alpha=0.75,label = 'Linf error')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('sample error value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('Histogram of L_2 error')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdrsAWpYo0-w"
   },
   "source": [
    "## injection accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "mArjSN-So0-x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 10000) (118, 10000)\n",
      "mean p_inj l2 err: 0.08509241366837896\n"
     ]
    }
   ],
   "source": [
    "print(p_inj_true.shape,p_inj.shape)\n",
    "\n",
    "p_inj_true_sort = np.reshape(p_inj_true,n_bus*n_sample)\n",
    "p_inj_sort = np.reshape(p_inj,n_bus*n_sample)\n",
    "\n",
    "p_err = np.zeros(n_sample)\n",
    "for i in range(n_sample):\n",
    "  p_err[i] = np.linalg.norm(p_inj_true[:,i]-p_inj[:,i]) / np.linalg.norm(p_inj_true[:,i])\n",
    "\n",
    "print('mean p_inj l2 err:',np.mean(p_err))\n",
    "# fig3 = plt.figure(figsize=(16, 8))\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.hist(p_inj_sort, bins = 50, facecolor='b', alpha=0.75,label = 'pred. injection')\n",
    "# plt.hist(p_inj_true_sort, bins = 50, facecolor='g', alpha=0.75,label = 'true injection')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('injection histogram')s_binary\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.hist(p_err, bins = 50, facecolor='b', alpha=0.75,label = 'injection err')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('error histogram')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujU84tOSpqsy"
   },
   "source": [
    "# Test AC feasibility\n",
    "* P in actual value, V in p.u.\n",
    "* Use P to recover $\\theta$, or solve $\\theta$ and Q for PF\n",
    "$$ Q_m = V_m \\sum_{n=1}^N V_n \\left(G_{mn}\\sin\\theta_{mn} - B_{mn}\\cos\\theta_{mn} \\right) $$\n",
    "calculate $Q_{mn}$ directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "Of6mEXF4puDN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 118) (118, 118)\n",
      "(117, 117) (118, 10000) (118, 10000)\n",
      "(118, 10000) (118, 10000)\n"
     ]
    }
   ],
   "source": [
    "# Bbus and B_r inverse\n",
    "filename1 = root+'ieee118_Bbus.txt'\n",
    "Bbus=pd.read_table(filename1,sep=',',header=None).to_numpy()\n",
    "B_r = np.delete(Bbus,68,axis=0)\n",
    "B_r = np.delete(B_r,68,axis=1)\n",
    "Br_inv = np.linalg.inv(B_r)\n",
    "\n",
    "# Y = G + jB\n",
    "filename1 = root+'ieee118_Gmat.txt'\n",
    "G_mat=pd.read_table(filename1,sep=',',header=None).to_numpy()\n",
    "filename1 = root+'ieee118_Bmat.txt'\n",
    "B_mat=pd.read_table(filename1,sep=',',header=None).to_numpy()\n",
    "print(G_mat.shape,B_mat.shape)\n",
    "\n",
    "# line parameters\n",
    "filename1 = root+'ieee118_lineloc.txt'\n",
    "line_loc = pd.read_table(filename1,sep=',',header=None).to_numpy()\n",
    "\n",
    "# load line params\n",
    "filename1 = root+'ieee118_lineparams.txt'\n",
    "line_params = pd.read_table(filename1,sep=',',header=None).to_numpy()\n",
    "R_line = line_params[:,0].copy()\n",
    "X_line = line_params[:,1].copy()\n",
    "B_shunt = line_params[:,2].copy()\n",
    "Z_line = R_line + 1j * X_line \n",
    "Y_line = 1 / Z_line\n",
    "G_line = np.real(Y_line)\n",
    "B_line = np.imag(Y_line)\n",
    "# P_inj w/out reference bus in p.u.\n",
    "p_inj_r = np.delete(p_inj,68,axis=0) / 100\n",
    "p_inj_true_r = np.delete(p_inj_true,68,axis=0) / 100\n",
    "p_inj_pu = p_inj / 100\n",
    "p_inj_true_pu = p_inj_true / 100\n",
    "print(Br_inv.shape,p_inj.shape,p_inj_true.shape)#p_inj_true\n",
    "\n",
    "theta0 = np.matmul(Br_inv,p_inj_r)\n",
    "theta_true0 = np.matmul(Br_inv,p_inj_true_r)\n",
    "theta = np.insert(theta0,68,0,axis = 0)\n",
    "theta_true = np.insert(theta_true0,68,0,axis = 0)\n",
    "print(theta.shape,theta_true.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4600847652662442 -0.658688207485256\n",
      "2.7803011534120627 -9.166735486002148\n"
     ]
    }
   ],
   "source": [
    "print(np.max(theta),np.min(theta))\n",
    "math.sin(math.pi/6)\n",
    "print(G_line[0],B_line[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(186, 10000)\n",
      "1.0767918968200685 0.9261270570755005 (118, 10000)\n"
     ]
    }
   ],
   "source": [
    "# Calculate real and reactive flow\n",
    "f_p = np.zeros((n_line,n_sample))\n",
    "f_q = np.zeros((n_line,n_sample))\n",
    "fji_p = np.zeros((n_line,n_sample))\n",
    "fji_q = np.zeros((n_line,n_sample))\n",
    "print(f_q.shape)\n",
    "\n",
    "v_pred = y_pred1[:,1,:].copy()\n",
    "v_pred = v_pred / vy_scale + vy_deviation\n",
    "print(np.max(v_pred),np.min(v_pred),v_pred.shape)\n",
    "\n",
    "theta1 = theta[line_loc[:,0]-1,:]\n",
    "theta2 = theta[line_loc[:,1]-1,:]\n",
    "V1 = v_pred[line_loc[:,0]-1,:]\n",
    "V2 = v_pred[line_loc[:,1]-1,:] \n",
    "f_p=(a*G_line*(V1*V1).T)-a*((V1*V2).T)*(G_line*np.cos(theta1-theta2).T+B_line*np.sin(theta1-theta2).T)\n",
    "f_p=f_p.T\n",
    "f_q=-a*(V1.T)*(a*V1.T)*(B_line+B_shunt/2)+a*((V1*V2).T)*(B_line*np.cos(theta1-theta2).T-G_line*np.sin(theta1-theta2).T)\n",
    "f_q=f_q.T\n",
    "\n",
    "theta1 = theta[line_loc[:,1]-1,:]\n",
    "theta2 = theta[line_loc[:,0]-1,:]\n",
    "V1 = v_pred[line_loc[:,1]-1,:]\n",
    "V2 = v_pred[line_loc[:,0]-1,:]\n",
    "fji_p=(a*G_line*(V1*V1).T)-a*((V1*V2).T)*(G_line*np.cos(theta1-theta2).T+B_line*np.sin(theta1-theta2).T)\n",
    "fji_p=fji_p.T\n",
    "fji_q=-a*(V1.T)*(a*V1.T)*(B_line+B_shunt/2)+a*((V1*V2).T)*(B_line*np.cos(theta1-theta2).T-G_line*np.sin(theta1-theta2).T)\n",
    "fji_q=fji_q.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "gKfcrbTSVMeO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.179336800411875 -12.088389203071927\n",
      "18.06275263509782 151.0\n"
     ]
    }
   ],
   "source": [
    "s_pred = np.sqrt(f_p*f_p+f_q*f_q)*100\n",
    "sji_pred = np.sqrt(fji_p*fji_p+fji_q*fji_q)*100\n",
    "print(np.max(f_q),np.min(f_q))\n",
    "flow_est.shape\n",
    "print(np.mean(s_pred[0,:]),np.mean(f_max_numpy[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "YO4__LJ2brLu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48586\n",
      "hard violation rate: 0.026121505376344088\n",
      "38928\n",
      "0.020929032258064514\n"
     ]
    }
   ],
   "source": [
    "sij_binary = (np.abs(s_pred)-f_max_numpy[:n_line] > 0)\n",
    "sji_binary = (np.abs(sji_pred)-f_max_numpy[:n_line] > 0)\n",
    "s_binary = np.maximum(sij_binary,sji_binary)\n",
    "print(np.sum(s_binary))#,np.sum(f_binary0))\n",
    "print('hard violation rate:',np.sum(s_binary)/n_sample/n_line)#,np.sum(f_binary0)/f_tot_sample)\n",
    "s_binary_soft = (np.abs(s_pred)-f_max_numpy[:n_line] > 0.1*(f_max_numpy[:n_line]))\n",
    "print(np.sum(s_binary_soft))#,np.sum(f_binary0_soft))\n",
    "print(np.sum(s_binary_soft)/n_sample/n_line)#,np.sum(f_binary0_soft)/f_tot_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "Ty0WpBdfJwMR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S violation level:\n",
      "hard: 0.026121505376344088\n",
      "mean: 0.012495652129890194\n",
      "median: 0.0\n",
      "max: 2.620544620317131\n",
      "std: 0.10226484951271217\n",
      "p99: 0.5116603713072696\n",
      "f violation level:\n",
      "hard: 0.008764516129032259 0.007168817204301075\n",
      "mean: 0.0028822837585005755\n",
      "median: 0.0\n",
      "max: 0.9512878263099461\n",
      "std: 0.03835889619434495\n",
      "p99: 0.0\n"
     ]
    }
   ],
   "source": [
    "# violation level\n",
    "sij_violation = np.abs(s_pred)-f_max_numpy[:n_line] #/ f_max_numpy\n",
    "sij_violation_level = np.maximum(sij_violation,0)\n",
    "sji_violation = np.abs(sji_pred)-f_max_numpy[:n_line] #/ f_max_numpy\n",
    "sji_violation_level = np.maximum(sji_violation,0)\n",
    "s_violation_level = np.maximum(sij_violation_level,sji_violation_level)\n",
    "s_violation_level = np.divide(s_violation_level,f_max_numpy[:n_line])\n",
    "s_vio_lvl = np.reshape(s_violation_level,n_line*n_sample)\n",
    "\n",
    "print('S violation level:')\n",
    "print('hard:',np.sum(s_binary)/f_tot_sample)\n",
    "print('mean:',np.mean(s_vio_lvl))\n",
    "print('median:',np.median(s_vio_lvl))\n",
    "print('max:',np.max(s_vio_lvl))\n",
    "print('std:',np.std(s_vio_lvl))\n",
    "print('p99:',np.percentile(s_vio_lvl,99))\n",
    "\n",
    "f_violation = np.abs(flow_est)-f_max_numpy #/ f_max_numpy\n",
    "f_violation_level = np.maximum(f_violation,0)\n",
    "f_violation_level = np.divide(f_violation_level,f_max_numpy)\n",
    "f_vio_lvl = np.reshape(f_violation_level,n_line*n_sample)\n",
    "\n",
    "print('f violation level:')\n",
    "print('hard:',np.sum(f_binary)/f_tot_sample,np.sum(f_binary0)/f_tot_sample)\n",
    "print('mean:',np.mean(f_vio_lvl))\n",
    "print('median:',np.median(f_vio_lvl))\n",
    "print('max:',np.max(f_vio_lvl))\n",
    "print('std:',np.std(f_vio_lvl))\n",
    "print('p99:',np.percentile(f_vio_lvl,99))\n",
    "\n",
    "# fig4 = plt.figure(figsize=(6,4))\n",
    "# plt.hist(s_vio_lvl, bins = 50, facecolor='b', alpha=0.75,label = 's violation')\n",
    "# plt.hist(f_vio_lvl, bins = 50, facecolor='r', alpha=0.75,label = 'f violation')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('violation level')\n",
    "# plt.ylabel('frequency')\n",
    "# # plt.title('injection histogram')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "kWXEpj-ryjbJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price L2 mean: 0.05347990957965441 L_inf mean: 0.08817550115114071\n",
      "std: 0.030964873101308115\n",
      "Voltage L2 mean: 0.01127635148947856 L_inf mean: 0.04627788321325917\n",
      "std: 0.0016307353135002528\n"
     ]
    }
   ],
   "source": [
    "# err_L2_mean = np.mean(err_L2)\n",
    "# err_Linf_mean = np.mean(err_Linf)\n",
    "print('Price L2 mean:', err_L2_mean,'L_inf mean:', err_Linf_mean )\n",
    "print('std:',np.std(err_L2))\n",
    "# err_L2_mean_v = np.mean(err_L2_v)\n",
    "# err_Linf_mean_v = np.mean(err_Linf_v)\n",
    "print('Voltage L2 mean:', err_L2_mean_v,'L_inf mean:', err_Linf_mean_v )\n",
    "print('std:',np.std(err_L2_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "輸出內容已儲存到 C:\\Users\\USER\\Desktop\\GNN_OPF_electricity_market-main\\output_loss_fig/118ac_FR_output_02151315.txt\n"
     ]
    }
   ],
   "source": [
    "import os   \n",
    "output_content = f\"\"\"\n",
    "date: {timestamp}\n",
    "final_epoch: {final_epoch}\n",
    "training time: {t1 - t0:.4e}s\n",
    "\n",
    "start learning rate: {lr:.4e}\n",
    "final learning rate: {current_lr:.4e}\n",
    "final learning rate: {current_lr}\n",
    "batch size: {batch_size}\n",
    "\n",
    "Price L2 mean: {err_L2_mean:.4e} \n",
    "Price std: {np.std(err_L2):.4e}\n",
    "\n",
    "Voltage L2 mean: {err_L2_mean_v:.4e} \n",
    "Voltage std: {np.std(err_L2_v):.4e}\n",
    "\"\"\"\n",
    "\n",
    "# 設定儲存路徑和檔名\n",
    "output_dir = r'C:\\Users\\USER\\Desktop\\GNN_OPF_electricity_market-main\\output_loss_fig'\n",
    "timestamp = datetime.now().strftime('%m%d%H%M')\n",
    "output_file = f'{output_dir}/118ac_FR_output_{timestamp}.txt'\n",
    "\n",
    "# 確保目錄存在\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 將內容寫入 txt 檔案\n",
    "with open(output_file, 'w') as file:\n",
    "    file.write(output_content)\n",
    "\n",
    "print(f\"輸出內容已儲存到 {output_file}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "118ac_feasdnn0417.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "AC_OPF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
