{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JKLovDeXoCCP"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import math\n",
    "from datetime import datetime\n",
    "root=''\n",
    "# try:\n",
    "#   from google.colab import drive\n",
    "#   drive.mount('/content/drive')\n",
    "#   root='./drive/MyDrive/gnn/data/'\n",
    "# except:\n",
    "#   pass\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# print(torch.cuda.get_device_name(0))\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Hj9_eLfoWQY3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([186, 118]) torch.Size([186, 1]) tensor(72)\n",
      "1654.8 -332.4 949.67 0.94\n",
      "voltage range(scaled): 3.9999999999999925 16.000000000000004\n",
      "price range old: 11.547 949.67\n",
      "voltage range old: 3.9999999999999925 16.000000000000004\n",
      "price range new: 11.547 949.67\n",
      "voltage range new: 3.9999999999999925 16.000000000000004\n",
      "(118, 6, 10000) (118, 2, 10000)\n"
     ]
    }
   ],
   "source": [
    "root = 'C:/Users/USER/Desktop/GNN_OPF_electricity_market-main/GNN_OPF_electricity_market-main/GNN_OPF_electricity_market-main/data_gen/118_data/'\n",
    "# C:\\Users\\USER\\Desktop\\GNN_OPF_electricity_market-main\\GNN_OPF_electricity_market-main\\GNN_OPF_electricity_market-main\\data_gen\\118_data\\118dc_quad_ISF.txt\n",
    "filename = root + '118dc_quad_ISF.txt'\n",
    "\n",
    "S_isf=pd.read_table(filename,sep=',',header=None).to_numpy() # ISF matrix\n",
    "filename=root+'118ac_fmax.txt'\n",
    "f_max=pd.read_table(filename,sep=',',header=None).to_numpy() # flow limit\n",
    "n_line = np.size(S_isf,0)\n",
    "S = torch.from_numpy(S_isf).to(device) # ISF\n",
    "f_max = torch.from_numpy(f_max).to(device) # flow limit\n",
    "print(S.shape,f_max.shape,torch.min(f_max))\n",
    "\n",
    "x=np.load(root+'ac118_p10_x_v.npy')\n",
    "y=np.load(root+'ac118_p10_y_v.npy')\n",
    "W=np.load(root+'ac118_p10_w.npy')\n",
    "print(np.max(x),np.min(x),np.max(y),np.min(y))\n",
    "\n",
    "# scaling on voltage\n",
    "vy_deviation = 0.9\n",
    "vy_scale = 100\n",
    "y[:,1,:] = (y[:,1,:] - vy_deviation) * vy_scale\n",
    "print('voltage range(scaled):',np.min(y[:,1,:]),np.max(y[:,1,:]))\n",
    "\n",
    "# scaling on price\n",
    "pi_deviation = 0\n",
    "y[:,0,:] = y[:,0,:] + pi_deviation\n",
    "# filter out extreme points in price\n",
    "y_sort_arg = np.argsort(np.amax(np.abs(y[:,0,:]),axis=0)) # max extreme\n",
    "y_sort_arg1 = np.argsort(np.amin(y[:,0,:],axis=0),axis=0) # min extreme\n",
    "\n",
    "del_idx0 = []\n",
    "del_num = 0\n",
    "for i in range(del_num):\n",
    "  del_idx0.append(y_sort_arg[-i])\n",
    "  del_idx0.append(y_sort_arg1[i])\n",
    "# print(del_idx0)\n",
    "del_idx = [] # keep only non-repeated\n",
    "[del_idx.append(x) for x in del_idx0 if x not in del_idx]\n",
    "del_idx = np.sort(del_idx)\n",
    "# delete extreme points\n",
    "print('price range old:',np.min(y[:,0,:]),np.max(y[:,0,:]))\n",
    "print('voltage range old:',np.min(y[:,1,:]),np.max(y[:,1,:]))\n",
    "# x = np.delete(x, del_idx, axis=2)\n",
    "# y = np.delete(y, del_idx, axis=2)\n",
    "print('price range new:',np.min(y[:,0,:]),np.max(y[:,0,:]))\n",
    "print('voltage range new:',np.min(y[:,1,:]),np.max(y[:,1,:]))\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YcR42RBbdZqZ"
   },
   "outputs": [],
   "source": [
    "n_sample=y.shape[-1]\n",
    "n_bus=y.shape[0]\n",
    "x_total=x.transpose((1,0,2)).reshape(-1,x.shape[-1])\n",
    "y_total=y.transpose((1,0,2)).reshape(-1,y.shape[-1])\n",
    "x_train,x_test,y_train,y_test=train_test_split(x_total.T,y_total.T,test_size=0.2)\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        self.x=torch.from_numpy(x).float()\n",
    "        self.y=torch.from_numpy(y).float()\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "          idx=idx.tolist()\n",
    "        return self.x[idx],self.y[idx]\n",
    "params={'batch_size': 512,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 0}\n",
    "train=Dataset(x_train,y_train)\n",
    "train_set=torch.utils.data.DataLoader(train,**params)\n",
    "val=Dataset(x_test,y_test)\n",
    "val_set=torch.utils.data.DataLoader(val,**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "JN9gN9BnCNim"
   },
   "outputs": [],
   "source": [
    "# fig2 = plt.figure(figsize=(8,4))\n",
    "# flat_list = list(np.concatenate(y[:,n_sample:]).flat)\n",
    "# flat_list3 = list(np.concatenate(y[:,:n_sample]).flat)\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.hist(flat_list,bins = 100,label = 'voltage')\n",
    "# plt.subplot(1,2,2)\n",
    "# # plt.hist(flat_list3,range=[-2000, 2000],bins = 100,label = 'price')\n",
    "# plt.hist(flat_list3,bins = 100,label = 'price')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "iOyWpG_4yCtz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params: 5935636\n"
     ]
    }
   ],
   "source": [
    "class dnn(torch.nn.Module):\n",
    "  def __init__(self,shape):\n",
    "    super(dnn,self).__init__()\n",
    "    layers=[]\n",
    "    for idx in range(len(shape)-2):\n",
    "      layers.extend([\n",
    "        nn.Linear(shape[idx],shape[idx+1]),\n",
    "        nn.BatchNorm1d(shape[idx+1]),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "      ])\n",
    "    layers+=[nn.Linear(shape[-2],shape[-1])]\n",
    "    self.features=nn.Sequential(*layers)\n",
    "    for temp in self.features:\n",
    "      if type(temp)==nn.Linear:\n",
    "        torch.nn.init.normal_(temp.weight,mean=0,std=1)\n",
    "  def forward(self,x): return self.features(x)\n",
    "# net=dnn([n_bus*6,n_bus*10,n_bus*10,n_bus*10,n_bus*2]).to(device)\n",
    "net = dnn([n_bus*6, n_bus*5, n_bus*5, n_bus*10, n_bus*10, n_bus*10, n_bus*10, n_bus*2]).to(device)#my code\n",
    "\n",
    "print('number of params: %d'%(sum(temp.numel() for temp in net.parameters() if temp.requires_grad)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "GEQ-MDKOTqW1"
   },
   "outputs": [],
   "source": [
    "# threshold function for p_g\n",
    "class my_gen_pred_binary(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(my_gen_pred_binary,self).__init__()\n",
    "  def forward(self,x,thresh):\n",
    "    right_thresh=thresh.clone().detach().requires_grad_(True).double()\n",
    "    left_thresh=torch.tensor(0).double()\n",
    "    x=x.double()\n",
    "    output = torch.sigmoid(left_thresh - x)\n",
    "    output = torch.mul(output,left_thresh - x) + x\n",
    "    output = torch.sigmoid(output - right_thresh)\n",
    "    output = torch.mul(output,output - right_thresh) + right_thresh\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9mYCMszHaosA"
   },
   "outputs": [],
   "source": [
    "## params needed for S calculation\n",
    "# line parameters\n",
    "filename1 = root+'ieee118_lineloc.txt'\n",
    "filename2 = root+'ieee118_lineparams.txt'\n",
    "filename3 = root+'ieee118_Bmat.txt'\n",
    "# incidence info\n",
    "line_loc = pd.read_table(filename1,sep=',',header=None).to_numpy()\n",
    "# r, x, shunt, S_max\n",
    "line_params = pd.read_table(filename2,sep=',',header=None).to_numpy()\n",
    "B_mat=pd.read_table(filename3,sep=',',header=None).to_numpy()\n",
    "B_r = np.delete(B_mat,68,axis=0)\n",
    "B_r = np.delete(B_r,68,axis=1)\n",
    "Br_inv = np.linalg.inv(B_r)\n",
    "\n",
    "R_line = line_params[:,0].copy()\n",
    "X_line = line_params[:,1].copy()\n",
    "\n",
    "B_shunt = line_params[:,2].copy()\n",
    "\n",
    "Z_line = R_line + 1j * X_line \n",
    "Y_line = 1 / Z_line\n",
    "\n",
    "G_line = np.real(Y_line)\n",
    "B_line = np.imag(Y_line)\n",
    "\n",
    "# transformer indicator\n",
    "a = (R_line > 0).astype(int)\n",
    "\n",
    "# params to tensor and GPU\n",
    "G_line_tensor = torch.from_numpy(G_line).to(device) # conductance\n",
    "B_line_tensor = torch.from_numpy(B_line).to(device) # susceptance\n",
    "B_shunt_tensor = torch.from_numpy(B_shunt/2).to(device) # conductance\n",
    "Br_inv_tensor = torch.from_numpy(Br_inv).to(device) # reduced Bbus matrix\n",
    "a_tensor = torch.from_numpy(a).double().to(device) # line/transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Go4bwoEmoi0D"
   },
   "outputs": [],
   "source": [
    "class loss_func:\n",
    "    def __init__(self,s_max,G_line,B_line,B_shunt,Br_inv,a,line_loc):\n",
    "      self.s=s_max\n",
    "      self.g=G_line\n",
    "      self.b=B_line\n",
    "      self.c=B_shunt\n",
    "      self.r=Br_inv\n",
    "      self.a=a\n",
    "      self.mse=nn.MSELoss() # MSE loss\n",
    "      self.lmda1=torch.tensor(10).to(device) # V MSE \n",
    "      self.lmda2=torch.tensor(1).to(device) # pi MSE \n",
    "      self.lmda3=torch.tensor(0.1).to(device) # v l_inf\n",
    "      self.lmda4=torch.tensor(0.1).to(device) # s feasibility\n",
    "      self.lmda5=torch.tensor(0.01).to(device) # pi l_inf\n",
    "      self.line_loc=line_loc\n",
    "      self.binary_cell=my_gen_pred_binary()\n",
    "    def calc(self,pred,label,x,feas):\n",
    "      mse_p=self.mse(pred[:,:n_bus],label[:,:n_bus])\n",
    "      mse_v=self.mse(pred[:,n_bus:],label[:,n_bus:])\n",
    "      linf_p=(pred[:,:n_bus]-label[:,:n_bus]).norm(p=float('inf'))\n",
    "      linf_v=(pred[:,n_bus:]-label[:,n_bus:]).norm(p=float('inf'))\n",
    "      if feas==False:\n",
    "        return self.lmda1*mse_v+self.lmda2*mse_p+self.lmda3*linf_v+self.lmda5*linf_p\n",
    "      label_pred=pred[:,:n_bus]\n",
    "      p_max=x[:,:n_bus*1]-x[:,n_bus*1:n_bus*2]\n",
    "      quadratic_b=x[:,n_bus*4:n_bus*5]\n",
    "      quadratic_a=x[:,n_bus*5:n_bus*6]\n",
    "      quadratic_center=(label_pred-quadratic_b)/(quadratic_a+1e-5)/2\n",
    "      p_inj=self.binary_cell(quadratic_center,p_max)\n",
    "      bus_inj=p_inj+x[:,n_bus:n_bus*2]\n",
    "      p_inj_r=torch.cat((bus_inj[:,:68],bus_inj[:,69:]),1)/100\n",
    "      theta0=torch.matmul(self.r,p_inj_r.T)\n",
    "      ref_ang=torch.zeros(1,theta0.shape[1]).to(device)\n",
    "      theta=torch.cat([theta0[:68,:],ref_ang,theta0[68:,:]],0)\n",
    "      v_pred=(pred[:,n_bus:].transpose(0,1))*0.01+0.9\n",
    "      \n",
    "      # s penalty\n",
    "      theta1=theta[self.line_loc[:,0]-1,:]\n",
    "      theta2=theta[self.line_loc[:,1]-1,:]\n",
    "      V1=(v_pred[self.line_loc[:,0]-1,:]).double()\n",
    "      V2=(v_pred[self.line_loc[:,1]-1,:]).double()\n",
    "      f_p=(self.a*self.g*(V1*V1).T)-self.a*((V1*V2).T)*(self.g*torch.cos(theta1-theta2).T+self.b*torch.sin(theta1-theta2).T)\n",
    "      f_p=f_p.T\n",
    "      f_q=-self.a*(V1.T)*(self.a*V1.T)*(self.b+self.c/2)+self.a*((V1*V2).T)*(self.b*torch.cos(theta1-theta2).T-self.g*torch.sin(theta1-theta2).T)\n",
    "      f_q=f_q.T\n",
    "      s_pred=torch.sqrt(f_p*f_p+f_q*f_q+1e-5)*100\n",
    "      s_penalty=torch.sigmoid(s_pred-self.s)+torch.sigmoid(-s_pred-self.s)\n",
    "      s_total=torch.sum(s_penalty)\n",
    "\n",
    "      # sji penalty\n",
    "      theta1=theta[self.line_loc[:,1]-1,:]\n",
    "      theta2=theta[self.line_loc[:,0]-1,:]\n",
    "      V1=(v_pred[self.line_loc[:,1]-1,:]).double()\n",
    "      V2=(v_pred[self.line_loc[:,0]-1,:]).double() \n",
    "      fji_p=(self.a*self.g*(V1*V1).T)-self.a*((V1*V2).T)*(self.g*torch.cos(theta1-theta2).T+self.b*torch.sin(theta1-theta2).T)\n",
    "      fji_p=fji_p.T\n",
    "      fji_q=-self.a*(V1.T)*(self.a*V1.T)*(self.b+self.c/2)+self.a*((V1*V2).T)*(self.b*torch.cos(theta1-theta2).T-self.g*torch.sin(theta1-theta2).T)\n",
    "      fji_q=fji_q.T\n",
    "      sji_pred=torch.sqrt(fji_p*fji_p+fji_q*fji_q+1e-5)*100\n",
    "      sji_penalty=torch.sigmoid(sji_pred-self.s)+torch.sigmoid(-sji_pred-self.s)\n",
    "      sji_total=torch.sum(sji_penalty)\n",
    "\n",
    "      return self.lmda1*mse_v+self.lmda2*mse_p+self.lmda3*linf_v+self.lmda5*linf_p+self.lmda4*s_total+self.lmda4*sji_total\n",
    "my_loss=loss_func(f_max,G_line_tensor,B_line_tensor,B_shunt_tensor,Br_inv_tensor,a_tensor,line_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iu0PKvcgF2vN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cold start\n",
      "Epoch 0 | Training loss: 429.9530\n",
      "Epoch 0 | Testing loss: 405.6191\n",
      "Epoch 1 | Training loss: 412.4427\n",
      "Epoch 2 | Training loss: 397.0423\n",
      "Epoch 3 | Training loss: 382.5367\n",
      "Epoch 4 | Training loss: 368.3546\n",
      "Epoch 5 | Training loss: 354.2572\n",
      "Epoch 5 | Testing loss: 361.2104\n",
      "Epoch 6 | Training loss: 339.9227\n",
      "Epoch 7 | Training loss: 325.3085\n",
      "Epoch 8 | Training loss: 310.3448\n",
      "Epoch 9 | Training loss: 294.8542\n",
      "Epoch 10 | Training loss: 278.9551\n",
      "Epoch 10 | Testing loss: 283.4766\n",
      "Epoch 11 | Training loss: 262.7273\n",
      "Epoch 12 | Training loss: 246.5282\n",
      "Epoch 13 | Training loss: 229.9685\n",
      "Epoch 14 | Training loss: 213.5921\n",
      "Epoch 15 | Training loss: 197.7672\n",
      "Epoch 15 | Testing loss: 179.9912\n",
      "Epoch 16 | Training loss: 181.8691\n",
      "Epoch 17 | Training loss: 166.6963\n",
      "Epoch 18 | Training loss: 152.3884\n",
      "Epoch 19 | Training loss: 138.2755\n",
      "Epoch 20 | Training loss: 125.5983\n",
      "Epoch 20 | Testing loss: 84.9710\n",
      "Epoch 21 | Training loss: 113.9284\n",
      "Epoch 22 | Training loss: 103.1302\n",
      "Epoch 23 | Training loss: 93.3706\n",
      "Epoch 24 | Training loss: 84.6983\n",
      "Epoch 25 | Training loss: 77.2902\n",
      "Epoch 25 | Testing loss: 29.6417\n",
      "Epoch 26 | Training loss: 70.6718\n",
      "Epoch 27 | Training loss: 65.0712\n",
      "Epoch 28 | Training loss: 60.2263\n",
      "Epoch 29 | Training loss: 56.2000\n",
      "Epoch 30 | Training loss: 52.6214\n",
      "Epoch 30 | Testing loss: 9.4747\n",
      "Epoch 31 | Training loss: 49.7615\n",
      "Epoch 32 | Training loss: 47.0318\n",
      "Epoch 33 | Training loss: 44.7858\n",
      "Epoch 34 | Training loss: 42.9847\n",
      "Epoch 35 | Training loss: 41.2975\n",
      "Epoch 35 | Testing loss: 4.6812\n",
      "Epoch 36 | Training loss: 39.8776\n",
      "Epoch 37 | Training loss: 38.3751\n",
      "Epoch 38 | Training loss: 37.1891\n",
      "Epoch 39 | Training loss: 36.2092\n",
      "Epoch 40 | Training loss: 35.1156\n",
      "Epoch 40 | Testing loss: 3.6542\n",
      "Epoch 41 | Training loss: 34.1122\n",
      "Epoch 42 | Training loss: 33.1830\n",
      "Epoch 43 | Training loss: 32.4151\n",
      "Epoch 44 | Training loss: 31.5172\n",
      "Epoch 45 | Training loss: 30.7159\n",
      "Epoch 45 | Testing loss: 3.3567\n",
      "Epoch 46 | Training loss: 29.9415\n",
      "Epoch 47 | Training loss: 29.3447\n",
      "Epoch 48 | Training loss: 28.7147\n",
      "Epoch 49 | Training loss: 27.9586\n",
      "Epoch 50 | Training loss: 27.3812\n",
      "Epoch 50 | Testing loss: 3.3352\n",
      "Epoch 51 | Training loss: 26.7348\n",
      "Epoch 52 | Training loss: 26.1200\n",
      "Epoch 53 | Training loss: 25.5374\n",
      "Epoch 54 | Training loss: 25.0465\n",
      "Epoch 55 | Training loss: 24.5226\n",
      "Epoch 55 | Testing loss: 3.4148\n",
      "Epoch 56 | Training loss: 24.0527\n",
      "Epoch 57 | Training loss: 23.5131\n",
      "Epoch 58 | Training loss: 23.0173\n",
      "Epoch 59 | Training loss: 22.5419\n",
      "Epoch 60 | Training loss: 22.0758\n",
      "Epoch 60 | Testing loss: 3.3370\n",
      "Epoch 61 | Training loss: 21.7299\n",
      "Epoch 62 | Training loss: 21.3154\n",
      "Epoch 63 | Training loss: 20.8690\n",
      "Epoch 64 | Training loss: 20.5405\n",
      "Epoch 65 | Training loss: 20.1021\n",
      "Epoch 65 | Testing loss: 3.5140\n",
      "Epoch 66 | Training loss: 19.7744\n",
      "Epoch 67 | Training loss: 19.3750\n",
      "Epoch 68 | Training loss: 19.0615\n",
      "Epoch 69 | Training loss: 18.6590\n",
      "Epoch 70 | Training loss: 18.3034\n",
      "Epoch 70 | Testing loss: 3.6078\n",
      "Epoch 71 | Training loss: 18.1259\n",
      "Epoch 72 | Training loss: 17.7726\n",
      "Epoch 73 | Training loss: 17.4480\n",
      "Epoch 74 | Training loss: 17.0990\n",
      "Epoch 75 | Training loss: 16.8669\n",
      "Epoch 75 | Testing loss: 3.7628\n",
      "Epoch 76 | Training loss: 16.5480\n",
      "Epoch 77 | Training loss: 16.2891\n",
      "Epoch 78 | Training loss: 16.0760\n",
      "Epoch 79 | Training loss: 15.7465\n",
      "Epoch 80 | Training loss: 15.5369\n",
      "Epoch 80 | Testing loss: 3.8234\n",
      "Epoch 81 | Training loss: 15.2806\n",
      "Epoch 82 | Training loss: 15.0997\n",
      "Epoch 83 | Training loss: 14.8137\n",
      "Epoch 84 | Training loss: 14.5809\n",
      "Epoch 85 | Training loss: 14.2922\n",
      "Epoch 85 | Testing loss: 3.8208\n",
      "Epoch 86 | Training loss: 14.0941\n",
      "Epoch 87 | Training loss: 13.8796\n",
      "Epoch 88 | Training loss: 13.6748\n",
      "Epoch 89 | Training loss: 13.4998\n",
      "Epoch 90 | Training loss: 13.3131\n",
      "Epoch 90 | Testing loss: 3.5114\n",
      "Epoch 91 | Training loss: 13.0937\n",
      "Epoch 92 | Training loss: 12.8562\n",
      "Epoch 93 | Training loss: 12.7379\n",
      "Epoch 94 | Training loss: 12.5533\n",
      "Epoch 95 | Training loss: 12.3241\n",
      "Epoch 95 | Testing loss: 3.7174\n",
      "Epoch 96 | Training loss: 12.2339\n",
      "Epoch 97 | Training loss: 12.0194\n",
      "Epoch 98 | Training loss: 11.8187\n",
      "Epoch 99 | Training loss: 11.6635\n",
      "Epoch 100 | Training loss: 11.4858\n",
      "Epoch 100 | Testing loss: 3.6737\n",
      "Epoch 101 | Training loss: 11.3803\n",
      "Epoch 102 | Training loss: 11.1379\n",
      "Epoch 103 | Training loss: 11.0828\n",
      "Epoch 104 | Training loss: 10.8566\n",
      "Epoch 105 | Training loss: 10.7039\n",
      "Epoch 105 | Testing loss: 3.5021\n",
      "Epoch 106 | Training loss: 10.5975\n",
      "Epoch 107 | Training loss: 10.4750\n",
      "Epoch 108 | Training loss: 10.2815\n",
      "Epoch 109 | Training loss: 10.1732\n",
      "Epoch 110 | Training loss: 10.0603\n",
      "Epoch 110 | Testing loss: 3.3421\n",
      "Epoch 111 | Training loss: 9.9400\n",
      "Epoch 112 | Training loss: 9.8313\n",
      "Epoch 113 | Training loss: 9.6043\n",
      "Epoch 114 | Training loss: 9.5656\n",
      "Epoch 115 | Training loss: 9.4685\n",
      "Epoch 115 | Testing loss: 3.3155\n",
      "Epoch 116 | Training loss: 9.3314\n",
      "Epoch 117 | Training loss: 9.2548\n",
      "Epoch 118 | Training loss: 9.0608\n",
      "Epoch 119 | Training loss: 9.1172\n",
      "Epoch 120 | Training loss: 8.8911\n",
      "Epoch 120 | Testing loss: 3.0108\n",
      "Epoch 121 | Training loss: 8.7232\n",
      "Epoch 122 | Training loss: 8.6459\n",
      "Epoch 123 | Training loss: 8.5740\n",
      "Epoch 124 | Training loss: 8.4394\n",
      "Epoch 125 | Training loss: 8.3541\n",
      "Epoch 125 | Testing loss: 3.1314\n",
      "Epoch 126 | Training loss: 8.2709\n",
      "Epoch 127 | Training loss: 8.1171\n",
      "Epoch 128 | Training loss: 8.0406\n",
      "Epoch 129 | Training loss: 7.9421\n",
      "Epoch 130 | Training loss: 7.9583\n",
      "Epoch 130 | Testing loss: 3.0397\n",
      "Epoch 131 | Training loss: 7.8373\n",
      "Epoch 132 | Training loss: 7.7222\n",
      "Epoch 133 | Training loss: 7.7566\n",
      "Epoch 134 | Training loss: 7.5371\n",
      "Epoch 135 | Training loss: 7.5182\n",
      "Epoch 135 | Testing loss: 2.9814\n",
      "Epoch 136 | Training loss: 7.4465\n",
      "Epoch 137 | Training loss: 7.3535\n",
      "Epoch 138 | Training loss: 7.2092\n",
      "Epoch 139 | Training loss: 7.2288\n",
      "Epoch 140 | Training loss: 7.1714\n",
      "Epoch 140 | Testing loss: 2.9067\n",
      "Epoch 141 | Training loss: 7.0707\n",
      "Epoch 142 | Training loss: 6.9655\n",
      "Epoch 143 | Training loss: 6.8745\n",
      "Epoch 144 | Training loss: 6.8401\n",
      "Epoch 145 | Training loss: 6.7928\n",
      "Epoch 145 | Testing loss: 2.9649\n",
      "Epoch 146 | Training loss: 6.6227\n",
      "Epoch 147 | Training loss: 6.6398\n",
      "Epoch 148 | Training loss: 6.5702\n",
      "Epoch 149 | Training loss: 6.5171\n",
      "Epoch 150 | Training loss: 6.3845\n",
      "Epoch 150 | Testing loss: 2.9216\n",
      "Epoch 151 | Training loss: 6.3059\n",
      "Epoch 152 | Training loss: 6.3319\n",
      "Epoch 153 | Training loss: 6.1971\n",
      "Epoch 154 | Training loss: 6.1221\n",
      "Epoch 155 | Training loss: 6.1261\n",
      "Epoch 155 | Testing loss: 2.9132\n",
      "Epoch 156 | Training loss: 6.0976\n",
      "Epoch 157 | Training loss: 6.0171\n",
      "Epoch 158 | Training loss: 5.9531\n",
      "Epoch 159 | Training loss: 5.9057\n",
      "Epoch 160 | Training loss: 5.8291\n",
      "Epoch 160 | Testing loss: 3.0033\n",
      "Epoch 161 | Training loss: 5.8017\n",
      "Epoch 162 | Training loss: 5.6842\n",
      "Epoch 163 | Training loss: 5.6681\n",
      "Epoch 164 | Training loss: 5.6426\n",
      "Epoch 165 | Training loss: 5.6379\n",
      "Epoch 165 | Testing loss: 3.0657\n",
      "Epoch 166 | Training loss: 5.5507\n",
      "Epoch 167 | Training loss: 5.4997\n",
      "Epoch 168 | Training loss: 5.4383\n",
      "Epoch 169 | Training loss: 5.3972\n",
      "Epoch 170 | Training loss: 5.3918\n",
      "Epoch 170 | Testing loss: 3.1251\n",
      "Epoch 171 | Training loss: 5.3253\n",
      "Epoch 172 | Training loss: 5.2636\n",
      "Epoch 173 | Training loss: 5.2431\n",
      "Epoch 174 | Training loss: 5.1599\n",
      "Epoch 175 | Training loss: 5.1653\n",
      "Epoch 175 | Testing loss: 3.1253\n",
      "Epoch 176 | Training loss: 5.0688\n",
      "Epoch 177 | Training loss: 5.0569\n",
      "Epoch 178 | Training loss: 5.0345\n",
      "Epoch 179 | Training loss: 5.0041\n",
      "Epoch 180 | Training loss: 4.9594\n",
      "Epoch 180 | Testing loss: 3.2671\n",
      "Epoch 181 | Training loss: 4.9576\n",
      "Epoch 182 | Training loss: 4.8541\n",
      "Epoch 183 | Training loss: 4.8285\n",
      "Epoch 184 | Training loss: 4.7638\n",
      "Epoch 185 | Training loss: 4.7470\n",
      "Epoch 185 | Testing loss: 3.3785\n",
      "Epoch 186 | Training loss: 4.7474\n",
      "Epoch 187 | Training loss: 4.6947\n",
      "Epoch 188 | Training loss: 4.6942\n",
      "Epoch 189 | Training loss: 4.6093\n",
      "Epoch 190 | Training loss: 4.5605\n",
      "Epoch 190 | Testing loss: 3.2127\n",
      "Epoch 191 | Training loss: 4.5194\n",
      "Epoch 192 | Training loss: 4.5101\n",
      "Epoch 193 | Training loss: 4.4856\n",
      "Epoch 194 | Training loss: 4.4384\n",
      "Epoch 195 | Training loss: 4.4152\n",
      "Epoch 195 | Testing loss: 3.3741\n",
      "Epoch 196 | Training loss: 4.3944\n",
      "Epoch 197 | Training loss: 4.3678\n",
      "Epoch 198 | Training loss: 4.2846\n",
      "Epoch 199 | Training loss: 4.2560\n",
      "Epoch 200 | Training loss: 4.2401\n",
      "Epoch 200 | Testing loss: 3.4281\n",
      "Epoch 201 | Training loss: 4.2545\n",
      "Epoch 202 | Training loss: 4.1728\n",
      "Epoch 203 | Training loss: 4.1182\n",
      "Epoch 204 | Training loss: 4.1520\n",
      "Epoch 205 | Training loss: 4.1735\n",
      "Epoch 205 | Testing loss: 3.1690\n",
      "Epoch 206 | Training loss: 4.1275\n",
      "Epoch 207 | Training loss: 4.0705\n",
      "Epoch 208 | Training loss: 4.0043\n",
      "Epoch 209 | Training loss: 4.0526\n",
      "Epoch 210 | Training loss: 4.0228\n",
      "Epoch 210 | Testing loss: 3.3192\n",
      "Epoch 211 | Training loss: 3.9624\n",
      "Epoch 212 | Training loss: 3.9362\n",
      "Epoch 213 | Training loss: 3.9553\n",
      "Epoch 214 | Training loss: 3.9047\n",
      "Epoch 215 | Training loss: 3.8642\n",
      "Epoch 215 | Testing loss: 3.1785\n",
      "Epoch 216 | Training loss: 3.8846\n",
      "Epoch 217 | Training loss: 3.8611\n",
      "Epoch 218 | Training loss: 3.7853\n",
      "Epoch 219 | Training loss: 3.7567\n",
      "Epoch 220 | Training loss: 3.7396\n",
      "Epoch 220 | Testing loss: 3.1277\n",
      "Epoch 221 | Training loss: 3.7338\n",
      "Epoch 222 | Training loss: 3.7030\n",
      "Epoch 223 | Training loss: 3.7291\n",
      "Epoch 224 | Training loss: 3.6641\n",
      "Epoch 225 | Training loss: 3.6169\n",
      "Epoch 225 | Testing loss: 3.1611\n",
      "Epoch 226 | Training loss: 3.6628\n",
      "Epoch 227 | Training loss: 3.6053\n",
      "Epoch 228 | Training loss: 3.5542\n",
      "Epoch 229 | Training loss: 3.5649\n",
      "Epoch 230 | Training loss: 3.5846\n",
      "Epoch 230 | Testing loss: 3.0821\n",
      "Epoch 231 | Training loss: 3.5614\n",
      "Epoch 232 | Training loss: 3.5035\n",
      "Epoch 233 | Training loss: 3.5225\n",
      "Epoch 234 | Training loss: 3.4283\n",
      "Epoch 235 | Training loss: 3.4616\n",
      "Epoch 235 | Testing loss: 3.2027\n",
      "Epoch 236 | Training loss: 3.4510\n",
      "Epoch 237 | Training loss: 3.4190\n",
      "Epoch 238 | Training loss: 3.3909\n",
      "Epoch 239 | Training loss: 3.3624\n",
      "Epoch 240 | Training loss: 3.3112\n",
      "Epoch 240 | Testing loss: 2.8919\n",
      "Epoch 241 | Training loss: 3.3665\n",
      "Epoch 242 | Training loss: 3.3489\n",
      "Epoch 243 | Training loss: 3.3115\n",
      "Epoch 244 | Training loss: 3.3620\n",
      "Epoch 245 | Training loss: 3.2967\n",
      "Epoch 245 | Testing loss: 2.8651\n",
      "Epoch 246 | Training loss: 3.3108\n",
      "Epoch 247 | Training loss: 3.2878\n",
      "Epoch 248 | Training loss: 3.2502\n",
      "Epoch 249 | Training loss: 3.2453\n",
      "Epoch 250 | Training loss: 3.2050\n",
      "Epoch 250 | Testing loss: 2.8619\n",
      "Epoch 251 | Training loss: 3.1906\n",
      "Epoch 252 | Training loss: 3.1742\n",
      "Epoch 253 | Training loss: 3.1927\n",
      "Epoch 254 | Training loss: 3.1911\n",
      "Epoch 255 | Training loss: 3.1705\n",
      "Epoch 255 | Testing loss: 2.9776\n",
      "Epoch 256 | Training loss: 3.1200\n",
      "Epoch 257 | Training loss: 3.0959\n",
      "Epoch 258 | Training loss: 3.1004\n",
      "Epoch 259 | Training loss: 3.0730\n",
      "Epoch 260 | Training loss: 3.1103\n",
      "Epoch 260 | Testing loss: 2.7636\n",
      "Epoch 261 | Training loss: 3.0735\n",
      "Epoch 262 | Training loss: 3.0945\n",
      "Epoch 263 | Training loss: 3.0596\n",
      "Epoch 264 | Training loss: 3.0129\n",
      "Epoch 265 | Training loss: 3.0184\n",
      "Epoch 265 | Testing loss: 2.7629\n",
      "Epoch 266 | Training loss: 3.0236\n",
      "Epoch 267 | Training loss: 3.0056\n",
      "Epoch 268 | Training loss: 2.9790\n",
      "Epoch 269 | Training loss: 3.0123\n",
      "Epoch 270 | Training loss: 2.9933\n",
      "Epoch 270 | Testing loss: 2.6924\n",
      "Epoch 271 | Training loss: 2.9358\n",
      "Epoch 272 | Training loss: 2.9605\n",
      "Epoch 273 | Training loss: 2.9093\n",
      "Epoch 274 | Training loss: 2.9235\n",
      "Epoch 275 | Training loss: 2.9754\n",
      "Epoch 275 | Testing loss: 2.5107\n",
      "Epoch 276 | Training loss: 2.9096\n",
      "Epoch 277 | Training loss: 2.8834\n",
      "Epoch 278 | Training loss: 2.9517\n",
      "Epoch 279 | Training loss: 2.8608\n",
      "Epoch 280 | Training loss: 2.8706\n",
      "Epoch 280 | Testing loss: 2.3753\n",
      "Epoch 281 | Training loss: 2.8894\n",
      "Epoch 282 | Training loss: 2.8233\n",
      "Epoch 283 | Training loss: 2.8355\n",
      "Epoch 284 | Training loss: 2.8068\n",
      "Epoch 285 | Training loss: 2.8227\n",
      "Epoch 285 | Testing loss: 2.3914\n",
      "Epoch 286 | Training loss: 2.8002\n",
      "Epoch 287 | Training loss: 2.7975\n",
      "Epoch 288 | Training loss: 2.7887\n",
      "Epoch 289 | Training loss: 2.7637\n",
      "Epoch 290 | Training loss: 2.7521\n",
      "Epoch 290 | Testing loss: 2.2355\n",
      "Epoch 291 | Training loss: 2.7562\n",
      "Epoch 292 | Training loss: 2.7707\n",
      "Epoch 293 | Training loss: 2.7415\n",
      "Epoch 294 | Training loss: 2.7503\n",
      "Epoch 295 | Training loss: 2.7378\n",
      "Epoch 295 | Testing loss: 2.2421\n",
      "Epoch 296 | Training loss: 2.7950\n",
      "Epoch 297 | Training loss: 2.6861\n",
      "Epoch 298 | Training loss: 2.7084\n",
      "Epoch 299 | Training loss: 2.6700\n",
      "Training time:308.2697s\n"
     ]
    }
   ],
   "source": [
    "path=root+'data_118_quad/gnn_trained_ac118.pickle'\n",
    "try: \n",
    "  net.load_state_dict(torch.load(path))\n",
    "  print('params loaded')\n",
    "except: \n",
    "  print('cold start')\n",
    "\n",
    "optimizer=torch.optim.Adam(net.parameters())\n",
    "train_loss=[]\n",
    "val_loss=[]\n",
    "\n",
    "## Training\n",
    "t0=time.time()\n",
    "max_epochs=00\n",
    "eval_epoch=5\n",
    "\n",
    "# earlystopping\n",
    "tolerance=5\n",
    "min_delta=1e-3\n",
    "previous=0\n",
    "\n",
    "# add feasibility \n",
    "feas=False\n",
    "for epoch in range(max_epochs):\n",
    "  # training loop\n",
    "  total_loss=0.0\n",
    "  for local_batch,local_label in train_set:\n",
    "    optimizer.zero_grad() # clear the past gradient\n",
    "    local_batch,local_label=local_batch.to(device),local_label.to(device)\n",
    "    logits=net(local_batch)\n",
    "    loss=my_loss.calc(logits,local_label,local_batch,feas)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    total_loss+=loss.item()\n",
    "  avg_loss=total_loss/len(train_set.dataset)\n",
    "  train_loss.append(avg_loss)\n",
    "  print(\"Epoch %d | Training loss: %.4f\"%(epoch,avg_loss))\n",
    "  # eval\n",
    "  if epoch%eval_epoch==0:\n",
    "    net.eval()\n",
    "    total_loss=0.0\n",
    "    for local_batch,local_label in val_set:\n",
    "      local_batch,local_label=local_batch.to(device),local_label.to(device)\n",
    "      logits=net(local_batch)\n",
    "      loss=my_loss.calc(logits,local_label,local_batch,feas)\n",
    "      total_loss+=loss.item()\n",
    "    avg_loss=total_loss/len(val_set.dataset)\n",
    "    val_loss.append([epoch, avg_loss])\n",
    "    print(\"Epoch %d | Testing loss: %.4f\"%(epoch,avg_loss))\n",
    "    if epoch:\n",
    "      if previous-avg_loss<min_delta: tolerance-=1\n",
    "      if tolerance==0: pass\n",
    "    previous=avg_loss\n",
    "    net.train()\n",
    "t1=time.time()\n",
    "print(\"Training time:%.4fs\"%(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "AvUTphUSzqzA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Desktop\\GNN_OPF_electricity_market-main\\GNN_OPF_electricity_market-main\\GNN_OPF_electricity_market-main\\data_gen\\118_data\\FCNN_model\\dnn_02191305.pickle\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "timestamp=datetime.now().strftime('%m%d%H%M')\n",
    "path = 'C:\\\\Users\\\\USER\\\\Desktop\\\\GNN_OPF_electricity_market-main\\\\GNN_OPF_electricity_market-main\\\\GNN_OPF_electricity_market-main\\\\data_gen\\\\118_data\\\\FCNN_model\\\\'\n",
    "\n",
    "path = path+'dnn_%s.pickle'%(timestamp)\n",
    "if feas==False: path.replace('feas','')\n",
    "print(path)\n",
    "torch.save(net.state_dict(),path)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "L3M4jmjkscK_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjoAAAGwCAYAAACgi8/jAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAX0RJREFUeJzt3Qdc1VX/B/APG9kCiigobsU9cGdaalpZmpUtMyttaMum/Z8nW89j67GyzHY2bZhalppa5gr33gsFHIAgW/b9v77nx70MES8I/O69v8/79bre3x3A4eeF++Gc7znHyWQymUBERETkgJz1bgARERFRbWHQISIiIofFoENEREQOi0GHiIiIHBaDDhERETksBh0iIiJyWAw6RERE5LBcYWBFRUU4deoUfH194eTkpHdziIiIyAqyBGBGRgYaN24MZ+fK+2wMHXQk5ISHh+vdDCIiIqqGuLg4hIWFVfocQwcd6ckxnyg/Pz+9m0NERERWSE9PVx0V5vfxyhg66JiHqyTkMOgQERHZF2vKTliMTERERA6LQYeIiIgcFoMOEREROSxD1+gQEVHNL9uRl5endzPIAbi7u19y6rg1GHSIiKhGSMCJiYlRYYfocknIad68uQo8l4NBh4iIamQBt9OnT8PFxUVN+62Jv8TJuIqKF/SV11TTpk0va1FfBh0iIrpsBQUFyM7OVivVenl56d0ccgANGjRQYUdeW25ubtX+PIaM3LNnz0ZkZCSioqL0bgoRkUMoLCxU15c7zEBkZn4tmV9b1WXIoDN58mTs27cPmzdv1rspREQOhfsGkq29lgwZdIiIiMgYGHSIiIjIYTHoEBER1ZCIiAi88847Vj//77//VkM0qamptdquuXPnIiAgAEbEWVe1IS8L2PIFkLAHGP2h3q0hIqKLGDRoELp27VqlcFIZqf309va2+vn9+vVTU6j9/f1r5OvThRh0asOXNwAnt2jHvSYCTXro3SIiIrqMNYJk5o+rq6tVU6KrOrOoUaNGl9E6uhQOXdXCD8QCDLbczvnrDV3bQ0REFbvnnnuwevVqvPvuu2r4SC7Hjx+3DCctXboUPXr0gIeHB9atW4ejR4/ixhtvREhICHx8fNQSJStXrqx06Eo+z6efforRo0er9YVat26NX3/99aJDV+Yhpj/++APt27dXX2f48OGq18dM1pV59NFH1fOCgoLw7LPPYvz48Rg1alSVvv85c+agZcuWKmy1bdsWX3/9dZn3shdffFEt1iffv6yPJF/T7IMPPlDfi6enpzofN998M2wVe3RqmLxgD4Rcj9PxnyHUKQWeR5cBCXuBkA56N42IqE6NfG8dkjJy6/zrNvD1wOJHBlzyeRJwDh06hI4dO+Lll1/WPrZBAxV2xHPPPYe33noLLVq0QP369REXF4drr70W//nPf9Sb/1dffYWRI0fi4MGDKhBczEsvvYQ33ngDb775Jt577z3ceeedOHHiBAIDAyt8viy8KF9XgoesMH3XXXfhqaeewrfffqsef/3119XxF198ocKQfB+LFi3C4MElf2RfysKFC/HYY4+pUDZkyBD89ttvmDBhAsLCwtTn+fnnn/H222/j+++/R4cOHXDmzBns3LlTfeyWLVtU6JH2ydBbSkoK1q5dC1vFoFMLHro6Eh9vG4ln8aW6nb7idfjd9ZXezSIiqlMScs6k58BWSV2M9GZIT0tFw0cSfoYOHWq5LcGkS5cultuvvPKKCgzSQzNlypRKe45uv/12dfzf//4Xs2bNwqZNm1RPTUXy8/Px4Ycfqt4WIZ/bHMSEhKVp06apXiLx/vvvY8mSJVX63t966y3Vrocffljdnjp1KjZs2KDul6ATGxurzomEIFmVWIJcr1691HPlMalDuv766+Hr64tmzZqhW7dusFUMOrWgvrc7ggZOQvLqBQhyyoDPkcVA8lEgSHvREhEZgfSs2PPX7dmzZ5nbmZmZajjn999/V0NJMoR0/vx59cZfmc6dO1uOJSD4+fkhMTHxos+X4GUOOSI0NNTy/LS0NCQkJFhCh5D9xWSIrSqbqe7fvx+TJk0qc1///v1V75C45ZZbVG+P9GZJIJOeLOm9kjolCX8SbsyPycU8NGeLGHRqyV1XtMfc9SPxYOF3cEYREpa+hpC7PtG7WUREdcaa4SNbVn72lAwfrVixQvV6tGrVCvXq1VO1KbJre2XK79MkJQ6VhZKKni81M3UpPDxcDclJDZJ8z9LzI0NvUtMkvTjbtm1T9UXLly/HCy+8oAKgzDizxSnsLEauJZ5uLmg09FGkm7SEG3hkAYrOxendLCIiKkWGrqzdS2n9+vVquEd6Lzp16qSGdsz1PHU53CbFv6W3MJL2S/Coivbt26vvpzS5LftAmkmQk14cGWqTUBMdHY3du3erx6RnR4a1pPZo165d6jz89ddfsEXs0alFI3u1w4+rrsftuT/CDQU4/ttriBg3W+9mERFRqVlSGzduVG/UMsPpYgXCQmYZLViwQL35Sy/Lv//97yoNF9WURx55BDNmzFC9Su3atVM1O+fOnavS3lBPP/00br31VlVbI4Fl8eLF6nszzyKT2V8SoHr37q2GpL755hsVfGTISgqXjx07hoEDB6oibakPkvMgM7dsEXt0apGLsxPCr3sS2SZtvDj06I8oSk/Qu1lERFRqOEpqXKQnQ2ZcVVZvM3PmTPXGLjONJOxcc8016N69O+qaTCeX4ua7774bffv2VQFN2iJTva01atQoVY8jw3Ayq+qjjz5Ss7hkAUUhQ1CffPKJqtuRGiMJQBKGZDq7PCah6KqrrlI9Q1I4PW/ePPV5bJGTqa4H/mxIenq66gaU4i4pDqsNcnp/feMe3Hh+kbp9rO1EtLj9rVr5WkREesnJyUFMTAyaN29epTdcunzSmyKBQ3poZCaYEV5T6VV4/2aPTi2TrsTgYU8h16SNEjY6+A1M2ef0bhYREdkpWYNHeltkDSCpmXnooYdUILjjjjv0bppNYtCpA/26dsRfntpaDF44j7hlNbOnChERGY8sIig1NLIyswwtSdiRoSXp1aELsRi5jnp13K6cioI//oCrUxEC93wGXPcU4OGrd9OIiMjOyNTv8jOm6OIM2aMze/ZsVXgmabiuDOrdEytdB6pjn6IMJK6aU2dfm4iIyKgMGXQmT56Mffv2lVmHoLa5ujgju9ejKDJp0//qbZkD5J+vs69PRERkRIYMOnoZNuhKrEBvdexbkILzG7W9sIiIiKh2MOjUIR8PVxxr94Dldv7692T+ua5tIiIicmQMOnVs6NVDEV2oLbHtdz4eppRjejeJiIjIYTHo1LFWDX1xvH4fy+3jW5bq2h4iIrr8bSRkp+/SM20XLdIWia2IbDchz9mxY8dlfd2a+jyXIvt7yUrK9opBRweNu2hr6oi0fX/q2hYiIqpZp0+fxogRI2o9bMg0c/laHTt2rNGv5WgYdHTQq//VyDTVU8dhqVuRnZuvd5OIiKiGyK7mHh7aHoe1Sfbokq8lO4nTxTHo6KCepwfi/bqq42CnNKyL5sJPRER17eOPP0bjxo0v2IH8xhtvxL333quOjx49qm6HhISozTNl/TXzDt8XU37oatOmTWqXcNmvqWfPnti+fXuZ58su4ffdd5/a00l2CJddwGXDTbMXX3wRX375JX755Rf1ueXy999/Vzh0tXr1avTq1UsFrdDQUDz33HMoKCiwPC6bdj766KN45pln1E7tEpTk81dFbm6u+hwNGzZU39OAAQPKLNciO6nfeeedapNU+X5k13fZMFTk5eVhypQpqm3ysbIbuuzEXpsYdHTi3e4qy/HJ7X/o2hYiIiO65ZZbkJycjFWrVlnuS0lJwbJly9QbtcjMzMS1116LP//8UwWU4cOHq53LK9vlvDT5+Ouvv14tUrt161YVKmTH9NIkaIWFheGnn35Sa7y98MILeP755/Hjjz+qx+X5smGnfG0ZqpKL7KBe3smTJ1VbJYzt3LkTc+bMwWeffYZXX321zPMkNHl7e2Pjxo1444038PLLL2PFihVWnzcJST///LP6PNu2bUOrVq3U7uly7sS///1v9X0sXboU+/fvV+0IDg5Wj82aNQu//vqr+t4OHjyIb7/9VtU41Sb2d+kkrPs1wOb/qOPQlM2IS8lGeKCX3s0iIqo5H10JZCbW/df1aQg8sPqST6tfv76qpfnuu+9w9dVXq/vmz5+v3pQHDx6sbnfp0kVdzGR38IULF6o3a+mZuBT53BJkJHBID0aHDh0QHx+vNuI0c3Nzw0svvWS5LT070dHRKgxIwJGeJOkZkZ4U6YG5mA8++EDV7bz//vuqp6ddu3Y4deoUnn32WRWeZI8s0blzZ0yfPl0dS2+LPF+C3NChJfWjF5OVlaWCi+y1Za5Dkg1GJSjJ9/j000+rECg9WNJ7JUoHGXlMvqb0AkkbpUentjHo6MQppBNyXP3hWZCGPs778MWWWDwxrJ3ezSIiqjkScjJOwZZJz83EiRNVSJDhHulhuO222yyhQHpkpBfm999/Vz0pMgx0/vx5q3t0pEdDgoWEHLO+fftWuDXR559/rj6vfH4Z4unaVStxsJZ8LfncEiDM+vfvr74HCVdNmzZV90l7SpNhpMRE6wKpDOXl5+erz1s6qMlwmXx9ISFuzJgxqrdn2LBhqoja3AMlRdUSqGR4TnqopLdLnlObGHT0Ij9EEQOAI78jwCkLu7aug2lo2zIvUCIiuyY9Kzb+dWUYymQyqSAjQz5r167F22+/bXlcho2kt+Ktt95SQzTSs3LzzTerIFJTvv/+e/V1/ve//6mg4uvrizfffFMNLdUGNze3Mrflfad8ndLlkJ6eEydOYMmSJercSW+ZbL0k57B79+6IiYlRw1pS6yQ9VkOGDFE9abWFQUdHnq0HqaAjWmZuw874m9E1PEDvZhER1Qwrho/0Jj0tN910k+rJOXLkiOppkDdjM9klXHohRo8erW5L74gUAVurffv2+Prrr5GTk2Pp1dmwYUOZ58jXkB6Phx9+uEzPSWnu7u6qaPlSX0tqZyS4mf9oXr9+vQpOUgNUE1q2bKnaIp/XPOwkPTxSjPz4449bnieFyOPHj1eXK664Qg1pSdARfn5+GDt2rLpIaJSeHanvkeLo2sBiZD0113YzF32d92HpntO6NoeIyIhk+Ep6dGToyFyEbCb1JAsWLFAzm6TA94477qhS74c8X0KHDI9Jga70cpjf8Et/jS1btuCPP/7AoUOHVDFv+U2npc5l165dqoD37NmzKlyUJ0EpLi4OjzzyCA4cOKBmaU2fPh1Tp061DMVdLililqEpCS5StC3fk3xv2dnZauaYkHog+doSHPfu3YvffvtNhTAxc+ZMzJs3T7VPvlcpwJa6o4CA2vsjn0FHTw3aosirgTrs7bwfK3bHqyRORER156qrrlK9CRIiJJiUJm/MUrQsPS4yzCWzi0r3+FyKFBIvXrwYu3fvVgW6//d//4fXX3+9zHMeeOAB1askPRy9e/dWM8FK9+4ICRPS2yQFvtJbIj0q5TVp0kQFKZnOLgXUDz74oAof//rXv1CTXnvtNVWDM27cOHUuJNBISJPzJKTHZ9q0aaoWaODAgWq9HxmeE9K7JDO95PuQoULpHZM211QQq4iTycDvrOnp6fD390daWprqStPF/HuBPT+rw9G5L+E/j9yLyMY6tYWIqJpkaEZqL2TGUOnCW6LaeE1V5f2bPTp64/AVERFRrWHQ0VvEFZbDvs57sXTPGV2bQ0RE5EgYdPQW2ALw06rho5wPIjbxHI4kZujdKiIiIofAoKM3mQJYPHzl6ZSPrk5HsHQ3e3WIiIhqAoOOLWheMnzVz4XDV0Rkvww8v4Vs9LXEoGNzdTr7sO90utr7iojIXsgUYlGTKwaTseUVv5bMr63q4srItiAgXKvVSTmGbk6H4YlcrNyfgAn9m+vdMiIiq7i6usLLywtJSUlqi4HaXBeFHF9RUZF6LclrSl5bl4NBx5Z6dVKOwd2pED2dD+HP/U0YdIjIbsjqv7I5pKx7IvscEV0uCcuyEenl7gHJoGMrpCB525eWaeZvH+uM9Jx8+HmW3XyNiMhWyYq4sp0Bh6+opl5PNdEzyKBjg3U6/Zz34c0CE1YfTMLILo11bRYRUVXIGxNXRiZbYshB1NmzZyMyMlLts2EzfEOABu3UYSenY/BBNv46kKh3q4iIiOyaIYPO5MmT1Y6r5XeH1V3xejquTkVq8cC1h89yqiYREdFlMGTQsY/hq704m5mLA2e4SjIREVF1MejYkogBMnfBsp6OWHs4SedGERER2S8GHVviFQg06qgOI51OwB+ZaviKiIiIqodBx9Y0v1JdOTuZ0Md5PzbGpCAnv1DvVhEREdklBh1bU1yQLPo470NeQRE2xaTo2iQiIiJ7xaBja8JKpry3c4pT1+uOcPiKiIioOhh0bLFOx7uBOmzpfEpdrznEgmQiIqLqYNCxRcFt1FVDp1T4IltNMU9Mz9G7VURERHaHQccWBbWyHLZw0np1OHxFRERUdQw6NtyjI1oWBx1OMyciIqo6Bh0bDzptXc+o63+OcjsIIiKiqmLQsUXBrS2H3b21npyE9FwcTcrSsVFERET2h0HHFgU0BVw8ygxdieijHL4iIiKqCgYdW+TsAgS1VIcBOXFwgbYy8j9Hk3VuGBERkX1h0LHx4Svnony09zynjqOPJaOoiHU6RERE1mLQsVVBJXU6wxulq+vU7HzsO60dExER0aUx6NjBzKveviVDVtz3ioiIyHoMOnYw86qV82nLMYMOERGR9Rh07GB15IDsE/D1dFXHm4+ncD0dIiIiKzHo2CpPP8A3VB06JR9CVESgOk7OyuN6OkRERFZi0LGH4avsZAxoUvJfJb06REREdGkMOnZSkNzPX5tiLlinQ0REZB0GHTuZYt7K+RQ83bT/LgYdIiIi6zDo2MnMK9eUw+gWXl8dn0w9ry5ERERUOQYdOxm6wtnDiGquFSSLzezVISIiuiQGHVvm1wRwracdJx9G71JBZyODDhER0SUx6NgyZ2cguHg9nZQYdGviBVdnJ3WTM6+IiIgujUHHXoavTIXwyoxDxyb+6uaRxEwkZ+bq2zYiIiIbx6BjZ3U6vUrX6RwvmXJOREREF2LQsaOtIHD2EHoVr5AsOM2ciIiocgw6dtaj0zNCm2IuWKdDRERUOQYdO+vRCfByR7tGvurm3lNpyMjJ169tRERENo5Bx9a5ewH+4dpx8mHAZLJs8FlkArbFpurbPiIiIhvGoGNPKyTnpAFZSWUKkjfFJOvXLiIiIhvHoGN3dTqHys68iuHMKyIiooth0LGzPa+kIDnEzxPNgrzUzR1xqcjJL9SvbURERDaMQcfOdjGXoCPMdTp5hUXYFZ+mV8uIiIhsGoOOHQ5dibILB3KaORERkUMGnbi4OAwaNAiRkZHo3LkzfvrpJzgc30aAu2/ZoFNq4UBu8ElEROSgQcfV1RXvvPMO9u3bh+XLl+Pxxx9HVlYWHIqTU8nmnqmxQH6OqtFp6Ouh7tp24hwKCov0bSMREZENsvugExoaiq5du6rjRo0aITg4GCkpKQ48fGUCUo7CyckJUcXDV5m5Bdh/OkPX5hEREdki3YPOmjVrMHLkSDRu3Fi9eS9atOiC58yePRsRERHw9PRE7969sWnTpgo/19atW1FYWIjw8OIF9hx25tWFw1ebWKdDRERke0FHhpm6dOmiwkxFfvjhB0ydOhXTp0/Htm3b1HOvueYaJCYmlnme9OLcfffd+Pjjj+H4BclH1BUXDiQiIqqcK3Q2YsQIdbmYmTNnYuLEiZgwYYK6/eGHH+L333/H559/jueee07dl5ubi1GjRqnb/fr1u+jnkufJxSw9PR32OcVc69FpG+ILP09XpOcUYPPxczCZTKpXjIiIiGykR6cyeXl5ajhqyJAhlvucnZ3V7ejoaHVb3tzvueceXHXVVRg3blyln2/GjBnw9/e3XOxqiCuwBeDkXCboODs7WdbTScnKw9GkTD1bSEREZHNsOuicPXtW1dyEhISUuV9unzlzRh2vX79eDW9JbY8UJctl9+7dFX6+adOmIS0tzXKRqel2w80TCGimHScfUZt7CnNBstjE7SCIiIhsa+jqcg0YMABFRdZNrfbw8FAXuy5IPhcD5GUCGacBv8YX1Onc0buprk0kIiKyJTbdoyNTxV1cXJCQkFDmfrktU8kNp4IVkjs29oenm/bfKHU6REREZCdBx93dHT169MCff/5puU96b+R23759YfTNPYW7qzO6N62vjk+mnkf8uWy9WkdERGRzdB+6yszMxJEj2nRpERMTgx07diAwMBBNmzZVU8vHjx+Pnj17olevXmoVZJmSbp6FZdweHS3oCClI/udosmXfq7D62s7mRERERqd70NmyZQsGDx5suS3BRki4mTt3LsaOHYukpCS88MILqgBZio2XLVt2QYGyIVQwxVz0LlOnk4LR3cLqumVEREQ2SfegIxtyyhTxykyZMkVdaoosTigXmdFlV7yDAc8AICe1TI9Ot6b14ershIIikwo6REREZAc1OrVl8uTJahPQzZs3w/429ywevkqPB/K0zUvrubugU5i/Oj6alIWzmSWLIhIRERmZIYOOwxQky3o6xUrve7WF+14REREpDDr2JqhVyXGp4avS6+ls5PAVERGRwqBjbwJKLQiYfspy2LNZoBrZMs+8IiIiIgYd++PXpMKg4+/lpjb5FPtOpSMjJ1+P1hEREdkUBh1741866Jws85B5+KrIBGw9wVWSiYiIGHTsjY9sfeFUadARnGZORERk0KAja+hERkYiKioKdsfVHfBpeMHQVfmZV6zTISIiMmjQsdt1dMrX6WScAQpLanEa+nkiIkjb/mFnXBpy8u1sQUQiIqIaZsigY/f8GhcfmLSwU0rv5kHqOq+wCFu4mzkRERkcg449usjMK9GvlRZ0xD9Hz9Zlq4iIiGwOg46Dzbzq27Ik6Kwv3tGciIjIqBh07L5Hp2zQaejriTYhPup4d3wq0rmeDhERGRiDjl3X6Fw4dCX6tQy2rKez8RhnXxERkXEx6DhYj47oV2r4at3hpLpqFRERkc0xZNCx63V0hG9oyXHahUGnT8sguDpriwquOpgEk8lUl60jIiKyGYYMOna/jo4sGuhd8aKBws/TDT0j6qvj2JRsHDubVdctJCIisgmGDDoOVaeTKYsGFlzw8FXtioOQ9OocSKzLlhEREdkMBh175R+mXZuKgMyEyoPOQQYdIiIyJgYdh5h5dWGdTssGPgirX8+ywWdm7oW9PkRERI6OQcdBg46Tk5OlVye/0IR1h7lKMhERGQ+Djr3yKx66ukhBshjclnU6RERkbAw6jtCjU8EUc/N2EB6uzpY6HU4zJyIio2HQcdChK+Hp5mJZPDAxIxd7T6XXVeuIiIhsAoOOg24DYcZp5kREZGQMOvbK1QPwblBpj44YVKpOZ9neM3XRMiIiIpthyKBj91tAlO/Vyah40UARHuiFLmH+6liGro4mZdZlC4mIiHRlyKBj91tAlJ95ZSoEsi4+LDWyS8kw1+KdFx/mIiIicjSGDDpGmnklru/cGE5OJUGHs6+IiMgoGHQcfOaVaOTviaiIQHV8NCkL+05z9hURERkDg44j7Hd1iZlXFw5fna7NVhEREdkMBh0D9OiIazs2gouzNn7F4SsiIjIKBh2DBJ0gHw/0bxWsjk+mnsf2uNTabh0REZHuGHTsma91iwaajewcajn+dQdnXxERkeNj0LFnbp6AV7DVQWdYh0ZwL977auH2kzifV1jbLSQiItIVg46jDF9J0CmqPLj413PD9cW9Omnn87F4F3t1iIjIsTHo2Du/JiWLBmZeei+rcX2aWY6/2XCiNltGRESkOwYde+dfHHSsHL7qGh6ATk20LSF2xadhJ4uSiYjIgRky6DjMXlcXzLyKv+TTnZycyvTqfBXNXh0iInJchgw6DrPXVemhKyt7dMyLB/p5uqpjqdM5l5VXW60jIiLSlSGDjkMpE3QqX0vHrJ67C27pGa6O8wqK8NPWuNpqHRERka4YdAyysWd5d5UavvrynxPILyyq6ZYRERHpjkHHoWp0rJ8u3jzYG4PbNrCslPwLFxAkIiIHxKBj79zqAV5BVQ46YvLgVpbjD/4+gsIi7n9FRESOhUHHkXp1Mi69aGBpPSMC0at5oDo+lpSlNvskIiJyJAw6jlSQXFQAZCVV6UMfvaq15fj1ZQeQnVdQ060jIiLSDYOOwXYxL29A62BLrc7ptBx8tPpYTbeOiIhINww6jqAaa+mU9q/rI+Hq7KSOP1x9VBUnExEROQIGHUcLOlWYYm7WsoEP7u4boY5zC4rw2tIDNdk6IiIi3TDoGHzoyuyxq1ujvpebOpai5I3HkmuqdURERLph0HEE/mGXNXSlPoWXG6YOa2u5/ezPu3A+z/oZXERERLaIQccR+IZedo+OuKNXU3RvGqCOjydn480/DtZE64iIiHRjyKDjULuXC3cvoF79yw46Ls5OePOWLnB31V4Wc/+JwZ6TaTXVSiIiojpnyKDjULuXm/kVD1+lnwaKqr9vlRQmPz5EW1tHFkr+16I9KOKKyUREZKcMGXQcuiC5KL/KiwaWd/+AFmjV0Ecd74hLxZfRx2uihURERHWOQcdR1MDMKzMZunr5xg6W2zOWHOAQFhER2SUGHUfhf3mLBpbXr2Uw7hvQXB3nFRbh4W+3ITkz97I/LxERUV1i0HHI1ZEvr0fH7Nnh7dA5zF8dx6Zk4/6vtiAnn1POiYjIfjDoOIoaHLoqPYT14V090NDXQ93eHpuKaQt2w2RicTIREdkHBh1Hm3VVQ0NXZo0D6uGLCVHwcndRtxduP4lvNpyosc9PRERUmxh0HIVf6GXtd1WZDo398ebNXSy3X/5tH7aeOFejX4OIiKg2MOg4CndvwDOgRoeuSruucygmXqEVJ+cXmvDwt1uRlMHiZCIism0MOo5YkJxxeYsGVlac3Kt5oDpOSM/FQ99sRW4Bi5OJiMh2Meg44hTzwjwgu+Z3H3d1ccb7d3RDiJ9WnLzlxDk89/NurpxMREQ2i0HHYWdexdfKl2jo64lP746Cp5uzpTj5379wmwgiIrJNDDoOu5ZOzc28Kq9TmD/eGdsNzk7a7W83xuI/S/bX2tcjIiKq06Dz5Zdf4vfff7fcfuaZZxAQEIB+/frhxAlOPXb0oCOGd2yEt8d2tYSdz9bFYO76mFr9mkRERHUSdP773/+iXr166jg6OhqzZ8/GG2+8geDgYDzxxBPV+ZRU00NXabUzdFXajV2b4L+jO5WZdr5yX0Ktf10iIqJaDTpxcXFo1aqVOl60aBHGjBmDSZMmYcaMGVi7dm11PiXZWY+O2W29muLhQS3VsZTpPDJvO3bFp9bJ1yYiIqqVoOPj44PkZG1Wz/LlyzF06FB17OnpifPnz1fnU1KNFyPXTdARTw1ri5FdtK99Pr8Q987djONns+rs6xMREdVo0JFgc//996vLoUOHcO2116r79+7di4iIiOp8SqoJHj6Ap3+tzrqqiLOzE968uTOiIuqr22cz8zDu842IS8muszYQERHVWNCRmpy+ffsiKSkJP//8M4KCgtT9W7duxe233w5bJ+2PjIxEVFQUHHb4Snp06nDzTU83FzXtvG2Ir7odl3IeN835B/tOpddZG4iIiMpzMhl4K+r09HT4+/sjLS0Nfn5+cAjfjAGOrNSOnz4GeGshtK6cScvBHZ9uwLEkbejK18MVH93dA/1aBtdpO4iIyHFV5f27Wj06y5Ytw7p168r0kHTt2hV33HEHzp3jZo+68m1UcixbQdSxRv6emP9gP3QN1/bdysgtwD2fb8baw0l13hYiIqJqBZ2nn35apSmxe/duPPnkk6pOJyYmBlOnTq3pNlJV+JbaxTzjjC5NCPR2x3cTe+Oqdg3V7bzCIjz8zTYcOMNhLCIisoOgI4FGalyE1Ohcf/31am0d6dlZunRpTbeRqh106r5Hx8zL3RUfj+uBoZEhWlNyC3DXpxuxOz5NtzYREZHxVCvouLu7Iztbm1GzcuVKDBs2TB0HBgZaenrI2EHHvAnorNu6oUvxMJbMxhr7cTT+OXpW13YREZFxVCvoDBgwQA1RvfLKK9i0aROuu+46db9MNQ8LC6vpNpId1eiUV8/dBV9N6IVeEYHqdnaets4Oww4REdls0Hn//ffh6uqK+fPnY86cOWjSRJvSLMNWw4cPr+k2kp3V6JTn7+WGr+7rhSHttZqdnPwiLewcYdghIqLaxenljja9vLAAeLUBYCoCGncDJv0NW5FXUISHv92KlfsT1W1PN2d8Pj4K/Vpx6jkREdnQ9HJRWFioCpFfffVVdVm4cKG6j3Tm4gp4N7SpHh0zd1dnfHBnDwxpH1LSs/PlZqxnzw4REdWSagWdI0eOoH379rj77ruxYMECdbnrrrvQoUMHHD16tOZbSdWr08lMAIoKbTDsdC8TdibM3Yxfd9bd3lxERGQc1Qo6jz76KFq2bKl2Md+2bZu6xMbGonnz5uoxspHNPWX4Ksv2Fuozhx3z1HMZ0np03na8/9dhGHgklYiIbCXorF69Gm+88YaaTm4m+1299tpr6jGyoZlXdbiLeXXCztie4Zb73lp+CE/9tEsFHyIiIt2CjoeHBzIyMi64PzMzU62xQzqzwZlXFXFzccZrYzrh2eHtLPf9vC0ed3++EWnZ+bq2jYiIDBx0ZCXkSZMmYePGjWqoQS4bNmzAgw8+iBtuuKHmW0l2vZZOZZycnPDQoJaqd8fDVXs5bjiWgls/ikZiRo7ezSMiIiMGnVmzZqkanb59+8LT01Nd+vXrh1atWuGdd96p+VaSQ/bolHZtp1DMm9QHwT5aj+DBhAzc+mE04s9pK3ATERFVh2t1PiggIAC//PKLmn21f/9+dZ/MwpKgQzbAjnp0SuvetD5+erCf2hPrZOp5HE/OVmHnm/t7o0UDH72bR0REjhx0LrUr+apVqyzHM2fOvLxWkeF6dMyaB3vjpwf7qrBz7GwWTqXl4OYPo/H5PVHoWrxnFhERUY0Hne3bt1tdc0E68woCnN2Aony7CzqicUA9/PBAX9z9+SbsP52OlKw83P7xBlXHM7hd8WKIREREVuAWEI62BYTZ252AtFgt9DxzDPYo7Xw+Hvh6iypOFi7OTpg+MhLj+jRjoCYiMrD0utgCguykTic7GSjIhT3yr+eGL+/thes6aUNxhUUmvPDLXoz/YjPOZtrn90RERHWLQccIBcmyFYSd8nB1wXu3d8N9A5pb7ltzKAk3vr8e+06l69o2IiKyfQw6jsqOC5LLc3Z2wr+vj8TcCVFo6Ouh7pNZWWPm/IOlu+1nVhkREdU9Bh1HZadTzCszqG1DLH5kALoUz746n1+Ih77dhndWHkJRkWFLzYiIqBIMOo7KgXp0Sgvx88QPk/pgdLcmlvveWXkYk7/bhuy8Al3bRkREtodBx1H5hTpcj46Zp5sLZt7aBdNGtIN58tXSPWcwZg5XUiYiorIYdIzQo5PuWEFHyPTyB65sic/HR8HXQ1sOStbckSLlzce16ehERESGDDqzZ89GZGQkoqKi4LAcsEanIrKA4MLJ/RAR5KVuJ2fl4Y5PNuD7TbF6N42IiGwAFwx01AUD5b/1v42B/GwguC0wZRMcWWp2HqZ8tx3rjpy13HdLjzBMv6EDfIp7fIiIyDFwwUCSsZ2SXh0HKka+mAAvdzX9fEL/CMt9P22Nx3Wz1uJoUqaubSMiIv0w6BihTic3DcjLgqNzdXHG9JEd8L9busDb3UXddyI5W623w7odIiJjYtBxZGXqdBy/V8dsTI8wLH1sINo18lW3U7PzcecnG7FgWzzX2yEiMhgGHUfmoGvpWKNpkBd+erAvrmgdrG7nFRZh6o87MWTmaqw6kKh384iIqI4w6Bgm6DjuzKuL8fV0w+f3ROHWnmGW+46dzcK9X27GR6uPwsB1+EREhsGg48gMMsW8Mm4uznh9TGd8NK4HoiLqq/sk38xYegBPz9+F3IJCvZtIRES1iEHHkRl46Kr84oLXdGiEHx/oiyeGtLHcP39rPO76dCOSM3N1bR8REdUeBh1Hxh6dCwLPY0NaY/Yd3eHppr30Nx8/hxveX4/Vh5L0bh4REdUCBh1HZtBZV5dyXedQ1bsT4uehbp9MPY/xn2/Co/O2c2NQIiIHw6DjyNy9AQ9/7Zg9OmV0DgvAL5MHoFdEoOW+X3eews1zohFz1vHXHCIiMgoGHUdXenVkzjIqo5G/J354oI/aCd28TcS+0+kY8e4afLLmGPIKivRuIhERXSYGHUfnV1yQLHte5abr3RqbrNu5qXsYFj7cD82DvdV9OflF+M+S/bjmnTXYHZ+mdxOJiOgyMOgYaeZVOoevLqZ1iC9+f3QA7u7bzHKfDGGN/Tgaf+5P0LVtRERUfQw6jo4zr6zm5e6Kl2/siMVTBqBLeIC6LzuvEPd9uQUPf7sVCek5ejeRiIiqiEHH0XEtnSrrFOaPHyb1wYiOJSFxye4zaijr912nuaIyEZEdYdBxdOzRqRZPNxe13s5rN3VCsI+7ZXPQyd9tw6jZ61m7Q0RkJxh0HB17dKrN2dkJt/VqihVPXInhHUoC4874NNz84T/4ZcdJXdtHRESXxqDj6Nijc9nqe7tjzl3d8fG4HmjXyFfdl1tQhMe+34GpP+xA2vl8vZtIREQXwaDj6Hy4OnJNTUMf1qERfpnSv8xu6Au2n8ToD9YjNjlb1/YREVHFGHQcnas74BWsHbNH57J5uLqo3dDfuqULfIsXGTyWlIUbZ69TQ1ksVCYisi0MOkaq05EenSKu9lsTvTs39wjDkseuQOuGPuq+c9n5aijr7s834UQyt5AgIrIVDDpGqtMpygfOp+jdGocRHuiF+Q/1w7WdSoYH1x4+i2Fvr8G7Kw8jJ79Q1/YRERGDjjGwILnW+Ndzwwd39sCnd/dEY39PS6Hy2ysPqcCzch9XVSYi0hODjhFwinmtGxIZghVTr8T9A5rDxdlJ3Rebko37v9qCe+duZrEyEZFOGHSMgD06dcLbwxX/uj4SSx+7An1aBFru/+tAotoRnevuEBHVPQYdI/BrXHLMHp1a1ybEF/Mm9sGs27shxM9D3ZeVV6iKlact2M3aHSKiOsSgY7QenfRTerbEUDOzbujSGH8+OQg3dW9iuX/eplgMmbkas1cdQUYOFxokIqptDDpGwBod3fh4uGLmrV3x5s2d4emm/bjFnzuPN/84iKEz12D5Xv5/EBHVJgYdI/BuADgV/1ezRkcXt/QMx69TBmBgmwaW+86k52DS11vx6m/7OJxFRFRLGHSMwNkF8AnRjtmjo2vtzlf39sKqpwZhUNuSwPPpuhj0eGUFnpm/E6nZebq2kYjI0TDoGK1OJysRKCzQuzWG1jzYG1/cE4VXbuwA1+Kp6FKs/OOWeFw3ax22njindxOJiBwGg47R6nRMRUBWkt6tMTwpVh7XNwILH+6PW3qEWfbNOpl6Hrd+FI0P/j6CgkJu10FEdLkYdAxZkMw6HVvRKcwfb97SBcunDkRURH11X2GRCW8sO4jh767F+iNn9W4iEZFdY9AxCgYdmxbqX0+tvTNlcCs4aaNZOJKYiTs/3ahqd44mZerdRCIiu8SgYxRcHdnmubo446lr2mLBQ/3QvWmA5X6p3bn6f6sx9YcdyC3g7Cwioqpg0DEKrqVjN7o1rY/5D/bDq6M6wtvdxXL/gu0ncc/nm7F092lk57GgnIjIGloFJDk+9ujYFWdnJ9zVpxmu7RSK+VvjMHPFIeTkFyH6WLK6hPp74osJUWjXyE/vphIR2TT26BgFe3TsUqC3OyYNbIlv7++N+l5ulvtPp+Xglg+j1UahJpNJ1zYSEdkyBh2j8AoEXNy1YwYdu9OjWSDWPDMYn9zdE13C/NV9GTkFaqNQCTyrDiQy8BAROWrQGT16NOrXr4+bb75Z76bYLpnKYx6+4tCVXfL1dMPQyBDMm9QH13Uq6aHbcuIcJszdjOHvrMWS3fy/JSJyuKDz2GOP4auvvtK7GfYzfJWdDBTk6t0aqiYvd1fMvrM7PhvfEy0aeFvuP5iQgYe/3YZ/LdrN2VlERI4UdAYNGgRfX1+9m2FnBckcvrJ3V7cPwconrsRH43qgW6np6N9siMWVb/yNT9YcQ2YuZ2cRkbHpHnTWrFmDkSNHonHjxmpZ/EWLFl3wnNmzZyMiIgKenp7o3bs3Nm3apEtb7R4Lkh1ydtY1HRqptXfeuLkz3F2dLTuj/2fJfvSb8aeasZWWna93U4mIjBl0srKy0KVLFxVmKvLDDz9g6tSpmD59OrZt26aee8011yAxMbHKXys3Nxfp6ellLobCKeYOS/5IuLVnOBY93B/DIot3qgeQnlOAWX8exoDX/8L/lh9E2nkGHiIyFt2DzogRI/Dqq6+qguKKzJw5ExMnTsSECRMQGRmJDz/8EF5eXvj888+r/LVmzJgBf39/yyU8PByGwh4dhxfZ2A8f390TK6cOxM09wiy7o2fkFuC9v45g9Oz1SEzP0buZRETGCTqVycvLw9atWzFkyBDLfc7Ozup2dHR0lT/ftGnTkJaWZrnExcXBUNijYxitGvrirVu6YNVTg3B7r6Zwc9ECz7GzWbhpzj94afFetWEop6QTkaOz6aBz9uxZFBYWIiSkpCteyO0zZ0p6JCT43HLLLViyZAnCwsIuGoI8PDzg5+dX5mIo7NExnPBAL8y4qRNWPHElwurXU/fFnzuPL9YfVxuGjnh3Lf7hDulE5MAcYguIlStX6t0E+ws6aQbrzTK4iGBvbXf077ZhZ3ya5f4DZzJwx6cbMaR9QzXUNSyykSpwJiJyFDYddIKDg+Hi4oKEhIQy98vtRo1KDcOQdTz9AJ8QIDMBSDqod2tIh96dX6YMwNnMXEQfTcYna49hV3HoWbk/UV1kIcKHBrXEuew89GsZDBeGHiKyczY9dOXu7o4ePXrgzz//tNxXVFSkbvft21fXttmtBm216+yzQBaHLIwo2McDI7s0VjO0/jO6Ixr6elge+333aVz/3jqM+2wTHvxmKwoKi3RtKxGR3QedzMxM7NixQ11ETEyMOo6NjVW3ZWr5J598gi+//BL79+/HQw89pKakyywsqoYG7UqO2atjaDJEdWfvZoiedjVm39Ed9dxcyjy+Yl8Cnpm/i2GHiOya7kNXW7ZsweDBgy23JdiI8ePHY+7cuRg7diySkpLwwgsvqALkrl27YtmyZRcUKFeFrNkjFyl0NnbQ2Q9E9NezNWQDZHjqus6haFK/Hl5buh8xZ7OQkK5tEbJg+0kkZuTitTGdEFbfS++mEhFVmZPJwPNLZcFAWU9HppobZgbW8fXA3Gu1416TgGvf1LtFZIOW7TmNR+ZtR36hybInbJ/mQRjTIwzXdw6FZ7neHyIiW33/1n3oivTs0TmgZ0vIhg3vGIrvJvZBkLe7ui1/DkUfS8ZTP+3EtbPWYlvsOb2bSERkFQYdo/EOArwbaMeJDDp0cVERgVj2+EA8ObQNmgeX7JJ+LCkLN33wD8Z9thGrDiSiqMiwncJEZAc4dGW0oSsx93rg+Frt+JkYwCtQ7xaRjZNfE1tPnMMrv+0rsw6PaNnAG2+P7YrOYSU7qBMR1SYOXZF1U8wFZ16RlZuG9owIxM8P9cOLIyMRHqitsiyOJmXhlg+j8cmaY9w0lIhsDoOOEbFOh6rJ1cUZ9/Rvjr+fGoxP7u6JzmH+6v7cgiL8Z8l+9Pnvn3jzjwNYvvcM1h0+y6npRKQ73aeXkw4YdKgGpqQPjQzBwDbBeGnxPny3UVv36nx+IWavOmp5Xscmfnh9TGd0aKwFIiKiumbIoGPodXQEgw7VEA9XF/x3dCfc0y8CX0Ufxw+b4yxT0sWek+m44f31GBsVjsJCE9qF+qrnylAYEVFdYDGyEYuR5b/8jRbA+RRto88nGXaoZsSlZOPXnadQWGTCb7tO4VBC5gXPeW5EOzx4ZUtd2kdExnv/ZtAxYtARX1wLnFivHT97AqjHGTNUs/IKivDB30cwe9WRMr080pkzvEMjdGzij1t6hKGhn6eu7SQi+8OgYyVDB53fngC2fK4d37scaNpb7xaRA/fy7D6ZhgXbTmLl/oQyj7k6O2FM9zA8PrQ1Qv1LZnIREVWG08vp0linQ3UkPNAL13YKxcfjemBC/whVyGxWUGTCD1vicNVbq9W2Ewb+u4uIaokhi5GJu5iTPrulTx/ZQdXoxKWcx8Lt8fgq+gQycgrUbK0Hv9mmdlCv5+6Coe1DMK5vMzW8RUR0OdijY1TldzEnqsOZWq0a+uDpa9phzdODcWPXxpbHJPCkZOWpXp6R76/D/y3cjdTsPF3bS0T2jUHHqHwaAp7FBcjs0SGd1Pd2xztju+KZ4W3RwNcDLRp4w8td2xldRrG+3RiLq/63Gj9uieOeWkRULSxGNmoxsvjsGiBug3b8XBzgacBzQDYnJ78QX0efwDsrDyErr2Stq67hARjRsRF6twhC5yb+aiiMiIwpncXIlZPFAiMjIxEVFQVDa1hq+OrsIT1bQmTh6eaCiQNb4M8nB+G6zqGW+3fEpWLG0gMYNXs9+r32l9o5nYjoUtijY+QenQ1zgGXPacc3vA90H6d3i4guIHtmTf91j9o8tDRZj2fiFS1UwfKAVsEI9HbXrY1EZLvv35x1ZWRldjHnFHOyTQNaB2PFE1ficGImtpxIwS87TmFTTIqq4fl4zTH1HE83Z4zu1gSjujZBr+aB3GKCiIw9dEXFGrQvOWZBMtkwqcdp28gXd/Zuhm/v742r2jUs83hOfhHmbYrD2I834L4vtyArt0C3thKRbWGPjpH5NgI8/IHcNPbokN1wc3HGB3d2xzcbTiA7T5uOLrOy5Fj8dSARt34UjVdGdVQzuGQ6e/Ngb72bTUQ6YY2OkWt0xKdDgfhN2vG0eMDDV+8WEVWZ9OAs3XMGLy/ei/ScC3tzuJEokWPhrCuqXp0OZ16RnfL2cMXNPcIw/6F+FfbevLb0AKYt2IVTqefVAoRck4fIODh0ZXQNy9XpNOmhZ2uILkubEF/88fhAtbLyyn0JambW3weT1GNSwyMX4evhiq5NA/DAwJaq2JmIHBeDjtGV7tFJ5FYQZP/cXZ0xrk8zdRHfb4rFS4v3qe0lzDJyC7D28Fl1ubJNAzWs1acFZ2sROSIGHaPj5p7k4G7r1RRXtW+Iz9bGYO+pdNXLcyghAwnpuerx1YeS1KVpoBdu6RGGewc0V0NhROQYDFmMLCsjy6WwsBCHDh0ydjGy/PfPCAfyMoCAZsDju/RuEVGtkxqdX3aexFt/HMLJ1PNlHpM9t4a0b4ir2oWoa/byENl3MbIhg44ZZ10V++Rq4OQWeTkAz58E3DkVl4whv7AIv+86jflb4/HP0bMoX6M8vm8zTLu2vdqWgohsB4OOlRh0ii2aDOz4Rjue9DfQuJveLSKqczFns/D60gNYvu/MBYEn2McDTQI80TigHiJD/XBP/wj4errp1VQiw0vnFhBU/a0gDjLokCHJtPQPx/VQa/LINhMv/LIHBcWJ52xmrrrsjE9T6/XM2xSL569rj2s6NFILGBKR7WLQoXJTzLlCMhmbFCLf0bsp2jbywdfRJxCbko1TqTlIyMhRJW3iVFoOpny3XfX03DegOe7s0xR+7OEhskkMOlRuijmDDpHo0SxQXczyCorU8NbLv+3F+iPJ6j7p5Xl92QG8veIQekbUVzuo92hWH7f3asq6HiIbwRod1ujIFBRgRhiQnwXUbw48tkPvFhHZLPmVue7IWXy3MRbL9p6x9PKU1tDXA1OuaoWxUeFqry0iqlksRrYSg04pHw8GTm3TZl7932nArZ7eLSKyeUeTMtXw1tI9py3r8pTm6eaMto38MPGK5ri+c2Nd2kjkiBh0rMSgU8rCh4Cd32nHD6wFQjvr3SIiuyG/RmUXdVmT54NVR1VPT3kyW8vX01UNa43q1kSXdhI5Cs66osuceXWAQYeoCmRRwSAfD3WRmVu749Pwydpj2BWfiuPJ2eo5+06nq+uNMSlqJebuTQPQPtQPHZv4s56HqBYx6JCGM6+IakynMH/Mul1bpuHXnacwY8l+nE7LsTy+cPtJdRFuLk7o0NhfrcI8olMo/Ou5qdlcRFQzGHSo4rV0iKhG3NClMUZ2DlVr8izcdhL/+mWPmsFlll9owo64VHV5a/khdZ/M3JpzZ3c09PPUseVEjsGQQaf0XldUzL8p4FoPKDjPXcyJamFoS3pubo0Kx9XtG6rNRePOZWNHbCq2xp7DsaSsMs/feuIcbprzj9qBfUhkCFo28NGt7UT2jsXILEYu8dFA4PROwMkZeF5mXvGvSaK6EH8uGwu2ncTBMxnYFnuuzDCXsxNUAfPgtg0R4OWm6nmktsdFHiAyqHQWI1O1NGivBR1TEZB8BGjUUe8WERlCWH0vPHp1a3WckJ6DSV9vxc64VHVbdqH4dmOsuphFBHlhZJfGqp7n2k6hag8uIqoYgw5VXKeTsIdBh0gHIX6eWPhQPxxJysSyPWfw4eqjyM4rO8wuM7ne++uIOn73z8OYeWtXDI0M0anFRLaNQ1ccuipx7G/gqxu146Z9gXuX6d0iIsOTbSbWHk5CTFKWCjxS3xN9TNuCorSu4QG4p18ERnRqxNWYyeGlc8FA6zDoVLAVxJy+JdPLJywDmvXVu1VEVM6hhAy1OOGPm+PUburlV2OWXqG2Ib7o0yIIN3VvggAvd93aSlQbGHSsxKBTgR3zgEUPasetrwHu/FHvFhHRRciv75+3ncSna4/hwJmMCp9Tz81FDWtFRdRXm5S2beTLQmayeww6VmLQqUBhPjCrG5AWp91+cD1rdYhsnPwa33z8HL7ZcAJ7T6WpfbcycwsqfK6Ph6vaaf2JIW3QJTygzttKVBMYdKzEoHMRGz8Clj6jHXe6BRjzqd4tIqIqKCoyqR6eHzbH4qet8RcUMwt3V2f867r2uLFLE/h7uenSTqLqYtCxEoPOReRlA+90BLKTtTV1HtkGBDbXu1VEVA35hUXYdyodW06cw9YTKarnJymjZKd1GcVqGugFeSM4m5Gr1ux5bkQ7uLo469puosow6FiJQacSq98EVr2qHfe8D7h+pt4tIqIaINtPTP91L+ZtKlmXp7wBrYIx46ZOauHCf46eVSs3d2jsh9t7N4WfJ3t/SH8MOlZi0KnE+XPA2x2BvEzAxQN4Yg/g01DvVhFRDZBf+5tiUvDH3gRsPp6Co0mZkPLk3IIitSfXxfh6uuLxIW0wvm8z9viQrhh0rMSgcwnL/wX88552PGAqMGS63i0iolokoeehb7aptXsqExnqh/sGNMeaw0lwd3FWs7quateQ4YfqDIOOlRh0LiH9NPBuZ6AwD/Dw03p1PP31bhUR1aL0nHzM/usIft15Su2pdV2nUEQEe+O7jbH4eVv8RT+uUxN/vDKqI7qE+SM5K0/N7pJ9uYhqA4NOFXYvP3ToEINOZX59FNj2pXY85EVgwBN6t4iIdLI99hye/XkXDiVkXvQ5Xu4uapaXh6uz6vV54MqWak8uoprEoGMl9uhYIfko8F4PGdUHvBsCj+/mruZEBpaTX4j3/zqCXSfTcEuPMLUg4evLDuBwYsXhx9vdBa1DfBFzNkttRnpVuxBMHNgcXu7capGqj0HHSgw6VvpxPLBvkXZ83Uwg6j44pII8IPUEkBIDnIspe52VpD3HyUmbco/ia/PthpHANf8FGrTR+7sgqnO5BYX4cUs8VuxLwJGEDDSpXw8749KQV1hU4fM7NvHDR+N6ogl3XadqYtCxEoOOlU7tAD6+UjuuHwFM2Qq4OMBfY/LSP7UN2PUjcGgZkBoLmCr+xWwVNy9gxOtAt3FaALJWfg7g6lG1jyGycbHJ2fhozVHM3xqvZnM18PW4YP2enhGB6BYeoIJRQ19P9G8VhJz8IrWPV+cm/nDmVhV0EQw6VmLQqYKvRwNH/9KOx3wGdLoZduvccWDXT8CuH4Dkw5d+vkyv9wvVem4kCKkLikORCcjNBHLTSp7fYTRw/TtAvYDKN1A9uARY+xZwajvg6gn4NgJ8Qy+8Dm4DhHZhECK7HerKzS9Sqy8fOJOO++ZuUUGmIjKDy9wL1Kt5IN67vZvaoFQWPZRXP2d1kRmDjpUYdKogZi3w5fXacXBbYMISwDsYdiM7Bdj3ixZuYqMvfNzFXRt+khWgA1sA9eW6uXYtYcPZufKVpP+YBmydW3Kff1Nt64ymvcs+t6hQa8eat4DEvda3P6QTEHUv0OlWwMPH+o8jsjEpWXn4ZO0xLN19GseTsyt9boCXG67tFIrFO06pLSumXdseY7o3gRNDv+GlM+hYh0GnCuRl8ukQ4OQW7bZPI+Cmj4EWxUNattZW6bWJ2wTEbdSuJVRUNCzVbADQ+VYg8sbKe2CsIQHm10eAnOLeHScXYNA04IqpWpv2LtACztmDZT8uqDXg7ApknAZyUiv/Gu6+QJex2mrVIZGX114inSWm52Dv6XQkZ+ZhZ1wqVh1MhK+nG85l5eFMek6FH3NF62A8dnVrJGbkYuuJc6o+qHfzILWOj7eHAwypk1UYdKzEoFNFp3cC34wpKcyVzmSZbj74ecBF5+mjifuBwytKgk1W4sWfKz1SEhZkw9KApjXbjtQ4YMHEsr1G4b21c5ZyrOxzm/QArnwWaD2sZFgq/zyQcab4chpIP6kFqPjNF36tpn2BqPuB9iO1Gh8iB+r1eXnxXizacUrdllKdShZsVkL8PPDksLZIP5+Pbk0D0KNZYN00lnTBoGMlBp1qyEgAFj4AHFtVcl9YlFa3U79Z3bZF6lyO/glEvw8c+/viz5PampAOQMRArfemtutdCgu02pvVr1fciyQBZeDTQMurrG+HhMzNnwG7fwLyy3X3ewUDXe8AetwDBLWsme+ByAbsjk9Te20NiQzBieQs/N/CPWr/rUtxcXbCx+N6qOEu2dKieZA3mgV5ccjLgTDoWIlB5zICxj+zgL9eAYoKtPs8/IEb3tUKcatKalwS9mgXGZqRIBLU6uJ1MdLrsfN7YMOcC4eBzG0Jj9J6UsJ7aT0nHr6ocyf+AX6eCKQXryYbcYXWgxMxoPpBS4bF5HuX0FPR9978Si3wtLsecHW/vPaTY5I6sbT4sksoyIxDqUfr9wjgZbs9IRk5+fjyn+OqtkempncO84ebizM+XnMM646crfRj24b44uYeYRjVrYmaAUb2jUHHSgw6lyl+K/DzvVo9jJlMre54k7ZlhIQL88XNWwsuMpU6Ya82rVumrcuMo6QDgKmw7Od29wEadQJCuwKNu2rhR7af2PK5dslOLvt8mfbea5LWSyJDU5UVD9f15qgy9CSFzhK6aor82J5YrwWe/YuBovyyj3s3ALreCfQYr72B8S9Z+yU/XzIsK//f0nsnQbZRR+s/PisZ2P41ELOmONTEXfh6MZPauxtmAW2ugT0pKjLhp61x2HcqHd9tikV+oanS3p4r2zTArT3DMSwyBIt3nVLT3ruGB6hLTkERVh1IRJewADQN8qrT74Osx6BjJQadGpCTDvz2BLBn/iWe6KQFHhl2MfcC1YSm/YC+k4G2IwBng+6rk5kE7PxOm/VVvg7IPKOsXn3AM0C7lqJr8+0GUq90O1e7tiUFuVpvoISbIyuAs4cufI70DvZ+sPLX/cltwKZPgD0/A4WVb9J5ga53AcP/e+m97eQPlwO/aa+7pn204n6d/8jIzivAA19vxcZjKRgS2RARQd7YGJOiCpfLC/X3LDMUJr1ERSaTus/TzRkvXN9BXTfy90RURKDqPSLbwKBjJQadGiIvoR3fAUueurB+5FJkZlLD9lqvTaMu2no00tMjNSlpcRV/jMxQkiGyPg8DTbrXyLfgMEOKx9dogWf/bxf/q728Jj2B277V1u0hfchaTNLzJ6Hh2GogP8u6j5NieunJlJ5UCbASkvYuAjZ9XDJDsvyilpalEyJKllDwCgL+elULVmZ+TYAb3gNaXX3h50k8oO2Bt3Oe1mtZelmFLrdpFx3rxcxva6Vrco4lZarFCxdtP4lTVtT5lOfn6Yq7+jTD6G5N1OKGsi4Q6YdBx0oMOjXs3AntF7X84svNKL6ka70+5tsyO0iGodSQVDetC96t3sW73E9L6DEHn5NARH+g1wOAf5O6/u7sr5dnxzfaIo+yhtD5VO3/5WJvoPKmdtt3WuCkuiG/emV23vZvgb0LK/6/kUL6sF5A66FAy8FaL83GD4HkI2WfJ0PDbYZp611ll6tVkV4ZCUKqWL3VxYcxpT0yxLXseSAvo+T+HhOAYa9of2BIGJMgXdFaVBUV3UtvYYdRl+4Zqg3y/UhPk6z3VerryzDX8n0JmP7rHiSk56Khr4fafHT90WSsOaTNKA0PrIe4lIoXNTS7pkMIJg9uhchQP8tChvK51a4wHCqudQw6VmLQIcOR/bxkrR4JPrKv1+LHS4qlXesBo+dUr6CcrJd+SusJkYCTcvTCx6W+qtVQoPUQoMXgC4uD1WzDv4CNc4AjKytfZLLXRG0ZBfcq1JpIYfIvU4CY1WWDcF5myRpRpVcNlyDTrJ9WKybtKj/TUFb9VrVzrYt7k1poPUnyOSsadpNZixLWMhO0wC6hq0F7bYXwSw2LqW1dtmuBTC5SkyTDtGO/1f5IKlfYvC02VU1F9/PUemcOJWQgNTsfURH18cPmOLVxadNAL+w9lY5le05XWPsjm5p2aOyH8/mF2Hc6XTVBip37twzCuL7NOM29ljDoWIlBhwxPlgv44c6y6/TIIocDn7n4m4oUx8rwiAyNyJuWTNuX+gyjrtgsPY9SUJ+0Hzh7uPLhW5ntJEshlA8DUrzfcYy2TIAMJVpb5yJfb+NH2tCx9AhJr0v7G7ThLPk/qW7PgrwtbPkMWP5CxT1NDdppPT2yXEPpIJZ+Gtj9I7BjnnY+KiO1YwHNtCG0wjxtrSkJN9IDqfZYwYXnSGZQhvXUlrSQ8+QdpLX15FZt42EJNxLUKvpao+Zc1tY1p1LP4/tNsYg/d17N8JIFCy9FTv+Efs0xoX8Egn081H+rh6uLWifoeHKWKniW4miqOgYdKzHoEBUXlP72uNbLYBY5SntjMPcEyEwdeSPZs0CbMVeevMHKm5AUyTa/Qpvaf7EhSXslb8AyY1BCjSxQmXRQOy4/VFQVzQdqhb+y6GNVel3Kk54WaZsEz5qstZLp579M1mZ8Sc9Mh5u0ITCZQVhZiDL3rMhravd84LyEl1ogPUSF+SW9kuWH/fzDygafoS8D/R697FmIUvAs9T5S5LwrPlUNc8mnbNPQVxUvH0nMRFZeuZmk0rnl7KT28NoWe05tXtqqoQ8Cvd3VrK9BbRvggYEtVeEzXRqDjpUYdIiKya+Bf94DVrxQ8te01FLJsMfFVmaujPwFLcFHFmqUv/7lIkXn9rA/mgQ/WaMoYZ+2tlOiXO8DMs/UzOf3D9d6buQivRm2TobKZAsVKXyuTq2NrNsjYUOt23NMC0/m9XvkuqC4FkaClHdDwKf4IkN4ci2vJanRk9eg9PhcanKDbEsjW7rIWlLSXpkVKrVHZrKa+Ig3anSWZmp2ntpp3TwElldQpNb7efOPg5ZNSq0hw2BPDmuDNiG+OJedh+y8QrRt5Kt2cpc6INkgNSu3AEE+XAconUHHOgw6ROUcXAb8fH/ZYtTyZH0j+cteCmSlKFYKYI+vrXgadHkyu0fqLRq2A0I6asMQctvFtWp1RlJYLaGpJt6sZAHK2A1aTYqsNSOz/sqv63QxPiElQU6m6sulXiU1GfKmLb0utrLOk97k7UeGrCTkyPITl+olkqE/CTzxW7ShU/P/VYtBWi9ku+surGmSj5M95la9WnJfmxHAzZ8B7t61831JD2hsNE75d8WCo05Ye/is+tZizmapAmjZpb1lQx/sP52uni7DV4WV7HHhX88N/VsFYfXBJNVTFBHkpWaASRG1KF38LPuESU1R7xaOPR0+nUGncrNnz1aXwsJCHDp0iEGHqDQZlvlurFasbCahRIpOI0cDwa0q/jipzzi+TpvirhanK7WQZGVkxpAsEyChp0lx/YVviFaUKj0AUuuRWOoiBbyyFpMMl8nQhPQ0SK2HushxU603QFaGlmJZ2YdNZvtJyJCPkR4GGVaJ+Vubyi17o11qnRkpaG3YQdtIVRZ/bFhcHGvDqwgbggxbyVuYNauAS93Qr1NK1vFq3B244wet16gmyOvfPLwbv6mkwF/2ApSlMFxcVZjZGZ+KsPr10MDHQ63w7OLkBC8PF7y78jC+3lDqZ84KMuNLan3k+q1buqhi6qfn70Ja8X5fcyf0QkFhEV74dS+cnZzw6qiOKjQ5AgYdK7FHh6iSAtvo97QQEnmD1lNRnZoWVcdSXM+i6loOXHr4wbxCr9R1SJFqjZK5v86V99hID5MM24WYg00Hre6FU4btnxSC/zBOW/ZCSDju/5jWOyi9jV7F1xJsrelllFlh+3/Rwo0s8lhREbWQ15OsSSTXldh6IgXL9yaonpj63u6qpmdTTAr+PJCganpk7y4JNRKWrHnnDvJ2R3JWyc/Q0MgQtQeY1BDtPpmGPi2C0DjAPmvpGHSsxKBDpAMVgA5ovSrmYYiLLQ5ZnvTQSE+KX6j2F7T0OpnftKpLeoBkjzAZ/pDi4Jr6C59skxRtf3sLkH6ykic5aQswykwvGR6V2h8JyJZjJ60nSWqXKtq419zrJwHIHH7k42QvsUHPVblQPz0nH4fOZKBZkLeauv7n/gQ8Om+7GsaqaGd3NxenSrfBMAvwcsNX9/ZSBdGyCKIEKXvBoGMlBh0iG5FxRgs85uAjBcASOOTNQtX0FF9klk35v7TNawJJwasqej2hrRUkvUFSz1NY/pKvLZwnRasScGRNFzLeWkbf3gok7K65zxnUWtvnT+rXpAZNxG0Gfn2k7FR7eQ3LfmISqi9DYkYO4lKy0bKBD2YsOYANMclq49JrOjRCl3B/PDJvh6UGKNDbXU1pr0x9L6kDCsaxpCy1NYbsGC/Da8G+Hmr396qsBC07zUtwqudee9vyMOhYiUGHiMigJPBKAbr0DMomwbJMgPQ2Zp0tvp2s9RZKj41cZPaZOi4svl0I1G+mFUFLwJE6toqGNyVsr38HWPNm2aHYTreWDAlbPs6p5LYMo0m9mjULJVZA3tplGvvu+DRc2ykUP22Nx6w/D6O+l7ta+Xnz8Qv3/qpMnxaBuK5TKFqH+CK3oEj1MMWmZKNnRH0MaR+C5Mw81Pd2w0uL96mp960b+uDXKQNqLeww6FiJQYeIiOqE1Kn9+igQt6FqH+fhD4TJQom9gPDihRJlWO0yZeYW4Lmfd+FwQqYaDttwLBkFlcz8qo5Hr2qFMT3CcOBMhuppqkkMOlZi0CEiojojvUJbPwdWvFj5Eg6XIssZyCrPfafU2MKcZzNz1erPsojhvlPp2BWfptbsOZ2egw1Hk3HsrJUbzVbA290FW/41tEZ7dxh0rMSgQ0REdU6GyKQWTQ2Dmd+CTSXHMjQmSytIzZosf5CVWPHnkVljsvhh2+G12lyTSabFp2FnXCpOJGfD28NFzdaSGp4F2+NxOi1HbY4qjwkJNNtjU8t8jvdu74aRXRrXWJsYdKzEoENERDZN3qKl2F6CkYQeWaPn9K6ySyS0GQ4Mf81mCuszcvJx85xoHEzIQPtQP9zYtTFGd2uCEL+a296CQcdKDDpERGR3Eg8AS57SViQvvfTCFVO1dYFsYJ+5nFrerqIq79/2M2meiIiItOnr4xcDN38O+IZq98nq3n/PAGb31rZy0Zmnm4vN7MnFoENERGRvZAp6xzHAlM3ajuyyvYmQYa55Y4EvrgWOrCxVA2RcDDpERET2SjZDHfYK8OB6IOKKkvtPrAe+GQN8NBDYu0hb98egGHSIiIgcaThLVmk2O7ML+Gm8NqS1/VttoUSDYTEyi5GJiMiRSO/Ngd+Atf8DTu8s+5h/uLaSs6zYLCs/56Rp17kZQE46kJ8NNOkBXP1C9TbzrSOcdWUlBh0iInJY8vZ+9E9g7UxtKKsqpOan1yTgyme03dxtDGddERERGZ0ULLcaAkxYAtz7B9D6mkqe6wx4+ms7touiAmDDB8B7PYAtn9t1jQ97dNijQ0RERnH2CJB6XAs0cvEsvnb31oJRXjbwz3vAureBgvMlHxfSCRjxGhAxALaAQ1dWYtAhIiKqQGocsHI6sOfnsve3H6nt1F6Qq+3GLsXNsoaPXJvva3cd0OU22Mr7d/HEeyIiIqJiAeHaDK6o+4Glz2qzt8T+xdqlMvWbwZawRoeIiIgq1qwfMOlvYOQswCsYVpEZXTaEPTpERER0cc4uQI/x2rT0uI3abC4XN21/LRd3wNVduzZfZBFDG8KgQ0RERJcmAUZmcdkZDl0RERGRw2LQISIiIofFoENEREQOi0GHiIiIHJYhg87s2bMRGRmJqKgovZtCREREtYgrI3NlZCIiIrvCTT2JiIiIGHSIiIjIkTHoEBERkcNi0CEiIiKHxaBDREREDotBh4iIiBwWgw4RERE5LEPvXm5eQkjm4xMREZF9ML9vW7MUoKGDTkZGhroODw/XuylERERUjfdxWTiwMoZeGbmoqAinTp2Cr68vnJycqpwmJSDFxcVxVWUr8ZxVHc9Z1fGcVR3PWdXxnOl7ziS6SMhp3LgxnJ0rr8IxdI+OnJywsLDL+hzyn8UXedXwnFUdz1nV8ZxVHc9Z1fGc6XfOLtWTY8ZiZCIiInJYDDpERETksBh0qsnDwwPTp09X12QdnrOq4zmrOp6zquM5qzqeM/s5Z4YuRiYiIiLHxh4dIiIiclgMOkREROSwGHSIiIjIYTHoEBERkcNi0Kmm2bNnIyIiAp6enujduzc2bdqkd5NswosvvqhWmS59adeuneXxnJwcTJ48GUFBQfDx8cGYMWOQkJAAI1mzZg1GjhypVvSU87No0aIyj8v8gBdeeAGhoaGoV68ehgwZgsOHD5d5TkpKCu6880616FZAQADuu+8+ZGZmwqjn7J577rngdTd8+HBDn7MZM2YgKipKrfzesGFDjBo1CgcPHizzHGt+HmNjY3HdddfBy8tLfZ6nn34aBQUFMOo5GzRo0AWvtQcffNCw52zOnDno3LmzZRHAvn37YunSpTb1GmPQqYYffvgBU6dOVdPktm3bhi5duuCaa65BYmKi3k2zCR06dMDp06ctl3Xr1lkee+KJJ7B48WL89NNPWL16tdqC46abboKRZGVlqdeMhOWKvPHGG5g1axY+/PBDbNy4Ed7e3ur1Jb8wzOQNe+/evVixYgV+++03FQQmTZoEo54zIcGm9Otu3rx5ZR432jmTny95g9mwYYP6nvPz8zFs2DB1Lq39eSwsLFRvQHl5efjnn3/w5ZdfYu7cuSqIG/WciYkTJ5Z5rcnPrFHPWVhYGF577TVs3boVW7ZswVVXXYUbb7xR/azZzGtMppdT1fTq1cs0efJky+3CwkJT48aNTTNmzDAZ3fTp001dunSp8LHU1FSTm5ub6aeffrLct3//flnewBQdHW0yIvneFy5caLldVFRkatSokenNN98sc948PDxM8+bNU7f37dunPm7z5s2W5yxdutTk5ORkOnnypMlo50yMHz/edOONN170Y4x+zkRiYqI6B6tXr7b653HJkiUmZ2dn05kzZyzPmTNnjsnPz8+Um5trMto5E1deeaXpscceu+jHGP2cifr165s+/fRTm3mNsUeniiR1SnKV4YTSe2bJ7ejoaF3bZitkmEWGGFq0aKH+ipZuSSHnTf5CKn3uZFiradOmPHfFYmJicObMmTLnSPZzkeFR8zmSaxl66dmzp+U58nx5HUoPkFH9/fffqtu7bdu2eOihh5CcnGx5jOcMSEtLU9eBgYFW/zzKdadOnRASEmJ5jvQuyuaM5r/YjXTOzL799lsEBwejY8eOmDZtGrKzsy2PGfmcFRYW4vvvv1c9YDKEZSuvMUNv6lkdZ8+eVf+Zpf9ThNw+cOAAjE7ekKXbUd5spEv3pZdewhVXXIE9e/aoN3B3d3f1hlP+3MljBMt5qOj1ZX5MruUNvTRXV1f1y9io51GGraQ7vHnz5jh69Cief/55jBgxQv0SdXFxMfw5KyoqwuOPP47+/furN2dhzc+jXFf0WjQ/ZrRzJu644w40a9ZM/TG3a9cuPPvss6qOZ8GCBYY9Z7t371bBRobXpQ5n4cKFiIyMxI4dO2ziNcagQzVK3lzMpEBNgo/8Uvjxxx9VYS1Rbbjtttssx/LXobz2WrZsqXp5rr76ahid1J3IHxul6+WoeuesdF2XvNZk0oC8xiRgy2vOiNq2batCjfSAzZ8/H+PHj1f1OLaCQ1dVJN2V8hdi+apxud2oUSPd2mWrJMm3adMGR44cUedHhv5SU1PLPIfnroT5PFT2+pLr8oXvMkNBZhXxPGpk2FR+VuV1Z/RzNmXKFFV8vWrVKlU4ambNz6NcV/RaND9mtHNWEfljTpR+rRntnLm7u6NVq1bo0aOHmrkmEwfeffddm3mNMehU4z9U/jP//PPPMl2cclu67qgsmb4rf+nIXz1y3tzc3MqcO+nylRoenjuNDL3ID3fpcyRj1VJHYj5Hci2/OGT82+yvv/5Sr0PzL12ji4+PVzU68roz6jmTum15w5ZhBPle5bVVmjU/j3ItwxKlQ6LMRpJpxDI0YbRzVhHpyRClX2tGOmcVkZ+r3Nxc23mN1UhJs8F8//33ahbM3Llz1WyOSZMmmQICAspUjRvVk08+afr7779NMTExpvXr15uGDBliCg4OVrMXxIMPPmhq2rSp6a+//jJt2bLF1LdvX3UxkoyMDNP27dvVRX4EZ86cqY5PnDihHn/ttdfU6+mXX34x7dq1S80mat68uen8+fOWzzF8+HBTt27dTBs3bjStW7fO1Lp1a9Ptt99uMuI5k8eeeuopNYtDXncrV640de/eXZ2TnJwcw56zhx56yOTv769+Hk+fPm25ZGdnW55zqZ/HgoICU8eOHU3Dhg0z7dixw7Rs2TJTgwYNTNOmTTMZ8ZwdOXLE9PLLL6tzJa81+Rlt0aKFaeDAgYY9Z88995yalSbnQ35fyW2Zzbh8+XKbeY0x6FTTe++9p/7z3N3d1XTzDRs26N0kmzB27FhTaGioOi9NmjRRt+WXg5m8WT/88MNq+qGXl5dp9OjR6heJkaxatUq9WZe/yBRp8xTzf//736aQkBAVqK+++mrTwYMHy3yO5ORk9Sbt4+OjpmFOmDBBveEb8ZzJm5D8kpRfjjKVtVmzZqaJEyde8IeH0c5ZRedLLl988UWVfh6PHz9uGjFihKlevXrqjxb5YyY/P99kxHMWGxurQk1gYKD62WzVqpXp6aefNqWlpRn2nN17773qZ05+58vPoPy+MoccW3mNOck/NdM3RERERGRbWKNDREREDotBh4iIiBwWgw4RERE5LAYdIiIiclgMOkREROSwGHSIiIjIYTHoEBERkcNi0CEiIiKHxaBDRFSK7Hju5OR0wUaERGSfGHSIiIjIYTHoEBERkcNi0CEim1JUVIQZM2agefPmqFevHrp06YL58+eXGVb6/fff0blzZ3h6eqJPnz7Ys2dPmc/x888/o0OHDvDw8EBERAT+97//lXk8NzcXzz77LMLDw9VzWrVqhc8++6zMc7Zu3YqePXvCy8sL/fr1w8GDB+vguyeimsagQ0Q2RULOV199hQ8//BB79+7FE088gbvuugurV6+2POfpp59W4WXz5s1o0KABRo4cifz8fEtAufXWW3Hbbbdh9+7dePHFF/Hvf/8bc+fOtXz83XffjXnz5mHWrFnYv38/PvroI/j4+JRpx//93/+pr7Flyxa4urri3nvvrcOzQEQ1hbuXE5HNkJ6WwMBArFy5En379rXcf//99yM7OxuTJk3C4MGD8f3332Ps2LHqsZSUFISFhakgIwHnzjvvRFJSEpYvX275+GeeeUb1AklwOnToENq2bYsVK1ZgyJAhF7RBeo3ka0gbrr76anXfkiVLcN111+H8+fOqF4mI7Ad7dIjIZhw5ckQFmqFDh6oeFvNFeniOHj1qeV7pECTBSIKL9MwIue7fv3+Zzyu3Dx8+jMLCQuzYsQMuLi648sorK22LDI2ZhYaGquvExMQa+16JqG641tHXISK6pMzMTHUtvS9NmjQp85jU0pQOO9UldT/WcHNzsxxLXZC5foiI7At7dIjIZkRGRqpAExsbqwqES1+kcNhsw4YNluNz586p4aj27dur23K9fv36Mp9Xbrdp00b15HTq1EkFltI1P0TkuNijQ0Q2w9fXF0899ZQqQJYwMmDAAKSlpamg4ufnh2bNmqnnvfzyywgKCkJISIgqGg4ODsaoUaPUY08++SSioqLwyiuvqDqe6OhovP/++/jggw/U4zILa/z48aq4WIqRZVbXiRMn1LCU1PgQkWNh0CEimyIBRWZSyeyrY8eOISAgAN27d8fzzz9vGTp67bXX8Nhjj6m6m65du2Lx4sVwd3dXj8lzf/zxR7zwwgvqc0l9jQSje+65x/I15syZoz7fww8/jOTkZDRt2lTdJiLHw1lXRGQ3zDOiZLhKAhAR0aWwRoeIiIgcFoMOEREROSwOXREREZHDYo8OEREROSwGHSIiInJYDDpERETksBh0iIiIyGEx6BAREZHDYtAhIiIih8WgQ0RERA6LQYeIiIjgqP4fpSdi6SxgqLkAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_len = len(val_loss)\n",
    "print(val_len)\n",
    "val_plt = np.zeros((2,val_len))\n",
    "for i in range(val_len):\n",
    "  val_plt[0,i] = val_loss[i][0]\n",
    "  val_plt[1,i] = val_loss[i][1]\n",
    "\n",
    "plt.figure()\n",
    "plot_idx = np.arange(np.size(train_loss))\n",
    "plt.plot(plot_idx[5:-1],train_loss[5:-1],lw=2,label='training loss')\n",
    "plt.plot(val_plt[0,1:],val_plt[1,1:],lw=2,label='validation loss')\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "# 儲存圖片到指定資料夾（需提供路徑）\n",
    "timestamp=datetime.now().strftime('%m%d%H%M')\n",
    "output_path = r'C:\\Users\\USER\\Desktop\\GNN_OPF_electricity_market-main\\output_loss_fig'\n",
    "output_path = output_path + '/loss_fig_%s.png'%(timestamp)\n",
    "plt.savefig(output_path, dpi=300, bbox_inches='tight')  # 儲存為 PNG 格式，解析度 300 dpi\n",
    "\n",
    "\n",
    "plt.show(block=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9uGUm4rsco3"
   },
   "source": [
    "# Evaluate the model w/ validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "4u6001UQ2pN3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset size: torch.Size([2000, 708])\n",
      "Number of validation set:  2000\n",
      "torch.Size([2000, 236])\n"
     ]
    }
   ],
   "source": [
    "n_test = np.size(x_test,0)\n",
    "x_test_feed = torch.from_numpy(x_test).float()\n",
    "x_test_feed = x_test_feed#.transpose(1,2)\n",
    "x_test_feed = x_test_feed.to(device)\n",
    "print('Validation dataset size:',x_test_feed.shape)\n",
    "print('Number of validation set: ',n_test)\n",
    "y_pred = net(x_test_feed)\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnNbSGYoXS3J"
   },
   "source": [
    "* Visualization\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKZQaqwJkn3P"
   },
   "source": [
    " - Visualize errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "7-JCo0KmXwm3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 236) (2000, 236)\n"
     ]
    }
   ],
   "source": [
    "y_pred1 = y_pred.cpu().detach()\n",
    "y_pred1 = torch.squeeze(y_pred1,1).numpy()#.transpose()\n",
    "print(y_test.shape,y_pred1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "NOeXUzrd9h9y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 2, 2000) (2000, 236)\n"
     ]
    }
   ],
   "source": [
    "# x=np.reshape(x,(x.shape[0]*x.shape[1],x.shape[2])) # reshape by samples not dim1\n",
    "# y=np.reshape(y,(y.shape[0]*y.shape[1],y.shape[2]))\n",
    "# print(x_pre.shape,y_pre.shape)\n",
    "\n",
    "y_pred_temp = y_pred1.copy().transpose()\n",
    "# y_pred2=np.reshape(y_pred2,(y_pre.shape[0],y_pre.shape[1],n_test))\n",
    "y_pred2=np.zeros([y.shape[0],y.shape[1],n_test])\n",
    "y_pred2[:,0,:]=y_pred_temp[:n_bus,:]\n",
    "y_pred2[:,1,:]=y_pred_temp[n_bus:,:]\n",
    "print(y_pred2.shape,y_pred1.shape)\n",
    "\n",
    "y_test_temp = y_test.copy().transpose()\n",
    "# y_pred2=np.reshape(y_pred2,(y_pre.shape[0],y_pre.shape[1],n_test))\n",
    "y_test2=np.zeros([y.shape[0],y.shape[1],n_test])\n",
    "y_test2[:,0,:]=y_test_temp[:n_bus,:]\n",
    "y_test2[:,1,:]=y_test_temp[n_bus:,:]\n",
    "y_pred1 = y_pred2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "2eVE-t3nl0Cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 2, 2000) (118, 2, 2000)\n"
     ]
    }
   ],
   "source": [
    "# recover the original p.u. scale\n",
    "# vy_deviation) * vy_scale\n",
    "y_pred1[:,1,:] = y_pred1[:,1,:] / vy_scale + vy_deviation\n",
    "y_test2[:,1,:] = y_test2[:,1,:] / vy_scale + vy_deviation\n",
    "print(y_test2.shape,y_pred1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "FUVl5LXeknC4"
   },
   "outputs": [],
   "source": [
    "n_test = np.size(y_test2,2)\n",
    "err_L2 = np.zeros(n_test)\n",
    "err_Linf = np.zeros(n_test)\n",
    "for i in range(n_test):\n",
    "  err_L2[i] = np.linalg.norm(y_test2[:,0,i] - y_pred1[:,0,i]) / np.linalg.norm(y_test2[:,0,i])\n",
    "  err_Linf[i] = np.max(np.abs(y_test2[:,0,i] - y_pred1[:,0,i])) / np.max(np.abs(y_test2[:,0,i]))\n",
    "\n",
    "err_L2_v = np.zeros(n_test)\n",
    "err_Linf_v = np.zeros(n_test)\n",
    "for i in range(n_test):\n",
    "  err_L2_v[i] = np.linalg.norm(y_test2[:,1,i] - y_pred1[:,1,i]) / np.linalg.norm(y_test2[:,1,i])\n",
    "  err_Linf_v[i] = np.max(np.abs(y_test2[:,1,i] - y_pred1[:,1,i])) / np.max(np.abs(y_test2[:,1,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "79xFCVXklkLb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price L2 mean: 0.07489102664955795 L_inf mean: 0.11421099848835482\n",
      "Voltage L2 mean: 0.021676406520368695 L_inf mean: 0.06417531971404575\n"
     ]
    }
   ],
   "source": [
    "err_L2_mean = np.mean(err_L2)\n",
    "err_Linf_mean = np.mean(err_Linf)\n",
    "print('Price L2 mean:', err_L2_mean,'L_inf mean:', err_Linf_mean )\n",
    "err_L2_mean_v = np.mean(err_L2_v)\n",
    "err_Linf_mean_v = np.mean(err_Linf_v)\n",
    "print('Voltage L2 mean:', err_L2_mean_v,'L_inf mean:', err_Linf_mean_v )\n",
    "\n",
    "# fig2 = plt.figure(figsize=(16, 16))\n",
    "# plt.subplot(2, 2, 1)\n",
    "# # plt.hist(np.abs(ga),bins = 10)\n",
    "# plt.plot(err_L2,'bo',markersize=0.5,label = 'L2 error')\n",
    "# plt.plot(err_Linf,'r^',markersize=0.5,label = 'Linf error')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('sample index')\n",
    "# plt.ylabel('error')\n",
    "# plt.title('Normalized sample error')\n",
    "# plt.grid(True)\n",
    "# # error histogram\n",
    "# plt.subplot(2, 2, 2)\n",
    "# plt.hist(err_L2, bins = 50, facecolor='b', alpha=0.75,label = 'L2 error')\n",
    "# plt.hist(err_Linf, bins = 50, facecolor='g', alpha=0.75,label = 'Linf error')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('sample error value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('Histogram of L_2 error')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)\n",
    "\n",
    "# plt.subplot(2, 2, 3)\n",
    "# # plt.hist(np.abs(ga),bins = 10)\n",
    "# plt.plot(err_L2_v,'bo',markersize=0.5,label = 'L2 error')\n",
    "# plt.plot(err_Linf_v,'r^',markersize=0.5,label = 'Linf error')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('sample index')\n",
    "# plt.ylabel('error')\n",
    "# plt.title('Normalized sample error')\n",
    "# plt.grid(True)\n",
    "# # error histogram\n",
    "# plt.subplot(2, 2, 4)\n",
    "# plt.hist(err_L2_v, bins = 50, facecolor='b', alpha=0.75,label = 'L2 error')\n",
    "# plt.hist(err_Linf_v, bins = 50, facecolor='g', alpha=0.75,label = 'Linf error')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('sample error value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('Histogram of L_2 error')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "6sUddh-uGg_p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 2000) (118, 2000)\n",
      "true range: 1.06 0.94\n",
      "predicted range 1.2018882942199707 0.8691615128517151\n"
     ]
    }
   ],
   "source": [
    "print(y_pred1[:,1,:n_test].shape,y_test2[:,1,:n_test].shape)\n",
    "print('true range:',np.max(y_test2[:,1,:n_test]),np.min(y_test2[:,1,:n_test]))\n",
    "print('predicted range',np.max(y_pred1[:,1,:n_test]),np.min(y_pred1[:,1,:n_test]))\n",
    "\n",
    "# fig3 = plt.figure(figsize=(16, 8))\n",
    "# flat_list1 = list(np.concatenate(y_test2[:,1,:n_test]).flat)\n",
    "# flat_list2 = list(np.concatenate(y_pred1[:,1,:n_test]).flat)\n",
    "# plt.hist(flat_list1,bins = 100,label = 'true')\n",
    "\n",
    "# plt.hist(flat_list2,bins = 100,label = 'pred')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "jhfImaPsjKYq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 6, 10000) 10000\n"
     ]
    }
   ],
   "source": [
    "print(x.shape,n_sample)\n",
    "\n",
    "x_new = np.zeros([x.shape[0],x.shape[1],n_sample])\n",
    "for i in range(x.shape[1]):\n",
    "  x_new[:,i,:] = x_total[n_bus*i:n_bus*(i+1),:]\n",
    "\n",
    "y_new = np.zeros([y.shape[0],y.shape[1],n_sample])\n",
    "for i in range(y.shape[1]):\n",
    "  y_new[:,i,:] = y_total[n_bus*i:n_bus*(i+1),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9x7neMNj7n3"
   },
   "source": [
    "# Predict generation using $\\pi$\n",
    "* Using predicted $\\pi$ and find the active constraints in $p_G(i)$\n",
    "* For inactive $p_G(i)$ consider other methods like power flow balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Kr29K04j2KTN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 10000)\n",
      "<class 'numpy.ndarray'> 118 [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117]\n"
     ]
    }
   ],
   "source": [
    "gen_limit0 = x_new[:,4,:].copy() # lin cost\n",
    "print(gen_limit0.shape)\n",
    "\n",
    "gen_idx = []\n",
    "gen_idx = np.arange(n_bus)\n",
    "# for i in range(n_bus):\n",
    "#   if gen_limit0[i,0] > 0:\n",
    "#     gen_idx.append(i)\n",
    "print(type(gen_idx),len(gen_idx),gen_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "gHmx9lMDXzpM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 2, 10000) (236, 10000)\n"
     ]
    }
   ],
   "source": [
    "n_sample=x_total.shape[-1]\n",
    "x_feed = torch.from_numpy(x_total.T).float()\n",
    "y_pred1=net(x_feed.to(device)).cpu().detach().numpy().T\n",
    "y_pred_temp = y_pred1.copy()\n",
    "y_pred2=np.zeros([y.shape[0],y.shape[1],n_sample])\n",
    "y_pred2[:,0,:]=y_pred_temp[:n_bus,:]\n",
    "y_pred2[:,1,:]=y_pred_temp[n_bus:,:]\n",
    "print(y_pred2.shape,y_pred1.shape)\n",
    "y_pred1 = y_pred2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "GPEHF2L91bGv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0009472808837891478\n",
      "0.784000000000006\n",
      "0.4088040747576095\n",
      "0.24221453285700786\n"
     ]
    }
   ],
   "source": [
    "gen_cost0 = x_new[:,4,:].copy()\n",
    "lmp_data = y_new[:,0,:].copy()\n",
    "quadratic_a = x_new[:,5,:].copy()\n",
    "profit_pred = y_pred1[:,0,:] - gen_cost0\n",
    "print(np.min(np.abs(profit_pred)))\n",
    "profit_true = lmp_data - gen_cost0\n",
    "print(np.min(np.abs(profit_true)))\n",
    "profit_pred=(y_pred1[:,0,:]-gen_cost0)/(quadratic_a+1e-10)/2\n",
    "profit_true=(lmp_data-gen_cost0)/(quadratic_a+1e-10)/2\n",
    "print(np.min(np.abs(profit_pred)))\n",
    "print(np.min(np.abs(profit_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "uiHWtU5OLc1x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 10000) (118, 10000)\n",
      "0.07409513539806518\n"
     ]
    }
   ],
   "source": [
    "print(profit_pred.shape,profit_true.shape)\n",
    "profit_err = profit_true - profit_pred\n",
    "profit_err_l2 = np.zeros([n_sample,1])\n",
    "\n",
    "for i in range(n_sample):\n",
    "  profit_err_l2[i] = np.linalg.norm(profit_err[:,i])/np.linalg.norm(profit_true[:,i])\n",
    "print(np.mean(profit_err_l2))\n",
    "\n",
    "# fig5 = plt.figure(figsize=(16, 8))\n",
    "# # error histogram\n",
    "# plt.hist(profit_err_l2, bins = 50, facecolor='b', alpha=0.75,label = 'L2 error')\n",
    "# # plt.hist(err_linf_new, bins = 50, facecolor='g', alpha=0.75,label = 'Linf error')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('sample error value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('Histogram of L_2 error')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ZbHchRQd_g8-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1180000,)\n",
      "-3460.930747463118 -2.9391182643149665\n"
     ]
    }
   ],
   "source": [
    "p_pred_sort = np.reshape(profit_pred,n_bus*n_sample)\n",
    "p_true_sort = np.reshape(profit_true,n_bus*n_sample)\n",
    "print(p_pred_sort.shape)\n",
    "print(np.min(p_pred_sort),np.min(p_true_sort))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "TYsSdNGp-OLP"
   },
   "outputs": [],
   "source": [
    "# fig2 = plt.figure(figsize=(8, 8))\n",
    "# plt.hist(p_pred_sort, bins = 50, facecolor='b', alpha=0.75,label = 'pred. profit')\n",
    "# plt.hist(p_true_sort, bins = 50, facecolor='g', alpha=0.75,label = 'true profit')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('profit histogram')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "1J9j5tT9p_f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2178465.59514828 2764601.835122853\n",
      "2178465.59514828 43260060.56350001 2178465.59514828\n"
     ]
    }
   ],
   "source": [
    "# x = [load, gen_cost, gen_lim]\n",
    "binary_thres_true = 1e-5\n",
    "binary_thres = x_new[:,0,:].copy() # upper\n",
    "binary_thres_lo = x_new[:,1,:].copy() # lower\n",
    "gen_pred_binary_full = np.zeros((n_bus,n_sample))\n",
    "gen_true_binary_full = np.zeros((n_bus,n_sample))\n",
    "\n",
    "for i in range(n_sample):\n",
    "  for j in range(len(gen_idx)):\n",
    "    # predicted generator limit\n",
    "    if profit_pred[gen_idx[j],i] > binary_thres[gen_idx[j],i]:\n",
    "      gen_pred_binary_full[gen_idx[j],i] = binary_thres[gen_idx[j],i]\n",
    "    elif profit_pred[gen_idx[j],i] < binary_thres_lo[gen_idx[j],i]:\n",
    "      gen_pred_binary_full[gen_idx[j],i] = binary_thres_lo[gen_idx[j],i]\n",
    "    else:\n",
    "      gen_pred_binary_full[gen_idx[j],i] = profit_pred[gen_idx[j],i]\n",
    "    # true generator limit\n",
    "    if profit_true[gen_idx[j],i] > binary_thres[gen_idx[j],i]:\n",
    "      gen_true_binary_full[gen_idx[j],i] = binary_thres[gen_idx[j],i]\n",
    "    elif profit_true[gen_idx[j],i] < binary_thres_lo[gen_idx[j],i]:\n",
    "      gen_true_binary_full[gen_idx[j],i] = binary_thres_lo[gen_idx[j],i]\n",
    "    else:\n",
    "      gen_true_binary_full[gen_idx[j],i] = profit_true[gen_idx[j],i]\n",
    "\n",
    "gen_inj=gen_pred_binary_full\n",
    "gen_inj_true=gen_true_binary_full\n",
    "# nodal injection\n",
    "load0 = -x_new[:,1,:].copy() # load file\n",
    "p_inj = gen_inj #- load0\n",
    "p_inj_true = gen_inj_true #- load0\n",
    "print(np.sum(p_inj),np.sum(gen_inj_true))\n",
    "print(np.sum(p_inj),np.sum(load0),np.sum(gen_inj))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WAgqdRPjAONm"
   },
   "source": [
    "## injection accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "J46pLAw2AQor"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 10000) (118, 10000)\n",
      "mean p_inj l2 err: 0.16012327762206408\n"
     ]
    }
   ],
   "source": [
    "print(p_inj_true.shape,p_inj.shape)\n",
    "p_inj_true_sort = np.reshape(p_inj_true,n_bus*n_sample)\n",
    "p_inj_sort = np.reshape(p_inj,n_bus*n_sample)\n",
    "p_err = np.zeros(n_sample)\n",
    "for i in range(n_sample):\n",
    "  p_err[i] = np.linalg.norm(p_inj_true[:,i]-p_inj[:,i]) / np.linalg.norm(p_inj_true[:,i])\n",
    "print('mean p_inj l2 err:',np.mean(p_err))\n",
    "# fig3 = plt.figure(figsize=(16, 8))\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.hist(p_inj_sort, bins = 50, facecolor='b', alpha=0.75,label = 'pred. injection')\n",
    "# plt.hist(p_inj_true_sort, bins = 50, facecolor='g', alpha=0.75,label = 'true injection')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('injection histogram')\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.hist(p_err, bins = 50, facecolor='b', alpha=0.75,label = 'injection err')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('error histogram')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)s_binary\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DaN7u4xeGIom"
   },
   "source": [
    "* Calculate flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "YT4sgn_n79MI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(186, 1) (186, 10000) (186, 10000)\n",
      "18222 13334\n",
      "0.009796774193548387 0.007168817204301075\n",
      "186 10000 (186, 10000)\n"
     ]
    }
   ],
   "source": [
    "filename=root+'118ac_fmax.txt'\n",
    "f_max1=pd.read_table(filename,sep=',',header=None).to_numpy() # flow limit\n",
    "\n",
    "n_line = np.size(S_isf,0)\n",
    "flow_est = np.zeros((n_line,n_sample))\n",
    "flow_est0 = np.zeros((n_line,n_sample))\n",
    "\n",
    "f_binary = np.zeros((n_line,n_sample))\n",
    "f_binary0 = np.zeros((n_line,n_sample))\n",
    "\n",
    "# for i in range(n_sample):\n",
    "flow_est = np.dot(S_isf,p_inj)\n",
    "flow_est0 = np.dot(S_isf,p_inj_true)\n",
    "# f_max\n",
    "# f_max_numpy = f_max.cpu().detach().numpy()\n",
    "f_max_numpy = f_max1.copy()\n",
    "f_binary = (np.abs(flow_est)-f_max_numpy > 0)\n",
    "f_binary0 = (np.abs(flow_est0)-f_max_numpy > 0)\n",
    "\n",
    "print(f_max_numpy.shape,flow_est.shape,flow_est0.shape)\n",
    "f_tot_sample = n_line * n_sample\n",
    "print(np.sum(f_binary),np.sum(f_binary0))\n",
    "print(np.sum(f_binary)/f_tot_sample,np.sum(f_binary0)/f_tot_sample)\n",
    "print(n_line,n_sample,flow_est.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "VyvDpKhyQj0M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179.55964777271595 39.64401431411352\n",
      "1.197064318484773 0.2642934287607568\n"
     ]
    }
   ],
   "source": [
    "# soft threshold\n",
    "f_err_est = np.abs(flow_est)-f_max_numpy\n",
    "f_err_true = np.abs(flow_est0)-f_max_numpy\n",
    "\n",
    "f_err_est = np.maximum(np.abs(flow_est)-f_max_numpy,0) # identify violations\n",
    "f_err_true = np.maximum(np.abs(flow_est0)-f_max_numpy,0)\n",
    "\n",
    "print(np.max(f_err_est),np.max(f_err_true))\n",
    "print(np.max(f_err_est/f_max_numpy),np.max(f_err_true/f_max_numpy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "7iuX7tN2a2Cp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13996 10451\n",
      "0.0075247311827956985 0.005618817204301075\n"
     ]
    }
   ],
   "source": [
    "f_binary_soft = (np.abs(flow_est)-f_max_numpy > 0.1*(f_max_numpy))\n",
    "f_binary0_soft = (np.abs(flow_est0)-f_max_numpy > 0.1*(f_max_numpy))\n",
    "print(np.sum(f_binary_soft),np.sum(f_binary0_soft))\n",
    "print(np.sum(f_binary_soft)/f_tot_sample,np.sum(f_binary0_soft)/f_tot_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "SYl9pxnOUUQF"
   },
   "outputs": [],
   "source": [
    "f_pred_sort = np.reshape(f_err_est/f_max_numpy,n_line*n_sample)\n",
    "f_true_sort = np.reshape(f_err_true/f_max_numpy,n_line*n_sample)\n",
    "\n",
    "# fig2 = plt.figure(figsize=(16, 8))\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.hist(f_pred_sort, bins = 10, facecolor='b', alpha=0.75,label = 'pred. f')\n",
    "# plt.hist(f_true_sort, bins = 10, facecolor='g', alpha=0.75,label = 'true f')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('percentage')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('flow violation level histogram')\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.hist(f_true_sort, bins = 10, facecolor='g', alpha=0.75,label = 'true f')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('percentage')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('flow violation level histogram')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "YglgTpWVRLri"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sample pred: 7\n",
      "max line pred: 7469\n",
      "max sample true: 3\n",
      "max line true: 10000\n"
     ]
    }
   ],
   "source": [
    "f_line = np.sum(f_binary,0)\n",
    "f_samp = np.sum(f_binary,1)\n",
    "print('max sample pred:',np.max(f_line))\n",
    "print('max line pred:',np.max(f_samp))\n",
    "\n",
    "f_line0  = np.sum(f_binary0,0)\n",
    "f_samp0 = np.sum(f_binary0,1)\n",
    "print('max sample true:',np.max(f_line0))\n",
    "print('max line true:',np.max(f_samp0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZQnarHmPl35"
   },
   "source": [
    "# Check objective optimality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "BZjhp-CgQaDz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11910659599800369\n"
     ]
    }
   ],
   "source": [
    "gen_cost_pred = np.zeros((n_bus,n_sample))\n",
    "gen_cost_true = np.zeros((n_bus,n_sample))\n",
    "objective_err = np.zeros(n_sample)\n",
    "\n",
    "gen_cost_pred = np.multiply(np.multiply(p_inj,p_inj),quadratic_a) + np.multiply(p_inj,gen_cost0)\n",
    "gen_cost_true = np.multiply(np.multiply(p_inj_true,p_inj_true),quadratic_a) + np.multiply(p_inj_true,gen_cost0)\n",
    "\n",
    "objective_err = np.sum(np.abs(gen_cost_true-gen_cost_pred),axis=0) / np.sum(gen_cost_true,axis=0)\n",
    "print(np.mean(objective_err))\n",
    "\n",
    "# fig6 = plt.figure(figsize=(16, 8))\n",
    "# # error histogram\n",
    "# plt.hist(objective_err, bins = 50, facecolor='b', alpha=0.75,label = 'L2 error')\n",
    "# # plt.hist(err_linf_new, bins = 50, facecolor='g', alpha=0.75,label = 'Linf error')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('sample error value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('Histogram of L_2 error')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdrsAWpYo0-w"
   },
   "source": [
    "## injection accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "mArjSN-So0-x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 10000) (118, 10000)\n",
      "mean p_inj l2 err: 0.16012327762206408\n"
     ]
    }
   ],
   "source": [
    "print(p_inj_true.shape,p_inj.shape)\n",
    "\n",
    "p_inj_true_sort = np.reshape(p_inj_true,n_bus*n_sample)\n",
    "p_inj_sort = np.reshape(p_inj,n_bus*n_sample)\n",
    "\n",
    "p_err = np.zeros(n_sample)\n",
    "for i in range(n_sample):\n",
    "  p_err[i] = np.linalg.norm(p_inj_true[:,i]-p_inj[:,i]) / np.linalg.norm(p_inj_true[:,i])\n",
    "\n",
    "print('mean p_inj l2 err:',np.mean(p_err))\n",
    "# fig3 = plt.figure(figsize=(16, 8))\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.hist(p_inj_sort, bins = 50, facecolor='b', alpha=0.75,label = 'pred. injection')\n",
    "# plt.hist(p_inj_true_sort, bins = 50, facecolor='g', alpha=0.75,label = 'true injection')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('injection histogram')s_binary\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.hist(p_err, bins = 50, facecolor='b', alpha=0.75,label = 'injection err')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('error histogram')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujU84tOSpqsy"
   },
   "source": [
    "# Test AC feasibility\n",
    "* P in actual value, V in p.u.\n",
    "* Use P to recover $\\theta$, or solve $\\theta$ and Q for PF\n",
    "$$ Q_m = V_m \\sum_{n=1}^N V_n \\left(G_{mn}\\sin\\theta_{mn} - B_{mn}\\cos\\theta_{mn} \\right) $$\n",
    "calculate $Q_{mn}$ directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Of6mEXF4puDN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 118) (118, 118)\n",
      "(117, 117) (118, 10000) (118, 10000)\n",
      "(118, 10000) (118, 10000)\n"
     ]
    }
   ],
   "source": [
    "# Bbus and B_r inverse\n",
    "filename1 = root+'ieee118_Bbus.txt'\n",
    "Bbus=pd.read_table(filename1,sep=',',header=None).to_numpy()\n",
    "B_r = np.delete(Bbus,68,axis=0)\n",
    "B_r = np.delete(B_r,68,axis=1)\n",
    "Br_inv = np.linalg.inv(B_r)\n",
    "\n",
    "# Y = G + jB\n",
    "filename1 = root+'ieee118_Gmat.txt'\n",
    "G_mat=pd.read_table(filename1,sep=',',header=None).to_numpy()\n",
    "filename1 = root+'ieee118_Bmat.txt'\n",
    "B_mat=pd.read_table(filename1,sep=',',header=None).to_numpy()\n",
    "print(G_mat.shape,B_mat.shape)\n",
    "\n",
    "# line parameters\n",
    "filename1 = root+'ieee118_lineloc.txt'\n",
    "line_loc = pd.read_table(filename1,sep=',',header=None).to_numpy()\n",
    "\n",
    "# load line params\n",
    "filename1 = root+'ieee118_lineparams.txt'\n",
    "line_params = pd.read_table(filename1,sep=',',header=None).to_numpy()\n",
    "R_line = line_params[:,0].copy()\n",
    "X_line = line_params[:,1].copy()\n",
    "B_shunt = line_params[:,2].copy()\n",
    "Z_line = R_line + 1j * X_line \n",
    "Y_line = 1 / Z_line\n",
    "G_line = np.real(Y_line)\n",
    "B_line = np.imag(Y_line)\n",
    "# P_inj w/out reference bus in p.u.\n",
    "p_inj_r = np.delete(p_inj,68,axis=0) / 100\n",
    "p_inj_true_r = np.delete(p_inj_true,68,axis=0) / 100\n",
    "p_inj_pu = p_inj / 100\n",
    "p_inj_true_pu = p_inj_true / 100\n",
    "print(Br_inv.shape,p_inj.shape,p_inj_true.shape)#p_inj_true\n",
    "\n",
    "theta0 = np.matmul(Br_inv,p_inj_r)\n",
    "theta_true0 = np.matmul(Br_inv,p_inj_true_r)\n",
    "theta = np.insert(theta0,68,0,axis = 0)\n",
    "theta_true = np.insert(theta_true0,68,0,axis = 0)\n",
    "print(theta.shape,theta_true.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4676360874930391 -0.980117316788114\n",
      "2.7803011534120627 -9.166735486002148\n"
     ]
    }
   ],
   "source": [
    "print(np.max(theta),np.min(theta))\n",
    "math.sin(math.pi/6)\n",
    "print(G_line[0],B_line[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(186, 10000)\n",
      "1.1975999641418458 0.8382061719894409 (118, 10000)\n"
     ]
    }
   ],
   "source": [
    "# Calculate real and reactive flow\n",
    "f_p = np.zeros((n_line,n_sample))\n",
    "f_q = np.zeros((n_line,n_sample))\n",
    "fji_p = np.zeros((n_line,n_sample))\n",
    "fji_q = np.zeros((n_line,n_sample))\n",
    "print(f_q.shape)\n",
    "\n",
    "v_pred = y_pred1[:,1,:].copy()\n",
    "v_pred = v_pred / vy_scale + vy_deviation\n",
    "print(np.max(v_pred),np.min(v_pred),v_pred.shape)\n",
    "\n",
    "theta1 = theta[line_loc[:,0]-1,:]\n",
    "theta2 = theta[line_loc[:,1]-1,:]\n",
    "V1 = v_pred[line_loc[:,0]-1,:]\n",
    "V2 = v_pred[line_loc[:,1]-1,:] \n",
    "f_p=(a*G_line*(V1*V1).T)-a*((V1*V2).T)*(G_line*np.cos(theta1-theta2).T+B_line*np.sin(theta1-theta2).T)\n",
    "f_p=f_p.T\n",
    "f_q=-a*(V1.T)*(a*V1.T)*(B_line+B_shunt/2)+a*((V1*V2).T)*(B_line*np.cos(theta1-theta2).T-G_line*np.sin(theta1-theta2).T)\n",
    "f_q=f_q.T\n",
    "\n",
    "theta1 = theta[line_loc[:,1]-1,:]\n",
    "theta2 = theta[line_loc[:,0]-1,:]\n",
    "V1 = v_pred[line_loc[:,1]-1,:]\n",
    "V2 = v_pred[line_loc[:,0]-1,:]\n",
    "fji_p=(a*G_line*(V1*V1).T)-a*((V1*V2).T)*(G_line*np.cos(theta1-theta2).T+B_line*np.sin(theta1-theta2).T)\n",
    "fji_p=fji_p.T\n",
    "fji_q=-a*(V1.T)*(a*V1.T)*(B_line+B_shunt/2)+a*((V1*V2).T)*(B_line*np.cos(theta1-theta2).T-G_line*np.sin(theta1-theta2).T)\n",
    "fji_q=fji_q.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "gKfcrbTSVMeO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.037925011630364 -31.083386834344736\n",
      "31.58420893028152 151.0\n"
     ]
    }
   ],
   "source": [
    "s_pred = np.sqrt(f_p*f_p+f_q*f_q)*100\n",
    "sji_pred = np.sqrt(fji_p*fji_p+fji_q*fji_q)*100\n",
    "print(np.max(f_q),np.min(f_q))\n",
    "flow_est.shape\n",
    "print(np.mean(s_pred[0,:]),np.mean(f_max_numpy[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "YO4__LJ2brLu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103317\n",
      "hard violation rate: 0.055546774193548386\n",
      "80389\n",
      "0.04321989247311828\n"
     ]
    }
   ],
   "source": [
    "sij_binary = (np.abs(s_pred)-f_max_numpy[:n_line] > 0)\n",
    "sji_binary = (np.abs(sji_pred)-f_max_numpy[:n_line] > 0)\n",
    "s_binary = np.maximum(sij_binary,sji_binary)\n",
    "print(np.sum(s_binary))#,np.sum(f_binary0))\n",
    "print('hard violation rate:',np.sum(s_binary)/n_sample/n_line)#,np.sum(f_binary0)/f_tot_sample)\n",
    "s_binary_soft = (np.abs(s_pred)-f_max_numpy[:n_line] > 0.1*(f_max_numpy[:n_line]))\n",
    "print(np.sum(s_binary_soft))#,np.sum(f_binary0_soft))\n",
    "print(np.sum(s_binary_soft)/n_sample/n_line)#,np.sum(f_binary0_soft)/f_tot_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "Ty0WpBdfJwMR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S violation level:\n",
      "hard: 0.055546774193548386\n",
      "mean: 0.03611345215041192\n",
      "median: 0.0\n",
      "max: 9.96923260043038\n",
      "std: 0.23865946668498503\n",
      "p99: 1.1094119909261948\n",
      "f violation level:\n",
      "hard: 0.009796774193548387 0.007168817204301075\n",
      "mean: 0.0032111106981756376\n",
      "median: 0.0\n",
      "max: 1.197064318484773\n",
      "std: 0.040870652402398885\n",
      "p99: 0.0\n"
     ]
    }
   ],
   "source": [
    "# violation level\n",
    "sij_violation = np.abs(s_pred)-f_max_numpy[:n_line] #/ f_max_numpy\n",
    "sij_violation_level = np.maximum(sij_violation,0)\n",
    "sji_violation = np.abs(sji_pred)-f_max_numpy[:n_line] #/ f_max_numpy\n",
    "sji_violation_level = np.maximum(sji_violation,0)\n",
    "s_violation_level = np.maximum(sij_violation_level,sji_violation_level)\n",
    "s_violation_level = np.divide(s_violation_level,f_max_numpy[:n_line])\n",
    "s_vio_lvl = np.reshape(s_violation_level,n_line*n_sample)\n",
    "\n",
    "print('S violation level:')\n",
    "print('hard:',np.sum(s_binary)/f_tot_sample)\n",
    "print('mean:',np.mean(s_vio_lvl))\n",
    "print('median:',np.median(s_vio_lvl))\n",
    "print('max:',np.max(s_vio_lvl))\n",
    "print('std:',np.std(s_vio_lvl))\n",
    "print('p99:',np.percentile(s_vio_lvl,99))\n",
    "\n",
    "f_violation = np.abs(flow_est)-f_max_numpy #/ f_max_numpy\n",
    "f_violation_level = np.maximum(f_violation,0)\n",
    "f_violation_level = np.divide(f_violation_level,f_max_numpy)\n",
    "f_vio_lvl = np.reshape(f_violation_level,n_line*n_sample)\n",
    "\n",
    "print('f violation level:')\n",
    "print('hard:',np.sum(f_binary)/f_tot_sample,np.sum(f_binary0)/f_tot_sample)\n",
    "print('mean:',np.mean(f_vio_lvl))\n",
    "print('median:',np.median(f_vio_lvl))\n",
    "print('max:',np.max(f_vio_lvl))\n",
    "print('std:',np.std(f_vio_lvl))\n",
    "print('p99:',np.percentile(f_vio_lvl,99))\n",
    "\n",
    "# fig4 = plt.figure(figsize=(6,4))\n",
    "# plt.hist(s_vio_lvl, bins = 50, facecolor='b', alpha=0.75,label = 's violation')\n",
    "# plt.hist(f_vio_lvl, bins = 50, facecolor='r', alpha=0.75,label = 'f violation')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('violation level')\n",
    "# plt.ylabel('frequency')\n",
    "# # plt.title('injection histogram')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "kWXEpj-ryjbJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price L2 mean: 0.07489102664955795 L_inf mean: 0.11421099848835482\n",
      "std: 0.032537259303998596\n",
      "Voltage L2 mean: 0.021676406520368695 L_inf mean: 0.06417531971404575\n",
      "std: 0.0020968756654083064\n"
     ]
    }
   ],
   "source": [
    "# err_L2_mean = np.mean(err_L2)\n",
    "# err_Linf_mean = np.mean(err_Linf)\n",
    "print('Price L2 mean:', err_L2_mean,'L_inf mean:', err_Linf_mean )\n",
    "print('std:',np.std(err_L2))\n",
    "# err_L2_mean_v = np.mean(err_L2_v)\n",
    "# err_Linf_mean_v = np.mean(err_Linf_v)\n",
    "print('Voltage L2 mean:', err_L2_mean_v,'L_inf mean:', err_Linf_mean_v )\n",
    "print('std:',np.std(err_L2_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "輸出內容已儲存到 C:\\Users\\USER\\Desktop\\GNN_OPF_electricity_market-main\\output_loss_fig/output_02191305.txt\n"
     ]
    }
   ],
   "source": [
    "import os   \n",
    "# 建立輸出內容字串\n",
    "output_content = f\"\"\"\n",
    "'number of params: {sum(p.numel() for p in net.parameters())}\n",
    "Price L2 mean: {err_L2_mean} \n",
    "Price std: {np.std(err_L2)}\n",
    "\n",
    "Voltage L2 mean: {err_L2_mean_v} \n",
    "Voltage std: {np.std(err_L2_v)}\n",
    "\"\"\"\n",
    "\n",
    "# 設定儲存路徑和檔名\n",
    "output_dir = r'C:\\Users\\USER\\Desktop\\GNN_OPF_electricity_market-main\\output_loss_fig'\n",
    "timestamp = datetime.now().strftime('%m%d%H%M')\n",
    "output_file = f'{output_dir}/output_{timestamp}.txt'\n",
    "\n",
    "# 確保目錄存在\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 將內容寫入 txt 檔案\n",
    "with open(output_file, 'w') as file:\n",
    "    file.write(output_content)\n",
    "\n",
    "print(f\"輸出內容已儲存到 {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "118ac_feasdnn0417.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "AC_OPF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
