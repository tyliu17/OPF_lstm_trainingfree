{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "JKLovDeXoCCP"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import math\n",
    "from datetime import datetime\n",
    "root=''\n",
    "# try:\n",
    "#   from google.colab import drive\n",
    "#   drive.mount('/content/drive')\n",
    "#   root='./drive/MyDrive/gnn/data/'\n",
    "# except:\n",
    "#   pass\n",
    "# device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# print(torch.cuda.get_device_name(0))\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# 檢查是否有可用的 GPU，否則使用 CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([186, 118]) torch.Size([186, 1]) tensor(72)\n",
      "1654.8 -332.4 949.67 0.94\n",
      "voltage range(scaled): 3.9999999999999925 16.000000000000004\n",
      "price range old: 11.547 949.67\n",
      "voltage range old: 3.9999999999999925 16.000000000000004\n",
      "price range new: 11.547 949.67\n",
      "voltage range new: 3.9999999999999925 16.000000000000004\n",
      "(118, 6, 10000) (118, 2, 10000)\n"
     ]
    }
   ],
   "source": [
    "root = 'C:/Users/USER/Desktop/GNN_OPF_electricity_market-main/GNN_OPF_electricity_market-main/GNN_OPF_electricity_market-main/data_gen/118_data/'\n",
    "# C:\\Users\\USER\\Desktop\\GNN_OPF_electricity_market-main\\GNN_OPF_electricity_market-main\\GNN_OPF_electricity_market-main\\data_gen\\118_data\\118dc_quad_ISF.txt\n",
    "filename = root + '118dc_quad_ISF.txt'\n",
    "\n",
    "S_isf=pd.read_table(filename,sep=',',header=None).to_numpy() # ISF matrix\n",
    "filename=root+'118ac_fmax.txt'\n",
    "f_max=pd.read_table(filename,sep=',',header=None).to_numpy() # flow limit\n",
    "n_line = np.size(S_isf,0)\n",
    "S = torch.from_numpy(S_isf).to(device) # ISF\n",
    "f_max = torch.from_numpy(f_max).to(device) # flow limit\n",
    "print(S.shape,f_max.shape,torch.min(f_max))\n",
    "\n",
    "x=np.load(root+'ac118_p10_x_v.npy')\n",
    "y=np.load(root+'ac118_p10_y_v.npy')\n",
    "W=np.load(root+'ac118_p10_w.npy')\n",
    "print(np.max(x),np.min(x),np.max(y),np.min(y))\n",
    "\n",
    "# scaling on voltage\n",
    "vy_deviation = 0.9\n",
    "vy_scale = 100\n",
    "y[:,1,:] = (y[:,1,:] - vy_deviation) * vy_scale\n",
    "print('voltage range(scaled):',np.min(y[:,1,:]),np.max(y[:,1,:]))\n",
    "\n",
    "# scaling on price\n",
    "pi_deviation = 0\n",
    "y[:,0,:] = y[:,0,:] + pi_deviation\n",
    "# filter out extreme points in price\n",
    "y_sort_arg = np.argsort(np.amax(np.abs(y[:,0,:]),axis=0)) # max extreme\n",
    "y_sort_arg1 = np.argsort(np.amin(y[:,0,:],axis=0),axis=0) # min extreme\n",
    "\n",
    "del_idx0 = []\n",
    "del_num = 0\n",
    "for i in range(del_num):\n",
    "  del_idx0.append(y_sort_arg[-i])\n",
    "  del_idx0.append(y_sort_arg1[i])\n",
    "# print(del_idx0)\n",
    "del_idx = [] # keep only non-repeated\n",
    "[del_idx.append(x) for x in del_idx0 if x not in del_idx]\n",
    "del_idx = np.sort(del_idx)\n",
    "# delete extreme points\n",
    "print('price range old:',np.min(y[:,0,:]),np.max(y[:,0,:]))\n",
    "print('voltage range old:',np.min(y[:,1,:]),np.max(y[:,1,:]))\n",
    "# x = np.delete(x, del_idx, axis=2)\n",
    "# y = np.delete(y, del_idx, axis=2)\n",
    "print('price range new:',np.min(y[:,0,:]),np.max(y[:,0,:]))\n",
    "print('voltage range new:',np.min(y[:,1,:]),np.max(y[:,1,:]))\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "id": "Hj9_eLfoWQY3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([186, 118]) torch.Size([186, 1]) tensor(72)\n",
      "1654.8 -332.4 949.67 0.94\n",
      "voltage range(scaled): 3.9999999999999925 16.000000000000004\n",
      "price range old: 11.547 949.67\n",
      "voltage range old: 3.9999999999999925 16.000000000000004\n",
      "price range new: 11.547 949.67\n",
      "voltage range new: 3.9999999999999925 16.000000000000004\n",
      "(118, 6, 10000) (118, 2, 10000)\n"
     ]
    }
   ],
   "source": [
    "root = 'C:/Users/USER/Desktop/GNN_OPF_electricity_market-main/GNN_OPF_electricity_market-main/GNN_OPF_electricity_market-main/data_gen/118_data/'\n",
    "# C:\\Users\\USER\\Desktop\\GNN_OPF_electricity_market-main\\GNN_OPF_electricity_market-main\\GNN_OPF_electricity_market-main\\data_gen\\118_data\\118dc_quad_ISF.txt\n",
    "filename = root + '118dc_quad_ISF.txt'\n",
    "\n",
    "S_isf=pd.read_table(filename,sep=',',header=None).to_numpy() # ISF matrix\n",
    "filename=root+'118ac_fmax.txt'\n",
    "f_max=pd.read_table(filename,sep=',',header=None).to_numpy() # flow limit\n",
    "n_line = np.size(S_isf,0)\n",
    "S = torch.from_numpy(S_isf).to(device) # ISF\n",
    "f_max = torch.from_numpy(f_max).to(device) # flow limit\n",
    "print(S.shape,f_max.shape,torch.min(f_max))\n",
    "\n",
    "x=np.load(root+'ac118_p10_x_v.npy')\n",
    "y=np.load(root+'ac118_p10_y_v.npy')\n",
    "W=np.load(root+'ac118_p10_w.npy')\n",
    "print(np.max(x),np.min(x),np.max(y),np.min(y))\n",
    "\n",
    "# scaling on voltage\n",
    "vy_deviation = 0.9\n",
    "vy_scale = 100\n",
    "y[:,1,:] = (y[:,1,:] - vy_deviation) * vy_scale\n",
    "print('voltage range(scaled):',np.min(y[:,1,:]),np.max(y[:,1,:]))\n",
    "\n",
    "# scaling on price\n",
    "pi_deviation = 0\n",
    "y[:,0,:] = y[:,0,:] + pi_deviation\n",
    "# filter out extreme points in price\n",
    "y_sort_arg = np.argsort(np.amax(np.abs(y[:,0,:]),axis=0)) # max extreme\n",
    "y_sort_arg1 = np.argsort(np.amin(y[:,0,:],axis=0),axis=0) # min extreme\n",
    "\n",
    "del_idx0 = []\n",
    "del_num = 0\n",
    "for i in range(del_num):\n",
    "  del_idx0.append(y_sort_arg[-i])\n",
    "  del_idx0.append(y_sort_arg1[i])\n",
    "# print(del_idx0)\n",
    "del_idx = [] # keep only non-repeated\n",
    "[del_idx.append(x) for x in del_idx0 if x not in del_idx]\n",
    "del_idx = np.sort(del_idx)\n",
    "# delete extreme points\n",
    "print('price range old:',np.min(y[:,0,:]),np.max(y[:,0,:]))\n",
    "print('voltage range old:',np.min(y[:,1,:]),np.max(y[:,1,:]))\n",
    "# x = np.delete(x, del_idx, axis=2)\n",
    "# y = np.delete(y, del_idx, axis=2)\n",
    "print('price range new:',np.min(y[:,0,:]),np.max(y[:,0,:]))\n",
    "print('voltage range new:',np.min(y[:,1,:]),np.max(y[:,1,:]))\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "id": "YcR42RBbdZqZ"
   },
   "outputs": [],
   "source": [
    "n_sample=y.shape[-1]\n",
    "n_bus=y.shape[0]\n",
    "x_total=x.transpose((1,0,2)).reshape(-1,x.shape[-1])\n",
    "y_total=y.transpose((1,0,2)).reshape(-1,y.shape[-1])\n",
    "x_train,x_test,y_train,y_test=train_test_split(x_total.T,y_total.T,test_size=0.2)\n",
    "\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        self.x=torch.from_numpy(x).float()\n",
    "        self.y=torch.from_numpy(y).float()\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "          idx=idx.tolist()\n",
    "        return self.x[idx],self.y[idx]\n",
    "    \n",
    "batch_size=512\n",
    "#512\n",
    "params={'batch_size': batch_size,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 0}\n",
    "train=Dataset(x_train,y_train)\n",
    "train_set=torch.utils.data.DataLoader(train,**params)\n",
    "val=Dataset(x_test,y_test)\n",
    "val_set=torch.utils.data.DataLoader(val,**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 708])\n",
      "Train Mean: 6.45963191986084\n",
      "Train Std: 3.6399807929992676\n",
      "Validation Mean: 6.4284539222717285\n",
      "Validation Std: 3.682328939437866\n"
     ]
    }
   ],
   "source": [
    "# 資料分析\n",
    "# 從資料集中建立 DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 取第一個 batch 進行分析\n",
    "train_batch_x, _ = next(iter(train_loader))\n",
    "val_batch_x, _ = next(iter(val_loader))\n",
    "\n",
    "# 計算均值和標準差\n",
    "print(train_batch_x.shape)\n",
    "train_mean, train_std = train_batch_x.mean(dim=0), train_batch_x.std(dim=0)\n",
    "val_mean, val_std = val_batch_x.mean(dim=0), val_batch_x.std(dim=0)\n",
    "\n",
    "mean_value_train = train_mean.mean().item()\n",
    "print('Train Mean:', mean_value_train)\n",
    "std_value_train = train_std.mean().item()\n",
    "print('Train Std:', std_value_train)\n",
    "mean_value_val = val_mean.mean().item()\n",
    "print('Validation Mean:', mean_value_val)\n",
    "std_value_val = val_std.mean().item()\n",
    "print('Validation Std:', std_value_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "id": "JN9gN9BnCNim"
   },
   "outputs": [],
   "source": [
    "# fig2 = plt.figure(figsize=(8,4))\n",
    "# flat_list = list(np.concatenate(y[:,n_sample:]).flat)\n",
    "# flat_list3 = list(np.concatenate(y[:,:n_sample]).flat)\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.hist(flat_list,bins = 100,label = 'voltage')\n",
    "# plt.subplot(1,2,2)\n",
    "# # plt.hist(flat_list3,range=[-2000, 2000],bins = 100,label = 'price')\n",
    "# plt.hist(flat_list3,bins = 100,label = 'price')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "id": "iOyWpG_4yCtz"
   },
   "outputs": [],
   "source": [
    "\n",
    "class dnn(torch.nn.Module):\n",
    "  def __init__(self,shape):\n",
    "    super(dnn,self).__init__()\n",
    "    layers=[]\n",
    "    for idx in range(len(shape)-2):\n",
    "      layers.extend([\n",
    "        nn.Linear(shape[idx],shape[idx+1]),\n",
    "        nn.BatchNorm1d(shape[idx+1]),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "      ])\n",
    "    layers+=[nn.Linear(shape[-2],shape[-1])]\n",
    "    self.features=nn.Sequential(*layers)\n",
    "    for temp in self.features:\n",
    "      if type(temp)==nn.Linear:\n",
    "        torch.nn.init.normal_(temp.weight,mean=0,std=1)\n",
    "  def forward(self,x): return self.features(x)\n",
    "#net=dnn([n_bus*6,n_bus*10,n_bus*10,n_bus*10,n_bus*2]).to(device)\n",
    "#print('number of params: %d'%(sum(temp.numel() for temp in net.parameters() if temp.requires_grad)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_bus: 118\n",
      "Total Parameters: 5,935,636\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 590]         418,310\n",
      "       BatchNorm1d-2                  [-1, 590]           1,180\n",
      "              ReLU-3                  [-1, 590]               0\n",
      "           Dropout-4                  [-1, 590]               0\n",
      "            Linear-5                  [-1, 590]         348,690\n",
      "       BatchNorm1d-6                  [-1, 590]           1,180\n",
      "              ReLU-7                  [-1, 590]               0\n",
      "           Dropout-8                  [-1, 590]               0\n",
      "            Linear-9                 [-1, 1180]         697,380\n",
      "      BatchNorm1d-10                 [-1, 1180]           2,360\n",
      "             ReLU-11                 [-1, 1180]               0\n",
      "          Dropout-12                 [-1, 1180]               0\n",
      "           Linear-13                 [-1, 1180]       1,393,580\n",
      "      BatchNorm1d-14                 [-1, 1180]           2,360\n",
      "             ReLU-15                 [-1, 1180]               0\n",
      "          Dropout-16                 [-1, 1180]               0\n",
      "           Linear-17                 [-1, 1180]       1,393,580\n",
      "      BatchNorm1d-18                 [-1, 1180]           2,360\n",
      "             ReLU-19                 [-1, 1180]               0\n",
      "          Dropout-20                 [-1, 1180]               0\n",
      "           Linear-21                 [-1, 1180]       1,393,580\n",
      "      BatchNorm1d-22                 [-1, 1180]           2,360\n",
      "             ReLU-23                 [-1, 1180]               0\n",
      "          Dropout-24                 [-1, 1180]               0\n",
      "           Linear-25                  [-1, 236]         278,716\n",
      "================================================================\n",
      "Total params: 5,935,636\n",
      "Trainable params: 5,935,636\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.18\n",
      "Params size (MB): 22.64\n",
      "Estimated Total Size (MB): 22.83\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# class dnn(torch.nn.Module):\n",
    "#   def __init__(self,shape,dropout=0):\n",
    "#     super(dnn,self).__init__()\n",
    "#     layers=[]\n",
    "#     for idx in range(len(shape)-2):\n",
    "#       layers.extend([\n",
    "#         nn.Linear(shape[idx],shape[idx+1]),\n",
    "#         nn.ReLU(),\n",
    "#         nn.BatchNorm1d(shape[idx+1]),\n",
    "#         nn.Dropout(dropout)\n",
    "#       ])\n",
    "#     layers.append(nn.Linear(shape[-2],shape[-1]))\n",
    "#     self.features=nn.Sequential(*layers)\n",
    "#     # initialize\n",
    "#     for temp in self.features:\n",
    "#       if type(temp)==nn.Linear:\n",
    "#         torch.nn.init.normal_(temp.weight,mean=0,std=1)\n",
    "#   def forward(self,x):\n",
    "#     return self.features(x)\n",
    "# net=dnn([118*6,118*5,118*5,118*10,118*10,118*10,118*10,118]).to(device)\n",
    "# print('number of params: %d'%(sum(temp.numel() for temp in net.parameters() if temp.requires_grad)))\n",
    "from torchsummary import summary\n",
    "class dnn(torch.nn.Module):\n",
    "    def __init__(self, shape):\n",
    "        super(dnn, self).__init__()\n",
    "        layers = []\n",
    "        for idx in range(len(shape) - 2):\n",
    "            layers.extend([\n",
    "                nn.Linear(shape[idx], shape[idx+1]),\n",
    "                nn.BatchNorm1d(num_features=shape[idx+1]),  # 確保 num_features 正確\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5),\n",
    "            ])\n",
    "        layers.append(nn.Linear(shape[-2], shape[-1]))  # 最後一層\n",
    "        self.features = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # 保證 batch_size 在第一維\n",
    "        return self.features(x)\n",
    "\n",
    "# 測試模型\n",
    "print('n_bus:',n_bus)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#net=dnn([n_bus*6,n_bus*10,n_bus*10,n_bus*10,n_bus*2]).to(device) #source code\n",
    "net = dnn([n_bus*6, n_bus*5, n_bus*5, n_bus*10, n_bus*10, n_bus*10, n_bus*10, n_bus*2]).to(device)#my code\n",
    "\n",
    "\n",
    "# 打印總參數數量\n",
    "total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "\n",
    "summary(net, (1, n_bus*6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "id": "GEQ-MDKOTqW1"
   },
   "outputs": [],
   "source": [
    "# threshold function for p_g\n",
    "class my_gen_pred_binary(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(my_gen_pred_binary,self).__init__()\n",
    "  def forward(self,x,thresh):\n",
    "    right_thresh=thresh.clone().detach().requires_grad_(True).double()\n",
    "    left_thresh=torch.tensor(0).double()\n",
    "    x=x.double()\n",
    "    output = torch.sigmoid(left_thresh - x)\n",
    "    output = torch.mul(output,left_thresh - x) + x\n",
    "    output = torch.sigmoid(output - right_thresh)\n",
    "    output = torch.mul(output,output - right_thresh) + right_thresh\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "id": "9mYCMszHaosA"
   },
   "outputs": [],
   "source": [
    "## params needed for S calculation\n",
    "# line parameters\n",
    "filename1 = root+'ieee118_lineloc.txt'\n",
    "filename2 = root+'ieee118_lineparams.txt'\n",
    "filename3 = root+'ieee118_Bmat.txt'\n",
    "# incidence info\n",
    "line_loc = pd.read_table(filename1,sep=',',header=None).to_numpy()\n",
    "# r, x, shunt, S_max\n",
    "line_params = pd.read_table(filename2,sep=',',header=None).to_numpy()\n",
    "B_mat=pd.read_table(filename3,sep=',',header=None).to_numpy()\n",
    "B_r = np.delete(B_mat,68,axis=0)\n",
    "B_r = np.delete(B_r,68,axis=1)\n",
    "Br_inv = np.linalg.inv(B_r)\n",
    "\n",
    "R_line = line_params[:,0].copy()\n",
    "X_line = line_params[:,1].copy()\n",
    "\n",
    "B_shunt = line_params[:,2].copy()\n",
    "\n",
    "Z_line = R_line + 1j * X_line \n",
    "Y_line = 1 / Z_line\n",
    "\n",
    "G_line = np.real(Y_line)\n",
    "B_line = np.imag(Y_line)\n",
    "\n",
    "# transformer indicator\n",
    "a = (R_line > 0).astype(int)\n",
    "\n",
    "# params to tensor and GPU\n",
    "G_line_tensor = torch.from_numpy(G_line).to(device) # conductance\n",
    "B_line_tensor = torch.from_numpy(B_line).to(device) # susceptance\n",
    "B_shunt_tensor = torch.from_numpy(B_shunt/2).to(device) # conductance\n",
    "Br_inv_tensor = torch.from_numpy(Br_inv).to(device) # reduced Bbus matrix\n",
    "a_tensor = torch.from_numpy(a).double().to(device) # line/transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "Go4bwoEmoi0D"
   },
   "outputs": [],
   "source": [
    "class loss_func:\n",
    "    def __init__(self,s_max,G_line,B_line,B_shunt,Br_inv,a,line_loc):\n",
    "      self.s=s_max\n",
    "      self.g=G_line\n",
    "      self.b=B_line\n",
    "      self.c=B_shunt\n",
    "      self.r=Br_inv\n",
    "      self.a=a\n",
    "      self.mse=nn.MSELoss() # MSE loss\n",
    "      self.lmda1=torch.tensor(10).to(device) # V MSE \n",
    "      self.lmda2=torch.tensor(1).to(device) # pi MSE \n",
    "      self.lmda3=torch.tensor(0.1).to(device) # v l_inf\n",
    "      self.lmda4=torch.tensor(0.1).to(device) # s feasibility\n",
    "      self.lmda5=torch.tensor(0.01).to(device) # pi l_inf\n",
    "      self.line_loc=line_loc\n",
    "      self.binary_cell=my_gen_pred_binary()\n",
    "    def calc(self,pred,label,x,feas):\n",
    "      mse_p=self.mse(pred[:,:n_bus],label[:,:n_bus])\n",
    "      mse_v=self.mse(pred[:,n_bus:],label[:,n_bus:])\n",
    "      linf_p=(pred[:,:n_bus]-label[:,:n_bus]).norm(p=float('inf'))\n",
    "      linf_v=(pred[:,n_bus:]-label[:,n_bus:]).norm(p=float('inf'))\n",
    "      if feas==False:\n",
    "        return self.lmda1*mse_v+self.lmda2*mse_p+self.lmda3*linf_v+self.lmda5*linf_p\n",
    "      label_pred=pred[:,:n_bus]\n",
    "      p_max=x[:,:n_bus*1]-x[:,n_bus*1:n_bus*2]\n",
    "      quadratic_b=x[:,n_bus*4:n_bus*5]\n",
    "      quadratic_a=x[:,n_bus*5:n_bus*6]\n",
    "      quadratic_center=(label_pred-quadratic_b)/(quadratic_a+1e-5)/2\n",
    "      p_inj=self.binary_cell(quadratic_center,p_max)\n",
    "      bus_inj=p_inj+x[:,n_bus:n_bus*2]\n",
    "      p_inj_r=torch.cat((bus_inj[:,:68],bus_inj[:,69:]),1)/100\n",
    "      theta0=torch.matmul(self.r,p_inj_r.T)\n",
    "      ref_ang=torch.zeros(1,theta0.shape[1]).to(device)\n",
    "      theta=torch.cat([theta0[:68,:],ref_ang,theta0[68:,:]],0)\n",
    "      v_pred=(pred[:,n_bus:].transpose(0,1))*0.01+0.9\n",
    "      \n",
    "      # s penalty\n",
    "      theta1=theta[self.line_loc[:,0]-1,:]\n",
    "      theta2=theta[self.line_loc[:,1]-1,:]\n",
    "      V1=(v_pred[self.line_loc[:,0]-1,:]).double()\n",
    "      V2=(v_pred[self.line_loc[:,1]-1,:]).double()\n",
    "      f_p=(self.a*self.g*(V1*V1).T)-self.a*((V1*V2).T)*(self.g*torch.cos(theta1-theta2).T+self.b*torch.sin(theta1-theta2).T)\n",
    "      f_p=f_p.T\n",
    "      f_q=-self.a*(V1.T)*(self.a*V1.T)*(self.b+self.c/2)+self.a*((V1*V2).T)*(self.b*torch.cos(theta1-theta2).T-self.g*torch.sin(theta1-theta2).T)\n",
    "      f_q=f_q.T\n",
    "      s_pred=torch.sqrt(f_p*f_p+f_q*f_q+1e-5)*100\n",
    "      s_penalty=torch.sigmoid(s_pred-self.s)+torch.sigmoid(-s_pred-self.s)\n",
    "      s_total=torch.sum(s_penalty)\n",
    "\n",
    "      # sji penalty\n",
    "      theta1=theta[self.line_loc[:,1]-1,:]\n",
    "      theta2=theta[self.line_loc[:,0]-1,:]\n",
    "      V1=(v_pred[self.line_loc[:,1]-1,:]).double()\n",
    "      V2=(v_pred[self.line_loc[:,0]-1,:]).double() \n",
    "      fji_p=(self.a*self.g*(V1*V1).T)-self.a*((V1*V2).T)*(self.g*torch.cos(theta1-theta2).T+self.b*torch.sin(theta1-theta2).T)\n",
    "      fji_p=fji_p.T\n",
    "      fji_q=-self.a*(V1.T)*(self.a*V1.T)*(self.b+self.c/2)+self.a*((V1*V2).T)*(self.b*torch.cos(theta1-theta2).T-self.g*torch.sin(theta1-theta2).T)\n",
    "      fji_q=fji_q.T\n",
    "      sji_pred=torch.sqrt(fji_p*fji_p+fji_q*fji_q+1e-5)*100\n",
    "      sji_penalty=torch.sigmoid(sji_pred-self.s)+torch.sigmoid(-sji_pred-self.s)\n",
    "      sji_total=torch.sum(sji_penalty)\n",
    "\n",
    "      return self.lmda1*mse_v+self.lmda2*mse_p+self.lmda3*linf_v+self.lmda5*linf_p+self.lmda4*s_total+self.lmda4*sji_total\n",
    "my_loss=loss_func(f_max,G_line_tensor,B_line_tensor,B_shunt_tensor,Br_inv_tensor,a_tensor,line_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cold start\n",
      "Epoch 0 | Training loss: 403.5626 | Learning Rate: 0.001000\n",
      "Epoch 0 | Validation loss: 406.5165 | Learning Rate: 0.001000\n",
      "Epoch 1 | Training loss: 387.9941 | Learning Rate: 0.001000\n",
      "Epoch 2 | Training loss: 373.1838 | Learning Rate: 0.001000\n",
      "Epoch 3 | Training loss: 357.4741 | Learning Rate: 0.001000\n",
      "Epoch 4 | Training loss: 340.2688 | Learning Rate: 0.001000\n",
      "Epoch 5 | Training loss: 321.1726 | Learning Rate: 0.001000\n",
      "Epoch 5 | Validation loss: 280.7936 | Learning Rate: 0.001000\n",
      "Epoch 6 | Training loss: 294.3197 | Learning Rate: 0.001000\n",
      "Epoch 7 | Training loss: 263.9802 | Learning Rate: 0.001000\n",
      "Epoch 8 | Training loss: 238.5010 | Learning Rate: 0.001000\n",
      "Epoch 9 | Training loss: 214.5891 | Learning Rate: 0.001000\n",
      "Epoch 10 | Training loss: 191.2568 | Learning Rate: 0.001000\n",
      "Epoch 10 | Validation loss: 181.2530 | Learning Rate: 0.001000\n",
      "Epoch 11 | Training loss: 169.0323 | Learning Rate: 0.001000\n",
      "Epoch 12 | Training loss: 148.0212 | Learning Rate: 0.001000\n",
      "Epoch 13 | Training loss: 128.4491 | Learning Rate: 0.001000\n",
      "Epoch 14 | Training loss: 109.9885 | Learning Rate: 0.001000\n",
      "Epoch 15 | Training loss: 93.3269 | Learning Rate: 0.001000\n",
      "Epoch 15 | Validation loss: 94.5327 | Learning Rate: 0.001000\n",
      "Epoch 16 | Training loss: 78.3499 | Learning Rate: 0.001000\n",
      "Epoch 17 | Training loss: 65.1149 | Learning Rate: 0.001000\n",
      "Epoch 18 | Training loss: 53.5095 | Learning Rate: 0.001000\n",
      "Epoch 19 | Training loss: 43.5685 | Learning Rate: 0.001000\n",
      "Epoch 20 | Training loss: 35.0571 | Learning Rate: 0.001000\n",
      "Epoch 20 | Validation loss: 27.9435 | Learning Rate: 0.001000\n",
      "Epoch 21 | Training loss: 27.8191 | Learning Rate: 0.001000\n",
      "Epoch 22 | Training loss: 22.1687 | Learning Rate: 0.001000\n",
      "Epoch 23 | Training loss: 17.3280 | Learning Rate: 0.001000\n",
      "Epoch 24 | Training loss: 13.6566 | Learning Rate: 0.001000\n",
      "Epoch 25 | Training loss: 10.7519 | Learning Rate: 0.001000\n",
      "Epoch 25 | Validation loss: 8.9654 | Learning Rate: 0.001000\n",
      "Epoch 26 | Training loss: 8.7620 | Learning Rate: 0.001000\n",
      "Epoch 27 | Training loss: 7.0999 | Learning Rate: 0.001000\n",
      "Epoch 28 | Training loss: 5.8544 | Learning Rate: 0.001000\n",
      "Epoch 29 | Training loss: 5.0042 | Learning Rate: 0.001000\n",
      "Epoch 30 | Training loss: 4.3845 | Learning Rate: 0.001000\n",
      "Epoch 30 | Validation loss: 2.4877 | Learning Rate: 0.001000\n",
      "Epoch 31 | Training loss: 3.8451 | Learning Rate: 0.001000\n",
      "Epoch 32 | Training loss: 3.4917 | Learning Rate: 0.001000\n",
      "Epoch 33 | Training loss: 3.2767 | Learning Rate: 0.001000\n",
      "Epoch 34 | Training loss: 3.0510 | Learning Rate: 0.001000\n",
      "Epoch 35 | Training loss: 2.8639 | Learning Rate: 0.001000\n",
      "Epoch 35 | Validation loss: 1.9637 | Learning Rate: 0.001000\n",
      "Epoch 36 | Training loss: 2.7820 | Learning Rate: 0.001000\n",
      "Epoch 37 | Training loss: 2.6074 | Learning Rate: 0.001000\n",
      "Epoch 38 | Training loss: 2.5339 | Learning Rate: 0.001000\n",
      "Epoch 39 | Training loss: 2.5370 | Learning Rate: 0.001000\n",
      "Epoch 40 | Training loss: 2.4772 | Learning Rate: 0.001000\n",
      "Epoch 40 | Validation loss: 1.4970 | Learning Rate: 0.001000\n",
      "Epoch 41 | Training loss: 2.3639 | Learning Rate: 0.001000\n",
      "Epoch 42 | Training loss: 2.3214 | Learning Rate: 0.001000\n",
      "Epoch 43 | Training loss: 2.3431 | Learning Rate: 0.001000\n",
      "Epoch 44 | Training loss: 2.2831 | Learning Rate: 0.001000\n",
      "Epoch 45 | Training loss: 2.2753 | Learning Rate: 0.001000\n",
      "Epoch 45 | Validation loss: 1.3854 | Learning Rate: 0.001000\n",
      "Epoch 46 | Training loss: 2.1897 | Learning Rate: 0.001000\n",
      "Epoch 47 | Training loss: 2.2444 | Learning Rate: 0.001000\n",
      "Epoch 48 | Training loss: 2.2247 | Learning Rate: 0.001000\n",
      "Epoch 49 | Training loss: 2.2288 | Learning Rate: 0.001000\n",
      "Epoch 50 | Training loss: 2.2107 | Learning Rate: 0.001000\n",
      "Epoch 50 | Validation loss: 1.3580 | Learning Rate: 0.001000\n",
      "Epoch 51 | Training loss: 2.1515 | Learning Rate: 0.001000\n",
      "Epoch 52 | Training loss: 2.1028 | Learning Rate: 0.001000\n",
      "Epoch 53 | Training loss: 2.1093 | Learning Rate: 0.001000\n",
      "Epoch 54 | Training loss: 2.0907 | Learning Rate: 0.001000\n",
      "Epoch 55 | Training loss: 2.1326 | Learning Rate: 0.001000\n",
      "Epoch 55 | Validation loss: 1.2979 | Learning Rate: 0.001000\n",
      "Epoch 56 | Training loss: 2.1661 | Learning Rate: 0.001000\n",
      "Epoch 57 | Training loss: 2.1173 | Learning Rate: 0.001000\n",
      "Epoch 58 | Training loss: 2.1403 | Learning Rate: 0.001000\n",
      "Epoch 59 | Training loss: 2.1760 | Learning Rate: 0.001000\n",
      "Epoch 60 | Training loss: 2.0972 | Learning Rate: 0.001000\n",
      "Epoch 60 | Validation loss: 1.2785 | Learning Rate: 0.001000\n",
      "Epoch 61 | Training loss: 2.0221 | Learning Rate: 0.001000\n",
      "Epoch 62 | Training loss: 2.0020 | Learning Rate: 0.001000\n",
      "Epoch 63 | Training loss: 2.0691 | Learning Rate: 0.001000\n",
      "Epoch 64 | Training loss: 1.9868 | Learning Rate: 0.001000\n",
      "Epoch 65 | Training loss: 2.0716 | Learning Rate: 0.001000\n",
      "Epoch 65 | Validation loss: 1.3177 | Learning Rate: 0.001000\n",
      "Epoch 66 | Training loss: 2.0263 | Learning Rate: 0.001000\n",
      "Epoch 67 | Training loss: 2.0284 | Learning Rate: 0.001000\n",
      "Epoch 68 | Training loss: 2.0102 | Learning Rate: 0.001000\n",
      "Epoch 69 | Training loss: 2.0467 | Learning Rate: 0.001000\n",
      "Epoch 70 | Training loss: 2.0157 | Learning Rate: 0.001000\n",
      "Epoch 70 | Validation loss: 1.2715 | Learning Rate: 0.001000\n",
      "Epoch 71 | Training loss: 2.0027 | Learning Rate: 0.001000\n",
      "Epoch 72 | Training loss: 1.9737 | Learning Rate: 0.001000\n",
      "Epoch 73 | Training loss: 1.9923 | Learning Rate: 0.001000\n",
      "Epoch 74 | Training loss: 1.9877 | Learning Rate: 0.001000\n",
      "Epoch 75 | Training loss: 2.0034 | Learning Rate: 0.001000\n",
      "Epoch 75 | Validation loss: 1.4414 | Learning Rate: 0.001000\n",
      "Epoch 76 | Training loss: 2.0067 | Learning Rate: 0.001000\n",
      "Epoch 77 | Training loss: 1.8989 | Learning Rate: 0.001000\n",
      "Epoch 78 | Training loss: 1.9740 | Learning Rate: 0.001000\n",
      "Epoch 79 | Training loss: 1.9531 | Learning Rate: 0.001000\n",
      "Epoch 80 | Training loss: 1.9468 | Learning Rate: 0.001000\n",
      "Epoch 80 | Validation loss: 1.6089 | Learning Rate: 0.001000\n",
      "Epoch 81 | Training loss: 1.9134 | Learning Rate: 0.001000\n",
      "Epoch 82 | Training loss: 1.9151 | Learning Rate: 0.001000\n",
      "Epoch 83 | Training loss: 1.8852 | Learning Rate: 0.001000\n",
      "Epoch 84 | Training loss: 1.9107 | Learning Rate: 0.001000\n",
      "Epoch 85 | Training loss: 1.9129 | Learning Rate: 0.001000\n",
      "Epoch 85 | Validation loss: 1.5984 | Learning Rate: 0.001000\n",
      "Epoch 86 | Training loss: 1.9423 | Learning Rate: 0.001000\n",
      "Epoch 87 | Training loss: 1.8561 | Learning Rate: 0.001000\n",
      "Epoch 88 | Training loss: 1.8725 | Learning Rate: 0.001000\n",
      "Epoch 89 | Training loss: 1.8480 | Learning Rate: 0.001000\n",
      "Epoch 90 | Training loss: 1.9398 | Learning Rate: 0.001000\n",
      "Epoch 90 | Validation loss: 1.2968 | Learning Rate: 0.001000\n",
      "Epoch 91 | Training loss: 1.8661 | Learning Rate: 0.001000\n",
      "Epoch 92 | Training loss: 1.8856 | Learning Rate: 0.001000\n",
      "Epoch 93 | Training loss: 1.8710 | Learning Rate: 0.001000\n",
      "Epoch 94 | Training loss: 1.8600 | Learning Rate: 0.001000\n",
      "Epoch 95 | Training loss: 1.8137 | Learning Rate: 0.001000\n",
      "Epoch 95 | Validation loss: 2.4727 | Learning Rate: 0.001000\n",
      "Epoch 96 | Training loss: 1.8848 | Learning Rate: 0.001000\n",
      "Epoch 97 | Training loss: 1.8925 | Learning Rate: 0.001000\n",
      "Epoch 98 | Training loss: 1.8990 | Learning Rate: 0.001000\n",
      "Epoch 99 | Training loss: 1.9626 | Learning Rate: 0.001000\n",
      "Epoch 100 | Training loss: 1.9037 | Learning Rate: 0.001000\n",
      "Epoch 100 | Validation loss: 2.3476 | Learning Rate: 0.001000\n",
      "Epoch 101 | Training loss: 1.8448 | Learning Rate: 0.001000\n",
      "Epoch 102 | Training loss: 1.8025 | Learning Rate: 0.001000\n",
      "Epoch 103 | Training loss: 1.8406 | Learning Rate: 0.001000\n",
      "Epoch 104 | Training loss: 1.8098 | Learning Rate: 0.001000\n",
      "Epoch 105 | Training loss: 1.8722 | Learning Rate: 0.001000\n",
      "Epoch 105 | Validation loss: 1.4059 | Learning Rate: 0.001000\n",
      "Epoch 106 | Training loss: 1.8107 | Learning Rate: 0.001000\n",
      "Epoch 107 | Training loss: 1.8132 | Learning Rate: 0.001000\n",
      "Epoch 108 | Training loss: 1.7974 | Learning Rate: 0.001000\n",
      "Epoch 109 | Training loss: 1.7576 | Learning Rate: 0.001000\n",
      "Epoch 110 | Training loss: 1.7644 | Learning Rate: 0.001000\n",
      "Epoch 110 | Validation loss: 1.5155 | Learning Rate: 0.001000\n",
      "Epoch 111 | Training loss: 1.7476 | Learning Rate: 0.001000\n",
      "Epoch 112 | Training loss: 1.7945 | Learning Rate: 0.001000\n",
      "Epoch 113 | Training loss: 1.7577 | Learning Rate: 0.001000\n",
      "Epoch 114 | Training loss: 1.7560 | Learning Rate: 0.001000\n",
      "Epoch 115 | Training loss: 1.8781 | Learning Rate: 0.001000\n",
      "Epoch 115 | Validation loss: 1.5147 | Learning Rate: 0.001000\n",
      "Epoch 116 | Training loss: 1.8532 | Learning Rate: 0.001000\n",
      "Epoch 117 | Training loss: 1.7902 | Learning Rate: 0.001000\n",
      "Epoch 118 | Training loss: 1.7462 | Learning Rate: 0.001000\n",
      "Epoch 119 | Training loss: 1.7347 | Learning Rate: 0.001000\n",
      "Epoch 120 | Training loss: 1.6662 | Learning Rate: 0.001000\n",
      "Epoch 120 | Validation loss: 1.2844 | Learning Rate: 0.001000\n",
      "Epoch 121 | Training loss: 1.7236 | Learning Rate: 0.001000\n",
      "Epoch 122 | Training loss: 1.7472 | Learning Rate: 0.001000\n",
      "Epoch 123 | Training loss: 1.7436 | Learning Rate: 0.001000\n",
      "Epoch 124 | Training loss: 1.7504 | Learning Rate: 0.001000\n",
      "Epoch 125 | Training loss: 1.6604 | Learning Rate: 0.001000\n",
      "Epoch 125 | Validation loss: 1.7737 | Learning Rate: 0.001000\n",
      "Epoch 126 | Training loss: 1.6599 | Learning Rate: 0.001000\n",
      "Epoch 127 | Training loss: 1.6607 | Learning Rate: 0.001000\n",
      "Epoch 128 | Training loss: 1.6398 | Learning Rate: 0.001000\n",
      "Epoch 129 | Training loss: 1.6152 | Learning Rate: 0.001000\n",
      "Epoch 130 | Training loss: 1.7166 | Learning Rate: 0.001000\n",
      "Epoch 130 | Validation loss: 1.2680 | Learning Rate: 0.001000\n",
      "Epoch 131 | Training loss: 1.6659 | Learning Rate: 0.001000\n",
      "Epoch 132 | Training loss: 1.6647 | Learning Rate: 0.001000\n",
      "Epoch 133 | Training loss: 1.6446 | Learning Rate: 0.001000\n",
      "Epoch 134 | Training loss: 1.6123 | Learning Rate: 0.001000\n",
      "Epoch 135 | Training loss: 1.6622 | Learning Rate: 0.001000\n",
      "Epoch 135 | Validation loss: 2.5363 | Learning Rate: 0.001000\n",
      "Epoch 136 | Training loss: 1.7429 | Learning Rate: 0.001000\n",
      "Epoch 137 | Training loss: 1.6197 | Learning Rate: 0.001000\n",
      "Epoch 138 | Training loss: 1.6318 | Learning Rate: 0.001000\n",
      "Epoch 139 | Training loss: 1.6386 | Learning Rate: 0.001000\n",
      "Epoch 140 | Training loss: 1.5457 | Learning Rate: 0.001000\n",
      "Epoch 140 | Validation loss: 1.3125 | Learning Rate: 0.001000\n",
      "Epoch 141 | Training loss: 1.5834 | Learning Rate: 0.001000\n",
      "Epoch 142 | Training loss: 1.6189 | Learning Rate: 0.001000\n",
      "Epoch 143 | Training loss: 1.5363 | Learning Rate: 0.001000\n",
      "Epoch 144 | Training loss: 1.5684 | Learning Rate: 0.001000\n",
      "Epoch 145 | Training loss: 1.6153 | Learning Rate: 0.001000\n",
      "Epoch 145 | Validation loss: 1.2283 | Learning Rate: 0.001000\n",
      "Epoch 146 | Training loss: 1.5130 | Learning Rate: 0.001000\n",
      "Epoch 147 | Training loss: 1.5601 | Learning Rate: 0.001000\n",
      "Epoch 148 | Training loss: 1.5739 | Learning Rate: 0.001000\n",
      "Epoch 149 | Training loss: 1.5574 | Learning Rate: 0.001000\n",
      "Epoch 150 | Training loss: 1.5042 | Learning Rate: 0.001000\n",
      "Epoch 150 | Validation loss: 1.2476 | Learning Rate: 0.001000\n",
      "Epoch 151 | Training loss: 1.5931 | Learning Rate: 0.001000\n",
      "Epoch 152 | Training loss: 1.5799 | Learning Rate: 0.001000\n",
      "Epoch 153 | Training loss: 1.5791 | Learning Rate: 0.001000\n",
      "Epoch 154 | Training loss: 1.5372 | Learning Rate: 0.001000\n",
      "Epoch 155 | Training loss: 1.5332 | Learning Rate: 0.001000\n",
      "Epoch 155 | Validation loss: 1.7199 | Learning Rate: 0.001000\n",
      "Epoch 156 | Training loss: 1.5718 | Learning Rate: 0.001000\n",
      "Epoch 157 | Training loss: 1.5726 | Learning Rate: 0.001000\n",
      "Epoch 158 | Training loss: 1.5099 | Learning Rate: 0.001000\n",
      "Epoch 159 | Training loss: 1.5237 | Learning Rate: 0.001000\n",
      "Epoch 160 | Training loss: 1.5038 | Learning Rate: 0.001000\n",
      "Epoch 160 | Validation loss: 1.8704 | Learning Rate: 0.001000\n",
      "Epoch 161 | Training loss: 1.4869 | Learning Rate: 0.001000\n",
      "Epoch 162 | Training loss: 1.5128 | Learning Rate: 0.001000\n",
      "Epoch 163 | Training loss: 1.4638 | Learning Rate: 0.001000\n",
      "Epoch 164 | Training loss: 1.4668 | Learning Rate: 0.001000\n",
      "Epoch 165 | Training loss: 1.5338 | Learning Rate: 0.001000\n",
      "Epoch 165 | Validation loss: 1.2267 | Learning Rate: 0.001000\n",
      "Epoch 166 | Training loss: 1.4386 | Learning Rate: 0.001000\n",
      "Epoch 167 | Training loss: 1.5010 | Learning Rate: 0.001000\n",
      "Epoch 168 | Training loss: 1.4803 | Learning Rate: 0.001000\n",
      "Epoch 169 | Training loss: 1.5128 | Learning Rate: 0.001000\n",
      "Epoch 170 | Training loss: 1.4614 | Learning Rate: 0.001000\n",
      "Epoch 170 | Validation loss: 1.2511 | Learning Rate: 0.001000\n",
      "Epoch 171 | Training loss: 1.3946 | Learning Rate: 0.001000\n",
      "Epoch 172 | Training loss: 1.4540 | Learning Rate: 0.001000\n",
      "Epoch 173 | Training loss: 1.4574 | Learning Rate: 0.001000\n",
      "Epoch 174 | Training loss: 1.4894 | Learning Rate: 0.001000\n",
      "Epoch 175 | Training loss: 1.4528 | Learning Rate: 0.001000\n",
      "Epoch 175 | Validation loss: 5.5446 | Learning Rate: 0.001000\n",
      "Epoch 176 | Training loss: 1.4430 | Learning Rate: 0.001000\n",
      "Epoch 177 | Training loss: 1.4444 | Learning Rate: 0.001000\n",
      "Epoch 178 | Training loss: 1.3923 | Learning Rate: 0.001000\n",
      "Epoch 179 | Training loss: 1.4174 | Learning Rate: 0.001000\n",
      "Epoch 180 | Training loss: 1.4087 | Learning Rate: 0.001000\n",
      "Epoch 180 | Validation loss: 1.3283 | Learning Rate: 0.001000\n",
      "Epoch 181 | Training loss: 1.3850 | Learning Rate: 0.001000\n",
      "Epoch 182 | Training loss: 1.3829 | Learning Rate: 0.001000\n",
      "Epoch 183 | Training loss: 1.4010 | Learning Rate: 0.001000\n",
      "Epoch 184 | Training loss: 1.4250 | Learning Rate: 0.001000\n",
      "Epoch 185 | Training loss: 1.4390 | Learning Rate: 0.001000\n",
      "Epoch 185 | Validation loss: 1.3028 | Learning Rate: 0.001000\n",
      "Epoch 186 | Training loss: 1.4380 | Learning Rate: 0.001000\n",
      "Epoch 187 | Training loss: 1.3702 | Learning Rate: 0.001000\n",
      "Epoch 188 | Training loss: 1.3893 | Learning Rate: 0.001000\n",
      "Epoch 189 | Training loss: 1.3471 | Learning Rate: 0.001000\n",
      "Epoch 190 | Training loss: 1.3930 | Learning Rate: 0.001000\n",
      "Epoch 190 | Validation loss: 1.8211 | Learning Rate: 0.001000\n",
      "Epoch 191 | Training loss: 1.4006 | Learning Rate: 0.001000\n",
      "Epoch 192 | Training loss: 1.3728 | Learning Rate: 0.001000\n",
      "Epoch 193 | Training loss: 1.3544 | Learning Rate: 0.001000\n",
      "Epoch 194 | Training loss: 1.3531 | Learning Rate: 0.001000\n",
      "Epoch 195 | Training loss: 1.3399 | Learning Rate: 0.001000\n",
      "Epoch 195 | Validation loss: 2.9286 | Learning Rate: 0.001000\n",
      "Epoch 196 | Training loss: 1.4158 | Learning Rate: 0.001000\n",
      "Epoch 197 | Training loss: 1.3528 | Learning Rate: 0.001000\n",
      "Epoch 198 | Training loss: 1.3690 | Learning Rate: 0.001000\n",
      "Epoch 199 | Training loss: 1.2913 | Learning Rate: 0.001000\n",
      "Epoch 200 | Training loss: 1.3303 | Learning Rate: 0.001000\n",
      "Epoch 200 | Validation loss: 1.9492 | Learning Rate: 0.001000\n",
      "Epoch 201 | Training loss: 1.3395 | Learning Rate: 0.001000\n",
      "Epoch 202 | Training loss: 1.3726 | Learning Rate: 0.001000\n",
      "Epoch 203 | Training loss: 1.3645 | Learning Rate: 0.001000\n",
      "Epoch 204 | Training loss: 1.3660 | Learning Rate: 0.001000\n",
      "Epoch 205 | Training loss: 1.3404 | Learning Rate: 0.001000\n",
      "Epoch 205 | Validation loss: 1.0834 | Learning Rate: 0.001000\n",
      "Epoch 206 | Training loss: 1.3188 | Learning Rate: 0.001000\n",
      "Epoch 207 | Training loss: 1.3271 | Learning Rate: 0.001000\n",
      "Epoch 208 | Training loss: 1.2961 | Learning Rate: 0.001000\n",
      "Epoch 209 | Training loss: 1.3490 | Learning Rate: 0.001000\n",
      "Epoch 210 | Training loss: 1.3156 | Learning Rate: 0.001000\n",
      "Epoch 210 | Validation loss: 1.2930 | Learning Rate: 0.001000\n",
      "Epoch 211 | Training loss: 1.3198 | Learning Rate: 0.001000\n",
      "Epoch 212 | Training loss: 1.3149 | Learning Rate: 0.001000\n",
      "Epoch 213 | Training loss: 1.3057 | Learning Rate: 0.001000\n",
      "Epoch 214 | Training loss: 1.3009 | Learning Rate: 0.001000\n",
      "Epoch 215 | Training loss: 1.3079 | Learning Rate: 0.001000\n",
      "Epoch 215 | Validation loss: 1.2215 | Learning Rate: 0.001000\n",
      "Epoch 216 | Training loss: 1.3842 | Learning Rate: 0.001000\n",
      "Epoch 217 | Training loss: 1.3200 | Learning Rate: 0.001000\n",
      "Epoch 218 | Training loss: 1.3462 | Learning Rate: 0.001000\n",
      "Epoch 219 | Training loss: 1.3303 | Learning Rate: 0.001000\n",
      "Epoch 220 | Training loss: 1.3117 | Learning Rate: 0.001000\n",
      "Epoch 220 | Validation loss: 1.1888 | Learning Rate: 0.001000\n",
      "Epoch 221 | Training loss: 1.2938 | Learning Rate: 0.001000\n",
      "Epoch 222 | Training loss: 1.2709 | Learning Rate: 0.001000\n",
      "Epoch 223 | Training loss: 1.2608 | Learning Rate: 0.001000\n",
      "Epoch 224 | Training loss: 1.2605 | Learning Rate: 0.001000\n",
      "Epoch 225 | Training loss: 1.2871 | Learning Rate: 0.001000\n",
      "Epoch 225 | Validation loss: 4.7648 | Learning Rate: 0.001000\n",
      "Epoch 226 | Training loss: 1.2823 | Learning Rate: 0.001000\n",
      "Epoch 227 | Training loss: 1.2644 | Learning Rate: 0.001000\n",
      "Epoch 228 | Training loss: 1.2868 | Learning Rate: 0.001000\n",
      "Epoch 229 | Training loss: 1.2658 | Learning Rate: 0.001000\n",
      "Epoch 230 | Training loss: 1.2594 | Learning Rate: 0.001000\n",
      "Epoch 230 | Validation loss: 2.4134 | Learning Rate: 0.001000\n",
      "Epoch 231 | Training loss: 1.2676 | Learning Rate: 0.001000\n",
      "Epoch 232 | Training loss: 1.2492 | Learning Rate: 0.001000\n",
      "Epoch 233 | Training loss: 1.2598 | Learning Rate: 0.001000\n",
      "Epoch 234 | Training loss: 1.2575 | Learning Rate: 0.001000\n",
      "Epoch 235 | Training loss: 1.2672 | Learning Rate: 0.001000\n",
      "Epoch 235 | Validation loss: 1.2899 | Learning Rate: 0.001000\n",
      "Epoch 236 | Training loss: 1.2432 | Learning Rate: 0.001000\n",
      "Epoch 237 | Training loss: 1.2412 | Learning Rate: 0.001000\n",
      "Epoch 238 | Training loss: 1.2579 | Learning Rate: 0.001000\n",
      "Epoch 239 | Training loss: 1.2446 | Learning Rate: 0.001000\n",
      "Epoch 240 | Training loss: 1.2390 | Learning Rate: 0.001000\n",
      "Epoch 240 | Validation loss: 1.0870 | Learning Rate: 0.001000\n",
      "Epoch 241 | Training loss: 1.2839 | Learning Rate: 0.001000\n",
      "Epoch 242 | Training loss: 1.2171 | Learning Rate: 0.001000\n",
      "Epoch 243 | Training loss: 1.2495 | Learning Rate: 0.001000\n",
      "Epoch 244 | Training loss: 1.2283 | Learning Rate: 0.001000\n",
      "Epoch 245 | Training loss: 1.2634 | Learning Rate: 0.001000\n",
      "Epoch 245 | Validation loss: 1.1068 | Learning Rate: 0.001000\n",
      "Epoch 246 | Training loss: 1.2117 | Learning Rate: 0.001000\n",
      "Epoch 247 | Training loss: 1.2189 | Learning Rate: 0.001000\n",
      "Epoch 248 | Training loss: 1.2045 | Learning Rate: 0.001000\n",
      "Epoch 249 | Training loss: 1.2119 | Learning Rate: 0.001000\n",
      "Epoch 250 | Training loss: 1.2362 | Learning Rate: 0.001000\n",
      "Epoch 250 | Validation loss: 3.5041 | Learning Rate: 0.001000\n",
      "Epoch 251 | Training loss: 1.2103 | Learning Rate: 0.001000\n",
      "Epoch 252 | Training loss: 1.1843 | Learning Rate: 0.001000\n",
      "Epoch 253 | Training loss: 1.1963 | Learning Rate: 0.001000\n",
      "Epoch 254 | Training loss: 1.1939 | Learning Rate: 0.001000\n",
      "Epoch 255 | Training loss: 1.1597 | Learning Rate: 0.001000\n",
      "Epoch 255 | Validation loss: 1.4593 | Learning Rate: 0.001000\n",
      "Epoch 256 | Training loss: 1.2008 | Learning Rate: 0.001000\n",
      "Epoch 257 | Training loss: 1.1821 | Learning Rate: 0.001000\n",
      "Epoch 258 | Training loss: 1.2067 | Learning Rate: 0.001000\n",
      "Epoch 259 | Training loss: 1.1974 | Learning Rate: 0.001000\n",
      "Epoch 260 | Training loss: 1.2204 | Learning Rate: 0.001000\n",
      "Epoch 260 | Validation loss: 1.5548 | Learning Rate: 0.001000\n",
      "Epoch 261 | Training loss: 1.1706 | Learning Rate: 0.001000\n",
      "Epoch 262 | Training loss: 1.2039 | Learning Rate: 0.001000\n",
      "Epoch 263 | Training loss: 1.1753 | Learning Rate: 0.001000\n",
      "Epoch 264 | Training loss: 1.1825 | Learning Rate: 0.001000\n",
      "Epoch 265 | Training loss: 1.1852 | Learning Rate: 0.001000\n",
      "Epoch 265 | Validation loss: 1.3763 | Learning Rate: 0.001000\n",
      "Epoch 266 | Training loss: 1.1656 | Learning Rate: 0.001000\n",
      "Epoch 267 | Training loss: 1.1460 | Learning Rate: 0.001000\n",
      "Epoch 268 | Training loss: 1.1455 | Learning Rate: 0.001000\n",
      "Epoch 269 | Training loss: 1.1780 | Learning Rate: 0.001000\n",
      "Epoch 270 | Training loss: 1.1620 | Learning Rate: 0.001000\n",
      "Epoch 270 | Validation loss: 1.6823 | Learning Rate: 0.001000\n",
      "Epoch 271 | Training loss: 1.2130 | Learning Rate: 0.001000\n",
      "Epoch 272 | Training loss: 1.2027 | Learning Rate: 0.001000\n",
      "Epoch 273 | Training loss: 1.1868 | Learning Rate: 0.001000\n",
      "Epoch 274 | Training loss: 1.1406 | Learning Rate: 0.001000\n",
      "Epoch 275 | Training loss: 1.1558 | Learning Rate: 0.001000\n",
      "Epoch 275 | Validation loss: 2.0497 | Learning Rate: 0.001000\n",
      "Epoch 276 | Training loss: 1.1896 | Learning Rate: 0.001000\n",
      "Epoch 277 | Training loss: 1.1948 | Learning Rate: 0.001000\n",
      "Epoch 278 | Training loss: 1.1492 | Learning Rate: 0.001000\n",
      "Epoch 279 | Training loss: 1.2125 | Learning Rate: 0.001000\n",
      "Epoch 280 | Training loss: 1.1964 | Learning Rate: 0.001000\n",
      "Epoch 280 | Validation loss: 1.6673 | Learning Rate: 0.001000\n",
      "Epoch 281 | Training loss: 1.1337 | Learning Rate: 0.001000\n",
      "Epoch 282 | Training loss: 1.1744 | Learning Rate: 0.001000\n",
      "Epoch 283 | Training loss: 1.1993 | Learning Rate: 0.001000\n",
      "Epoch 284 | Training loss: 1.2013 | Learning Rate: 0.001000\n",
      "Epoch 285 | Training loss: 1.1944 | Learning Rate: 0.001000\n",
      "Epoch 285 | Validation loss: 1.2909 | Learning Rate: 0.001000\n",
      "Epoch 286 | Training loss: 1.1827 | Learning Rate: 0.001000\n",
      "Epoch 287 | Training loss: 1.1443 | Learning Rate: 0.001000\n",
      "Epoch 288 | Training loss: 1.1680 | Learning Rate: 0.001000\n",
      "Epoch 289 | Training loss: 1.1491 | Learning Rate: 0.001000\n",
      "Epoch 290 | Training loss: 1.1510 | Learning Rate: 0.001000\n",
      "Epoch 290 | Validation loss: 1.1879 | Learning Rate: 0.001000\n",
      "Epoch 291 | Training loss: 1.1481 | Learning Rate: 0.001000\n",
      "Epoch 292 | Training loss: 1.1619 | Learning Rate: 0.001000\n",
      "Epoch 293 | Training loss: 1.1582 | Learning Rate: 0.001000\n",
      "Epoch 294 | Training loss: 1.1369 | Learning Rate: 0.001000\n",
      "Epoch 295 | Training loss: 1.1330 | Learning Rate: 0.001000\n",
      "Epoch 295 | Validation loss: 1.1371 | Learning Rate: 0.001000\n",
      "Epoch 296 | Training loss: 1.1655 | Learning Rate: 0.001000\n",
      "Epoch 297 | Training loss: 1.1610 | Learning Rate: 0.001000\n",
      "Epoch 298 | Training loss: 1.1599 | Learning Rate: 0.001000\n",
      "Epoch 299 | Training loss: 1.1233 | Learning Rate: 0.001000\n",
      "Training time: 300.8326s\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "path = root + 'data_118_quad/gnn_trained_ac118.pickle'\n",
    "try:\n",
    "    net.load_state_dict(torch.load(path))\n",
    "    print('params loaded')\n",
    "except:\n",
    "    print('cold start')\n",
    "\n",
    "# 初始學習率\n",
    "lr = 0.001\n",
    "factor = 0.9\n",
    "patience = 5\n",
    "optimizer = optim.AdamW(net.parameters(), lr)\n",
    "\n",
    "# Learning Rate Scheduler: ReduceLROnPlateau\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#     optimizer, \n",
    "#     mode='min',       # 監測 loss，越小越好\n",
    "#     factor=0.9,       # 學習率變成原來的 90%\n",
    "#     patience=5,       # 5 個 epoch 沒有改善才降低學習率\n",
    "#     verbose=True,     # 會輸出學習率變更訊息\n",
    "#     min_lr=1e-6       # 學習率最低不低於 1e-6\n",
    "# )\n",
    "\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#     optimizer, \n",
    "#     mode='min',       # 監測 loss\n",
    "#     factor=factor,       # 學習率變成 50% (下降更多)\n",
    "#     patience=patience,       # 3 個 epoch 沒有改善就降低學習率\n",
    "#     verbose=True,     \n",
    "#     min_lr=1e-6       \n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "lr_list = []\n",
    "\n",
    "t0 = time.time()\n",
    "max_epochs = 300\n",
    "eval_epoch = 5\n",
    "\n",
    "tolerance = 5  # 早停耐心值\n",
    "min_delta = 1e-3\n",
    "previous = float('inf')\n",
    "\n",
    "feas = False  # 加入可行性標記\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    total_loss = 0.0\n",
    "    for local_batch, local_label in train_set:\n",
    "        optimizer.zero_grad()\n",
    "        local_batch, local_label = local_batch.to(device), local_label.to(device)\n",
    "        logits = net(local_batch)\n",
    "        loss = my_loss.calc(logits, local_label, local_batch, feas)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_set.dataset)\n",
    "    train_loss.append(avg_loss)\n",
    "    lr_list.append(lr)\n",
    "    print(f\"Epoch {epoch} | Training loss: {avg_loss:.4f} | Learning Rate: {lr:.6f}\")\n",
    "    \n",
    "    if epoch % eval_epoch == 0:\n",
    "        net.eval()\n",
    "        total_loss = 0.0\n",
    "        for local_batch, local_label in val_set:\n",
    "            local_batch, local_label = local_batch.to(device), local_label.to(device)\n",
    "            logits = net(local_batch)\n",
    "            loss = my_loss.calc(logits, local_label, local_batch, feas)\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(val_set.dataset)\n",
    "        val_loss.append([epoch, avg_loss])\n",
    "        print(f\"Epoch {epoch} | Validation loss: {avg_loss:.4f} | Learning Rate: {lr:.6f}\")\n",
    "        \n",
    "        # 更新 learning rate\n",
    "        # scheduler.step(avg_loss)\n",
    "        \n",
    "        # 早停機制\n",
    "        # if epoch:\n",
    "        #     if previous - avg_loss < min_delta:\n",
    "        #         tolerance -= 1\n",
    "        #     if tolerance == 0:\n",
    "        #         print(\"Early stopping triggered\")\n",
    "        #         break\n",
    "        previous = avg_loss\n",
    "        net.train()\n",
    "\n",
    "        final_epoch = epoch\n",
    "\n",
    "t1 = time.time()\n",
    "print(f\"Training time: {t1 - t0:.4f}s\")\n",
    "#印出 lr validation loss training loss 圖\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 60 300\n",
      "✅ 輸出內容已儲存到 C:\\Users\\USER\\Desktop\\GNN_OPF_electricity_market-main\\output_loss_fig\\trainingloss_gunplot\\02191320.txt\n",
      "⚠️ Gnuplot Error: Warning: empty y range [0.001:0.001], adjusting to [0.00099:0.00101]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# **設定輸出目錄**\n",
    "output_dir = r'C:\\Users\\USER\\Desktop\\GNN_OPF_electricity_market-main\\output_loss_fig\\trainingloss_gunplot'\n",
    "timestamp = datetime.now().strftime('%m%d%H%M')\n",
    "output_file = os.path.join(output_dir, f'{timestamp}.txt')\n",
    "\n",
    "# **確保目錄存在**\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(len(train_loss), len(val_loss), len(lr_list))  # **確認長度**\n",
    "\n",
    "# **儲存數據到 txt 檔案**\n",
    "with open(output_file, 'w') as file:\n",
    "    for i in range(len(train_loss)):\n",
    "        if i % 5 == 0 and (i // 5) < len(val_loss):  # **確保 val_loss 沒有超出索引**\n",
    "            val = val_loss[i // 5]  # **正確存入數值**\n",
    "        else:\n",
    "            val = \"NaN\"  # **改成 NaN 讓 Gnuplot 忽略**\n",
    "        file.write(f\"{i} {train_loss[i]} {val[1]} {lr_list[i]}\\n\")\n",
    "\n",
    "print(f\"✅ 輸出內容已儲存到 {output_file}\")\n",
    "\n",
    "# **修正 Windows 路徑格式**\n",
    "gnuplot_file = output_file.replace(\"\\\\\", \"/\")  # **確保 Gnuplot 能識別路徑**\n",
    "loss_plot_path = os.path.join(output_dir, f'loss_plot_{timestamp}.png').replace(\"\\\\\", \"/\")\n",
    "lr_plot_path = os.path.join(output_dir, f'lr_plot_{timestamp}.png').replace(\"\\\\\", \"/\")\n",
    "\n",
    "# **定義 gnuplot 腳本**\n",
    "gnuplot_script = f\"\"\"\n",
    "set terminal pngcairo enhanced font 'Arial,12' size 800,600\n",
    "set output \"{loss_plot_path}\"\n",
    "\n",
    "set title \"Training Loss & Validation Loss\"\n",
    "set xlabel \"Step\"\n",
    "set ylabel \"Value\"\n",
    "set grid\n",
    "set key outside\n",
    "\n",
    "# **確保 Validation Loss 顯示**\n",
    "plot \"{gnuplot_file}\" using 1:2 with lines title \"Train Loss\" linecolor rgb \"blue\", \\\n",
    "     \"{gnuplot_file}\" using 1:3 with lines title \"Val Loss\" linecolor rgb \"red\"\n",
    "   \n",
    "\n",
    "# **第二張圖：Learning Rate**\n",
    "set output \"{lr_plot_path}\"\n",
    "set title \"Learning Rate\"\n",
    "set xlabel \"Step\"\n",
    "set ylabel \"Learning Rate\"\n",
    "set grid\n",
    "set key outside\n",
    "\n",
    "# **繪製 Learning Rate**\n",
    "plot \"{gnuplot_file}\" using 1:4 with lines title \"Learning Rate\" linecolor rgb \"green\" lw 2\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# **執行 gnuplot**\n",
    "try:\n",
    "    result = subprocess.run([\"gnuplot\"], input=gnuplot_script, text=True, check=True,\n",
    "                            stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    if result.stderr:\n",
    "        print(\"⚠️ Gnuplot Error:\", result.stderr)\n",
    "    else:\n",
    "        print(f\"✅ Plots generated successfully.\\n📂 Loss plot: {loss_plot_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ Error: Gnuplot is not installed or not in PATH.\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"❌ Gnuplot execution failed:\\n{e.stderr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "id": "AvUTphUSzqzA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Desktop\\GNN_OPF_electricity_market-main\\GNN_OPF_electricity_market-main\\GNN_OPF_electricity_market-main\\data_gen\\118_data\\FCNN_model\\dnn_02191320.pickle\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "timestamp=datetime.now().strftime('%m%d%H%M')\n",
    "path = 'C:\\\\Users\\\\USER\\\\Desktop\\\\GNN_OPF_electricity_market-main\\\\GNN_OPF_electricity_market-main\\\\GNN_OPF_electricity_market-main\\\\data_gen\\\\118_data\\\\FCNN_model\\\\'\n",
    "\n",
    "path = path+'dnn_%s.pickle'%(timestamp)\n",
    "if feas==False: path.replace('feas','')\n",
    "print(path)\n",
    "torch.save(net.state_dict(),path)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "id": "L3M4jmjkscK_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "val_len = len(val_loss)\n",
    "print(val_len)\n",
    "val_plt = np.zeros((2,val_len))\n",
    "for i in range(val_len):\n",
    "  val_plt[0,i] = val_loss[i][0]\n",
    "  val_plt[1,i] = val_loss[i][1]\n",
    "\n",
    "plt.figure()\n",
    "plot_idx = np.arange(np.size(train_loss))\n",
    "plt.plot(plot_idx[5:-1],train_loss[5:-1],lw=2,label='training loss')\n",
    "plt.plot(val_plt[0,1:],val_plt[1,1:],lw=2,label='validation loss')\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "\n",
    "# 在圖的右上角標示學習率\n",
    "plt.text(0.95, 0.95, f'Learning Rate: {lr:.6f}\\nbatch size: {batch_size}', \n",
    "         transform=plt.gca().transAxes, fontsize=10, ha='right', va='top', color='red')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 儲存圖片到指定資料夾（需提供路徑）\n",
    "timestamp=datetime.now().strftime('%m%d%H%M')\n",
    "output_path = r'C:\\Users\\USER\\Desktop\\GNN_OPF_electricity_market-main\\output_loss_fig'\n",
    "output_path = output_path + '/118ac_loss_fig_%s.png' % (timestamp)\n",
    "plt.savefig(output_path, dpi=300, bbox_inches='tight')  # 儲存為 PNG 格式，解析度 300 dpi\n",
    "\n",
    "\n",
    "plt.show(block=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9uGUm4rsco3"
   },
   "source": [
    "# Evaluate the model w/ validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "id": "4u6001UQ2pN3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset size: torch.Size([2000, 708])\n",
      "Number of validation set:  2000\n",
      "torch.Size([2000, 236])\n"
     ]
    }
   ],
   "source": [
    "n_test = np.size(x_test,0)\n",
    "x_test_feed = torch.from_numpy(x_test).float()\n",
    "x_test_feed = x_test_feed#.transpose(1,2)\n",
    "x_test_feed = x_test_feed.to(device)\n",
    "print('Validation dataset size:',x_test_feed.shape)\n",
    "print('Number of validation set: ',n_test)\n",
    "y_pred = net(x_test_feed)\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnNbSGYoXS3J"
   },
   "source": [
    "* Visualization\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKZQaqwJkn3P"
   },
   "source": [
    " - Visualize errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "id": "7-JCo0KmXwm3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 236) (2000, 236)\n"
     ]
    }
   ],
   "source": [
    "y_pred1 = y_pred.cpu().detach()\n",
    "y_pred1 = torch.squeeze(y_pred1,1).numpy()#.transpose()\n",
    "print(y_test.shape,y_pred1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "id": "NOeXUzrd9h9y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 2, 2000) (2000, 236)\n"
     ]
    }
   ],
   "source": [
    "# x=np.reshape(x,(x.shape[0]*x.shape[1],x.shape[2])) # reshape by samples not dim1\n",
    "# y=np.reshape(y,(y.shape[0]*y.shape[1],y.shape[2]))\n",
    "# print(x_pre.shape,y_pre.shape)\n",
    "\n",
    "y_pred_temp = y_pred1.copy().transpose()\n",
    "# y_pred2=np.reshape(y_pred2,(y_pre.shape[0],y_pre.shape[1],n_test))\n",
    "y_pred2=np.zeros([y.shape[0],y.shape[1],n_test])\n",
    "y_pred2[:,0,:]=y_pred_temp[:n_bus,:]\n",
    "y_pred2[:,1,:]=y_pred_temp[n_bus:,:]\n",
    "print(y_pred2.shape,y_pred1.shape)\n",
    "\n",
    "y_test_temp = y_test.copy().transpose()\n",
    "# y_pred2=np.reshape(y_pred2,(y_pre.shape[0],y_pre.shape[1],n_test))\n",
    "y_test2=np.zeros([y.shape[0],y.shape[1],n_test])\n",
    "y_test2[:,0,:]=y_test_temp[:n_bus,:]\n",
    "y_test2[:,1,:]=y_test_temp[n_bus:,:]\n",
    "y_pred1 = y_pred2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "2eVE-t3nl0Cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 2, 2000) (118, 2, 2000)\n"
     ]
    }
   ],
   "source": [
    "# recover the original p.u. scale\n",
    "# vy_deviation) * vy_scale\n",
    "y_pred1[:,1,:] = y_pred1[:,1,:] / vy_scale + vy_deviation\n",
    "y_test2[:,1,:] = y_test2[:,1,:] / vy_scale + vy_deviation\n",
    "print(y_test2.shape,y_pred1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "FUVl5LXeknC4"
   },
   "outputs": [],
   "source": [
    "n_test = np.size(y_test2,2)\n",
    "err_L2 = np.zeros(n_test)\n",
    "err_Linf = np.zeros(n_test)\n",
    "for i in range(n_test):\n",
    "  err_L2[i] = np.linalg.norm(y_test2[:,0,i] - y_pred1[:,0,i]) / np.linalg.norm(y_test2[:,0,i])\n",
    "  err_Linf[i] = np.max(np.abs(y_test2[:,0,i] - y_pred1[:,0,i])) / np.max(np.abs(y_test2[:,0,i]))\n",
    "\n",
    "err_L2_v = np.zeros(n_test)\n",
    "err_Linf_v = np.zeros(n_test)\n",
    "for i in range(n_test):\n",
    "  err_L2_v[i] = np.linalg.norm(y_test2[:,1,i] - y_pred1[:,1,i]) / np.linalg.norm(y_test2[:,1,i])\n",
    "  err_Linf_v[i] = np.max(np.abs(y_test2[:,1,i] - y_pred1[:,1,i])) / np.max(np.abs(y_test2[:,1,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "id": "79xFCVXklkLb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price L2 mean: 0.05384219318293283 L_inf mean: 0.06135265456466414\n",
      "Voltage L2 mean: 0.005790532160917715 L_inf mean: 0.01669858461669706\n"
     ]
    }
   ],
   "source": [
    "err_L2_mean = np.mean(err_L2)\n",
    "err_Linf_mean = np.mean(err_Linf)\n",
    "print('Price L2 mean:', err_L2_mean,'L_inf mean:', err_Linf_mean )\n",
    "err_L2_mean_v = np.mean(err_L2_v)\n",
    "err_Linf_mean_v = np.mean(err_Linf_v)\n",
    "print('Voltage L2 mean:', err_L2_mean_v,'L_inf mean:', err_Linf_mean_v )\n",
    "\n",
    "# fig2 = plt.figure(figsize=(16, 16))\n",
    "# plt.subplot(2, 2, 1)\n",
    "# # plt.hist(np.abs(ga),bins = 10)\n",
    "# plt.plot(err_L2,'bo',markersize=0.5,label = 'L2 error')\n",
    "# plt.plot(err_Linf,'r^',markersize=0.5,label = 'Linf error')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('sample index')\n",
    "# plt.ylabel('error')\n",
    "# plt.title('Normalized sample error')\n",
    "# plt.grid(True)\n",
    "# # error histogram\n",
    "# plt.subplot(2, 2, 2)\n",
    "# plt.hist(err_L2, bins = 50, facecolor='b', alpha=0.75,label = 'L2 error')\n",
    "# plt.hist(err_Linf, bins = 50, facecolor='g', alpha=0.75,label = 'Linf error')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('sample error value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('Histogram of L_2 error')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)\n",
    "\n",
    "# plt.subplot(2, 2, 3)\n",
    "# # plt.hist(np.abs(ga),bins = 10)\n",
    "# plt.plot(err_L2_v,'bo',markersize=0.5,label = 'L2 error')\n",
    "# plt.plot(err_Linf_v,'r^',markersize=0.5,label = 'Linf error')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('sample index')\n",
    "# plt.ylabel('error')\n",
    "# plt.title('Normalized sample error')\n",
    "# plt.grid(True)\n",
    "# # error histogram\n",
    "# plt.subplot(2, 2, 4)\n",
    "# plt.hist(err_L2_v, bins = 50, facecolor='b', alpha=0.75,label = 'L2 error')\n",
    "# plt.hist(err_Linf_v, bins = 50, facecolor='g', alpha=0.75,label = 'Linf error')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('sample error value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('Histogram of L_2 error')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "id": "6sUddh-uGg_p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 2000) (118, 2000)\n",
      "true range: 1.06 0.94\n",
      "predicted range 1.0885538291931152 0.9328948426246644\n"
     ]
    }
   ],
   "source": [
    "print(y_pred1[:,1,:n_test].shape,y_test2[:,1,:n_test].shape)\n",
    "print('true range:',np.max(y_test2[:,1,:n_test]),np.min(y_test2[:,1,:n_test]))\n",
    "print('predicted range',np.max(y_pred1[:,1,:n_test]),np.min(y_pred1[:,1,:n_test]))\n",
    "\n",
    "# fig3 = plt.figure(figsize=(16, 8))\n",
    "# flat_list1 = list(np.concatenate(y_test2[:,1,:n_test]).flat)\n",
    "# flat_list2 = list(np.concatenate(y_pred1[:,1,:n_test]).flat)\n",
    "# plt.hist(flat_list1,bins = 100,label = 'true')\n",
    "\n",
    "# plt.hist(flat_list2,bins = 100,label = 'pred')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "id": "jhfImaPsjKYq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 6, 10000) 10000\n"
     ]
    }
   ],
   "source": [
    "print(x.shape,n_sample)\n",
    "\n",
    "x_new = np.zeros([x.shape[0],x.shape[1],n_sample])\n",
    "for i in range(x.shape[1]):\n",
    "  x_new[:,i,:] = x_total[n_bus*i:n_bus*(i+1),:]\n",
    "\n",
    "y_new = np.zeros([y.shape[0],y.shape[1],n_sample])\n",
    "for i in range(y.shape[1]):\n",
    "  y_new[:,i,:] = y_total[n_bus*i:n_bus*(i+1),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9x7neMNj7n3"
   },
   "source": [
    "# Predict generation using $\\pi$\n",
    "* Using predicted $\\pi$ and find the active constraints in $p_G(i)$\n",
    "* For inactive $p_G(i)$ consider other methods like power flow balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "Kr29K04j2KTN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 10000)\n",
      "<class 'numpy.ndarray'> 118 [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117]\n"
     ]
    }
   ],
   "source": [
    "gen_limit0 = x_new[:,4,:].copy() # lin cost\n",
    "print(gen_limit0.shape)\n",
    "\n",
    "gen_idx = []\n",
    "gen_idx = np.arange(n_bus)\n",
    "# for i in range(n_bus):\n",
    "#   if gen_limit0[i,0] > 0:\n",
    "#     gen_idx.append(i)\n",
    "print(type(gen_idx),len(gen_idx),gen_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "gHmx9lMDXzpM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 2, 10000) (236, 10000)\n"
     ]
    }
   ],
   "source": [
    "n_sample=x_total.shape[-1]\n",
    "x_feed = torch.from_numpy(x_total.T).float()\n",
    "y_pred1=net(x_feed.to(device)).cpu().detach().numpy().T\n",
    "y_pred_temp = y_pred1.copy()\n",
    "y_pred2=np.zeros([y.shape[0],y.shape[1],n_sample])\n",
    "y_pred2[:,0,:]=y_pred_temp[:n_bus,:]\n",
    "y_pred2[:,1,:]=y_pred_temp[n_bus:,:]\n",
    "print(y_pred2.shape,y_pred1.shape)\n",
    "y_pred1 = y_pred2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "id": "GPEHF2L91bGv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.187561035211672e-05\n",
      "0.784000000000006\n",
      "0.010248073656925793\n",
      "0.24221453285700786\n"
     ]
    }
   ],
   "source": [
    "gen_cost0 = x_new[:,4,:].copy()\n",
    "lmp_data = y_new[:,0,:].copy()\n",
    "quadratic_a = x_new[:,5,:].copy()\n",
    "profit_pred = y_pred1[:,0,:] - gen_cost0\n",
    "print(np.min(np.abs(profit_pred)))\n",
    "profit_true = lmp_data - gen_cost0\n",
    "print(np.min(np.abs(profit_true)))\n",
    "profit_pred=(y_pred1[:,0,:]-gen_cost0)/(quadratic_a+1e-10)/2\n",
    "profit_true=(lmp_data-gen_cost0)/(quadratic_a+1e-10)/2\n",
    "print(np.min(np.abs(profit_pred)))\n",
    "print(np.min(np.abs(profit_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "uiHWtU5OLc1x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 10000) (118, 10000)\n",
      "0.0454726228062265\n"
     ]
    }
   ],
   "source": [
    "print(profit_pred.shape,profit_true.shape)\n",
    "profit_err = profit_true - profit_pred\n",
    "profit_err_l2 = np.zeros([n_sample,1])\n",
    "\n",
    "for i in range(n_sample):\n",
    "  profit_err_l2[i] = np.linalg.norm(profit_err[:,i])/np.linalg.norm(profit_true[:,i])\n",
    "print(np.mean(profit_err_l2))\n",
    "\n",
    "# fig5 = plt.figure(figsize=(16, 8))\n",
    "# # error histogram\n",
    "# plt.hist(profit_err_l2, bins = 50, facecolor='b', alpha=0.75,label = 'L2 error')\n",
    "# # plt.hist(err_linf_new, bins = 50, facecolor='g', alpha=0.75,label = 'Linf error')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('sample error value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('Histogram of L_2 error')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "id": "ZbHchRQd_g8-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1180000,)\n",
      "-895.0281978819572 -2.9391182643149665\n"
     ]
    }
   ],
   "source": [
    "p_pred_sort = np.reshape(profit_pred,n_bus*n_sample)\n",
    "p_true_sort = np.reshape(profit_true,n_bus*n_sample)\n",
    "print(p_pred_sort.shape)\n",
    "print(np.min(p_pred_sort),np.min(p_true_sort))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "id": "TYsSdNGp-OLP"
   },
   "outputs": [],
   "source": [
    "# fig2 = plt.figure(figsize=(8, 8))\n",
    "# plt.hist(p_pred_sort, bins = 50, facecolor='b', alpha=0.75,label = 'pred. profit')\n",
    "# plt.hist(p_true_sort, bins = 50, facecolor='g', alpha=0.75,label = 'true profit')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('profit histogram')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "id": "1J9j5tT9p_f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3028205.514319858 2764601.835122853\n",
      "3028205.514319858 43260060.56350001 3028205.514319858\n"
     ]
    }
   ],
   "source": [
    "# x = [load, gen_cost, gen_lim]\n",
    "binary_thres_true = 1e-5\n",
    "binary_thres = x_new[:,0,:].copy() # upper\n",
    "binary_thres_lo = x_new[:,1,:].copy() # lower\n",
    "gen_pred_binary_full = np.zeros((n_bus,n_sample))\n",
    "gen_true_binary_full = np.zeros((n_bus,n_sample))\n",
    "\n",
    "for i in range(n_sample):\n",
    "  for j in range(len(gen_idx)):\n",
    "    # predicted generator limit\n",
    "    if profit_pred[gen_idx[j],i] > binary_thres[gen_idx[j],i]:\n",
    "      gen_pred_binary_full[gen_idx[j],i] = binary_thres[gen_idx[j],i]\n",
    "    elif profit_pred[gen_idx[j],i] < binary_thres_lo[gen_idx[j],i]:\n",
    "      gen_pred_binary_full[gen_idx[j],i] = binary_thres_lo[gen_idx[j],i]\n",
    "    else:\n",
    "      gen_pred_binary_full[gen_idx[j],i] = profit_pred[gen_idx[j],i]\n",
    "    # true generator limit\n",
    "    if profit_true[gen_idx[j],i] > binary_thres[gen_idx[j],i]:\n",
    "      gen_true_binary_full[gen_idx[j],i] = binary_thres[gen_idx[j],i]\n",
    "    elif profit_true[gen_idx[j],i] < binary_thres_lo[gen_idx[j],i]:\n",
    "      gen_true_binary_full[gen_idx[j],i] = binary_thres_lo[gen_idx[j],i]\n",
    "    else:\n",
    "      gen_true_binary_full[gen_idx[j],i] = profit_true[gen_idx[j],i]\n",
    "\n",
    "gen_inj=gen_pred_binary_full\n",
    "gen_inj_true=gen_true_binary_full\n",
    "# nodal injection\n",
    "load0 = -x_new[:,1,:].copy() # load file\n",
    "p_inj = gen_inj #- load0\n",
    "p_inj_true = gen_inj_true #- load0\n",
    "print(np.sum(p_inj),np.sum(gen_inj_true))\n",
    "print(np.sum(p_inj),np.sum(load0),np.sum(gen_inj))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WAgqdRPjAONm"
   },
   "source": [
    "## injection accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "J46pLAw2AQor"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 10000) (118, 10000)\n",
      "mean p_inj l2 err: 0.10147515849364633\n"
     ]
    }
   ],
   "source": [
    "print(p_inj_true.shape,p_inj.shape)\n",
    "p_inj_true_sort = np.reshape(p_inj_true,n_bus*n_sample)\n",
    "p_inj_sort = np.reshape(p_inj,n_bus*n_sample)\n",
    "p_err = np.zeros(n_sample)\n",
    "for i in range(n_sample):\n",
    "  p_err[i] = np.linalg.norm(p_inj_true[:,i]-p_inj[:,i]) / np.linalg.norm(p_inj_true[:,i])\n",
    "print('mean p_inj l2 err:',np.mean(p_err))\n",
    "# fig3 = plt.figure(figsize=(16, 8))\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.hist(p_inj_sort, bins = 50, facecolor='b', alpha=0.75,label = 'pred. injection')\n",
    "# plt.hist(p_inj_true_sort, bins = 50, facecolor='g', alpha=0.75,label = 'true injection')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('injection histogram')\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.hist(p_err, bins = 50, facecolor='b', alpha=0.75,label = 'injection err')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('error histogram')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)s_binary\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DaN7u4xeGIom"
   },
   "source": [
    "* Calculate flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "id": "YT4sgn_n79MI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(186, 1) (186, 10000) (186, 10000)\n",
      "15847 13334\n",
      "0.00851989247311828 0.007168817204301075\n",
      "186 10000 (186, 10000)\n"
     ]
    }
   ],
   "source": [
    "filename=root+'118ac_fmax.txt'\n",
    "f_max1=pd.read_table(filename,sep=',',header=None).to_numpy() # flow limit\n",
    "\n",
    "n_line = np.size(S_isf,0)\n",
    "flow_est = np.zeros((n_line,n_sample))\n",
    "flow_est0 = np.zeros((n_line,n_sample))\n",
    "\n",
    "f_binary = np.zeros((n_line,n_sample))\n",
    "f_binary0 = np.zeros((n_line,n_sample))\n",
    "\n",
    "# for i in range(n_sample):\n",
    "flow_est = np.dot(S_isf,p_inj)\n",
    "flow_est0 = np.dot(S_isf,p_inj_true)\n",
    "# f_max\n",
    "# f_max_numpy = f_max.cpu().detach().numpy()\n",
    "f_max_numpy = f_max1.copy()\n",
    "f_binary = (np.abs(flow_est)-f_max_numpy > 0)\n",
    "f_binary0 = (np.abs(flow_est0)-f_max_numpy > 0)\n",
    "\n",
    "print(f_max_numpy.shape,flow_est.shape,flow_est0.shape)\n",
    "f_tot_sample = n_line * n_sample\n",
    "print(np.sum(f_binary),np.sum(f_binary0))\n",
    "print(np.sum(f_binary)/f_tot_sample,np.sum(f_binary0)/f_tot_sample)\n",
    "print(n_line,n_sample,flow_est.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "id": "VyvDpKhyQj0M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156.83468677741973 39.64401431411352\n",
      "1.0455645785161316 0.2642934287607568\n"
     ]
    }
   ],
   "source": [
    "# soft threshold\n",
    "f_err_est = np.abs(flow_est)-f_max_numpy\n",
    "f_err_true = np.abs(flow_est0)-f_max_numpy\n",
    "\n",
    "f_err_est = np.maximum(np.abs(flow_est)-f_max_numpy,0) # identify violations\n",
    "f_err_true = np.maximum(np.abs(flow_est0)-f_max_numpy,0)\n",
    "\n",
    "print(np.max(f_err_est),np.max(f_err_true))\n",
    "print(np.max(f_err_est/f_max_numpy),np.max(f_err_true/f_max_numpy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "id": "7iuX7tN2a2Cp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12024 10451\n",
      "0.006464516129032258 0.005618817204301075\n"
     ]
    }
   ],
   "source": [
    "f_binary_soft = (np.abs(flow_est)-f_max_numpy > 0.1*(f_max_numpy))\n",
    "f_binary0_soft = (np.abs(flow_est0)-f_max_numpy > 0.1*(f_max_numpy))\n",
    "print(np.sum(f_binary_soft),np.sum(f_binary0_soft))\n",
    "print(np.sum(f_binary_soft)/f_tot_sample,np.sum(f_binary0_soft)/f_tot_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "id": "SYl9pxnOUUQF"
   },
   "outputs": [],
   "source": [
    "f_pred_sort = np.reshape(f_err_est/f_max_numpy,n_line*n_sample)\n",
    "f_true_sort = np.reshape(f_err_true/f_max_numpy,n_line*n_sample)\n",
    "\n",
    "# fig2 = plt.figure(figsize=(16, 8))\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.hist(f_pred_sort, bins = 10, facecolor='b', alpha=0.75,label = 'pred. f')\n",
    "# plt.hist(f_true_sort, bins = 10, facecolor='g', alpha=0.75,label = 'true f')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('percentage')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('flow violation level histogram')\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.hist(f_true_sort, bins = 10, facecolor='g', alpha=0.75,label = 'true f')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('percentage')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('flow violation level histogram')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "id": "YglgTpWVRLri"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sample pred: 7\n",
      "max line pred: 8753\n",
      "max sample true: 3\n",
      "max line true: 10000\n"
     ]
    }
   ],
   "source": [
    "f_line = np.sum(f_binary,0)\n",
    "f_samp = np.sum(f_binary,1)\n",
    "print('max sample pred:',np.max(f_line))\n",
    "print('max line pred:',np.max(f_samp))\n",
    "\n",
    "f_line0  = np.sum(f_binary0,0)\n",
    "f_samp0 = np.sum(f_binary0,1)\n",
    "print('max sample true:',np.max(f_line0))\n",
    "print('max line true:',np.max(f_samp0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZQnarHmPl35"
   },
   "source": [
    "# Check objective optimality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "id": "BZjhp-CgQaDz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07883234263644179\n"
     ]
    }
   ],
   "source": [
    "gen_cost_pred = np.zeros((n_bus,n_sample))\n",
    "gen_cost_true = np.zeros((n_bus,n_sample))\n",
    "objective_err = np.zeros(n_sample)\n",
    "\n",
    "gen_cost_pred = np.multiply(np.multiply(p_inj,p_inj),quadratic_a) + np.multiply(p_inj,gen_cost0)\n",
    "gen_cost_true = np.multiply(np.multiply(p_inj_true,p_inj_true),quadratic_a) + np.multiply(p_inj_true,gen_cost0)\n",
    "\n",
    "objective_err = np.sum(np.abs(gen_cost_true-gen_cost_pred),axis=0) / np.sum(gen_cost_true,axis=0)\n",
    "print(np.mean(objective_err))\n",
    "\n",
    "# fig6 = plt.figure(figsize=(16, 8))\n",
    "# # error histogram\n",
    "# plt.hist(objective_err, bins = 50, facecolor='b', alpha=0.75,label = 'L2 error')\n",
    "# # plt.hist(err_linf_new, bins = 50, facecolor='g', alpha=0.75,label = 'Linf error')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('sample error value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('Histogram of L_2 error')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdrsAWpYo0-w"
   },
   "source": [
    "## injection accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "id": "mArjSN-So0-x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 10000) (118, 10000)\n",
      "mean p_inj l2 err: 0.10147515849364633\n"
     ]
    }
   ],
   "source": [
    "print(p_inj_true.shape,p_inj.shape)\n",
    "\n",
    "p_inj_true_sort = np.reshape(p_inj_true,n_bus*n_sample)\n",
    "p_inj_sort = np.reshape(p_inj,n_bus*n_sample)\n",
    "\n",
    "p_err = np.zeros(n_sample)\n",
    "for i in range(n_sample):\n",
    "  p_err[i] = np.linalg.norm(p_inj_true[:,i]-p_inj[:,i]) / np.linalg.norm(p_inj_true[:,i])\n",
    "\n",
    "print('mean p_inj l2 err:',np.mean(p_err))\n",
    "# fig3 = plt.figure(figsize=(16, 8))\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.hist(p_inj_sort, bins = 50, facecolor='b', alpha=0.75,label = 'pred. injection')\n",
    "# plt.hist(p_inj_true_sort, bins = 50, facecolor='g', alpha=0.75,label = 'true injection')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('injection histogram')s_binary\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.hist(p_err, bins = 50, facecolor='b', alpha=0.75,label = 'injection err')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('error histogram')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujU84tOSpqsy"
   },
   "source": [
    "# Test AC feasibility\n",
    "* P in actual value, V in p.u.\n",
    "* Use P to recover $\\theta$, or solve $\\theta$ and Q for PF\n",
    "$$ Q_m = V_m \\sum_{n=1}^N V_n \\left(G_{mn}\\sin\\theta_{mn} - B_{mn}\\cos\\theta_{mn} \\right) $$\n",
    "calculate $Q_{mn}$ directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "id": "Of6mEXF4puDN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 118) (118, 118)\n",
      "(117, 117) (118, 10000) (118, 10000)\n",
      "(118, 10000) (118, 10000)\n"
     ]
    }
   ],
   "source": [
    "# Bbus and B_r inverse\n",
    "filename1 = root+'ieee118_Bbus.txt'\n",
    "Bbus=pd.read_table(filename1,sep=',',header=None).to_numpy()\n",
    "B_r = np.delete(Bbus,68,axis=0)\n",
    "B_r = np.delete(B_r,68,axis=1)\n",
    "Br_inv = np.linalg.inv(B_r)\n",
    "\n",
    "# Y = G + jB\n",
    "filename1 = root+'ieee118_Gmat.txt'\n",
    "G_mat=pd.read_table(filename1,sep=',',header=None).to_numpy()\n",
    "filename1 = root+'ieee118_Bmat.txt'\n",
    "B_mat=pd.read_table(filename1,sep=',',header=None).to_numpy()\n",
    "print(G_mat.shape,B_mat.shape)\n",
    "\n",
    "# line parameters\n",
    "filename1 = root+'ieee118_lineloc.txt'\n",
    "line_loc = pd.read_table(filename1,sep=',',header=None).to_numpy()\n",
    "\n",
    "# load line params\n",
    "filename1 = root+'ieee118_lineparams.txt'\n",
    "line_params = pd.read_table(filename1,sep=',',header=None).to_numpy()\n",
    "R_line = line_params[:,0].copy()\n",
    "X_line = line_params[:,1].copy()\n",
    "B_shunt = line_params[:,2].copy()\n",
    "Z_line = R_line + 1j * X_line \n",
    "Y_line = 1 / Z_line\n",
    "G_line = np.real(Y_line)\n",
    "B_line = np.imag(Y_line)\n",
    "# P_inj w/out reference bus in p.u.\n",
    "p_inj_r = np.delete(p_inj,68,axis=0) / 100\n",
    "p_inj_true_r = np.delete(p_inj_true,68,axis=0) / 100\n",
    "p_inj_pu = p_inj / 100\n",
    "p_inj_true_pu = p_inj_true / 100\n",
    "print(Br_inv.shape,p_inj.shape,p_inj_true.shape)#p_inj_true\n",
    "\n",
    "theta0 = np.matmul(Br_inv,p_inj_r)\n",
    "theta_true0 = np.matmul(Br_inv,p_inj_true_r)\n",
    "theta = np.insert(theta0,68,0,axis = 0)\n",
    "theta_true = np.insert(theta_true0,68,0,axis = 0)\n",
    "print(theta.shape,theta_true.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4699030721830441 -0.9500264961741836\n",
      "2.7803011534120627 -9.166735486002148\n"
     ]
    }
   ],
   "source": [
    "print(np.max(theta),np.min(theta))\n",
    "math.sin(math.pi/6)\n",
    "print(G_line[0],B_line[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(186, 10000)\n",
      "1.0972457504272461 0.9324821829795837 (118, 10000)\n"
     ]
    }
   ],
   "source": [
    "# Calculate real and reactive flow\n",
    "f_p = np.zeros((n_line,n_sample))\n",
    "f_q = np.zeros((n_line,n_sample))\n",
    "fji_p = np.zeros((n_line,n_sample))\n",
    "fji_q = np.zeros((n_line,n_sample))\n",
    "print(f_q.shape)\n",
    "\n",
    "v_pred = y_pred1[:,1,:].copy()\n",
    "v_pred = v_pred / vy_scale + vy_deviation\n",
    "print(np.max(v_pred),np.min(v_pred),v_pred.shape)\n",
    "\n",
    "theta1 = theta[line_loc[:,0]-1,:]\n",
    "theta2 = theta[line_loc[:,1]-1,:]\n",
    "V1 = v_pred[line_loc[:,0]-1,:]\n",
    "V2 = v_pred[line_loc[:,1]-1,:] \n",
    "f_p=(a*G_line*(V1*V1).T)-a*((V1*V2).T)*(G_line*np.cos(theta1-theta2).T+B_line*np.sin(theta1-theta2).T)\n",
    "f_p=f_p.T\n",
    "f_q=-a*(V1.T)*(a*V1.T)*(B_line+B_shunt/2)+a*((V1*V2).T)*(B_line*np.cos(theta1-theta2).T-G_line*np.sin(theta1-theta2).T)\n",
    "f_q=f_q.T\n",
    "\n",
    "theta1 = theta[line_loc[:,1]-1,:]\n",
    "theta2 = theta[line_loc[:,0]-1,:]\n",
    "V1 = v_pred[line_loc[:,1]-1,:]\n",
    "V2 = v_pred[line_loc[:,0]-1,:]\n",
    "fji_p=(a*G_line*(V1*V1).T)-a*((V1*V2).T)*(G_line*np.cos(theta1-theta2).T+B_line*np.sin(theta1-theta2).T)\n",
    "fji_p=fji_p.T\n",
    "fji_q=-a*(V1.T)*(a*V1.T)*(B_line+B_shunt/2)+a*((V1*V2).T)*(B_line*np.cos(theta1-theta2).T-G_line*np.sin(theta1-theta2).T)\n",
    "fji_q=fji_q.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "id": "gKfcrbTSVMeO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0260716853244567 -1.363453276319376\n",
      "16.097348184395766 151.0\n"
     ]
    }
   ],
   "source": [
    "s_pred = np.sqrt(f_p*f_p+f_q*f_q)*100\n",
    "sji_pred = np.sqrt(fji_p*fji_p+fji_q*fji_q)*100\n",
    "print(np.max(f_q),np.min(f_q))\n",
    "flow_est.shape\n",
    "print(np.mean(s_pred[0,:]),np.mean(f_max_numpy[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "id": "YO4__LJ2brLu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20783\n",
      "hard violation rate: 0.011173655913978495\n",
      "13771\n",
      "0.007403763440860215\n"
     ]
    }
   ],
   "source": [
    "sij_binary = (np.abs(s_pred)-f_max_numpy[:n_line] > 0)\n",
    "sji_binary = (np.abs(sji_pred)-f_max_numpy[:n_line] > 0)\n",
    "s_binary = np.maximum(sij_binary,sji_binary)\n",
    "print(np.sum(s_binary))#,np.sum(f_binary0))\n",
    "print('hard violation rate:',np.sum(s_binary)/n_sample/n_line)#,np.sum(f_binary0)/f_tot_sample)\n",
    "s_binary_soft = (np.abs(s_pred)-f_max_numpy[:n_line] > 0.1*(f_max_numpy[:n_line]))\n",
    "print(np.sum(s_binary_soft))#,np.sum(f_binary0_soft))\n",
    "print(np.sum(s_binary_soft)/n_sample/n_line)#,np.sum(f_binary0_soft)/f_tot_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "id": "Ty0WpBdfJwMR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S violation level:\n",
      "hard: 0.011173655913978495\n",
      "mean: 0.0028862066419056004\n",
      "median: 0.0\n",
      "max: 1.131505783366941\n",
      "std: 0.03572942744232544\n",
      "p99: 0.026734476417562453\n",
      "f violation level:\n",
      "hard: 0.00851989247311828 0.007168817204301075\n",
      "mean: 0.0026537335546586577\n",
      "median: 0.0\n",
      "max: 1.0455645785161316\n",
      "std: 0.0363280536764233\n",
      "p99: 0.0\n"
     ]
    }
   ],
   "source": [
    "# violation level\n",
    "sij_violation = np.abs(s_pred)-f_max_numpy[:n_line] #/ f_max_numpy\n",
    "sij_violation_level = np.maximum(sij_violation,0)\n",
    "sji_violation = np.abs(sji_pred)-f_max_numpy[:n_line] #/ f_max_numpy\n",
    "sji_violation_level = np.maximum(sji_violation,0)\n",
    "s_violation_level = np.maximum(sij_violation_level,sji_violation_level)\n",
    "s_violation_level = np.divide(s_violation_level,f_max_numpy[:n_line])\n",
    "s_vio_lvl = np.reshape(s_violation_level,n_line*n_sample)\n",
    "\n",
    "print('S violation level:')\n",
    "print('hard:',np.sum(s_binary)/f_tot_sample)\n",
    "print('mean:',np.mean(s_vio_lvl))\n",
    "print('median:',np.median(s_vio_lvl))\n",
    "print('max:',np.max(s_vio_lvl))\n",
    "print('std:',np.std(s_vio_lvl))\n",
    "print('p99:',np.percentile(s_vio_lvl,99))\n",
    "\n",
    "f_violation = np.abs(flow_est)-f_max_numpy #/ f_max_numpy\n",
    "f_violation_level = np.maximum(f_violation,0)\n",
    "f_violation_level = np.divide(f_violation_level,f_max_numpy)\n",
    "f_vio_lvl = np.reshape(f_violation_level,n_line*n_sample)\n",
    "\n",
    "print('f violation level:')\n",
    "print('hard:',np.sum(f_binary)/f_tot_sample,np.sum(f_binary0)/f_tot_sample)\n",
    "print('mean:',np.mean(f_vio_lvl))\n",
    "print('median:',np.median(f_vio_lvl))\n",
    "print('max:',np.max(f_vio_lvl))\n",
    "print('std:',np.std(f_vio_lvl))\n",
    "print('p99:',np.percentile(f_vio_lvl,99))\n",
    "\n",
    "# fig4 = plt.figure(figsize=(6,4))\n",
    "# plt.hist(s_vio_lvl, bins = 50, facecolor='b', alpha=0.75,label = 's violation')\n",
    "# plt.hist(f_vio_lvl, bins = 50, facecolor='r', alpha=0.75,label = 'f violation')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('violation level')\n",
    "# plt.ylabel('frequency')\n",
    "# # plt.title('injection histogram')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "id": "kWXEpj-ryjbJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price L2 mean: 0.05384219318293283 L_inf mean: 0.06135265456466414\n",
      "std: 0.03605278680649565\n",
      "Voltage L2 mean: 0.005790532160917715 L_inf mean: 0.01669858461669706\n",
      "std: 0.0035244716531280065\n"
     ]
    }
   ],
   "source": [
    "# err_L2_mean = np.mean(err_L2)\n",
    "# err_Linf_mean = np.mean(err_Linf)\n",
    "print('Price L2 mean:', err_L2_mean,'L_inf mean:', err_Linf_mean )\n",
    "print('std:',np.std(err_L2))\n",
    "# err_L2_mean_v = np.mean(err_L2_v)\n",
    "# err_Linf_mean_v = np.mean(err_Linf_v)\n",
    "print('Voltage L2 mean:', err_L2_mean_v,'L_inf mean:', err_Linf_mean_v )\n",
    "print('std:',np.std(err_L2_v))\n",
    "\n",
    "params = (sum(temp.numel() for temp in net.parameters() if temp.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "輸出內容已儲存到 C:\\Users\\USER\\Desktop\\GNN_OPF_electricity_market-main\\output_loss_fig/118ac_output_02191320.txt\n"
     ]
    }
   ],
   "source": [
    "import os   \n",
    "# 建立輸出內容字串\n",
    "output_content = f\"\"\"\n",
    "date: {timestamp}\n",
    "final_epoch: {final_epoch}\n",
    "training time: {t1 - t0:.4e}s\n",
    "\n",
    "factor: {factor}\n",
    "patience: {patience}\n",
    "\n",
    "start learning rate: {lr:.4e}\n",
    "batch size: {batch_size}\n",
    "params number: {params:.4e}\n",
    "\n",
    "Price L2 mean: {err_L2_mean:.4e} \n",
    "Price std: {np.std(err_L2):.4e}\n",
    "\n",
    "Voltage L2 mean: {err_L2_mean_v:.4e} \n",
    "Voltage std: {np.std(err_L2_v):.4e}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# 設定儲存路徑和檔名\n",
    "output_dir = r'C:\\Users\\USER\\Desktop\\GNN_OPF_electricity_market-main\\output_loss_fig'\n",
    "timestamp = datetime.now().strftime('%m%d%H%M')\n",
    "output_file = f'{output_dir}/118ac_output_{timestamp}.txt'\n",
    "\n",
    "# 確保目錄存在\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 將內容寫入 txt 檔案\n",
    "with open(output_file, 'w') as file:\n",
    "    file.write(output_content)\n",
    "\n",
    "print(f\"輸出內容已儲存到 {output_file}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "118ac_feasdnn0417.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "AC_OPF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
