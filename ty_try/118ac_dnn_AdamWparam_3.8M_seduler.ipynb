{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JKLovDeXoCCP"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import math\n",
    "from datetime import datetime\n",
    "root=''\n",
    "# try:\n",
    "#   from google.colab import drive\n",
    "#   drive.mount('/content/drive')\n",
    "#   root='./drive/MyDrive/gnn/data/'\n",
    "# except:\n",
    "#   pass\n",
    "# device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# print(torch.cuda.get_device_name(0))\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# 檢查是否有可用的 GPU，否則使用 CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([186, 118]) torch.Size([186, 1]) tensor(72)\n",
      "1654.8 -332.4 949.67 0.94\n",
      "voltage range(scaled): 3.9999999999999925 16.000000000000004\n",
      "price range old: 11.547 949.67\n",
      "voltage range old: 3.9999999999999925 16.000000000000004\n",
      "price range new: 11.547 949.67\n",
      "voltage range new: 3.9999999999999925 16.000000000000004\n",
      "(118, 6, 10000) (118, 2, 10000)\n"
     ]
    }
   ],
   "source": [
    "root = 'C:/Users/USER/Desktop/GNN_OPF_electricity_market-main/GNN_OPF_electricity_market-main/GNN_OPF_electricity_market-main/data_gen/118_data/'\n",
    "# C:\\Users\\USER\\Desktop\\GNN_OPF_electricity_market-main\\GNN_OPF_electricity_market-main\\GNN_OPF_electricity_market-main\\data_gen\\118_data\\118dc_quad_ISF.txt\n",
    "filename = root + '118dc_quad_ISF.txt'\n",
    "\n",
    "S_isf=pd.read_table(filename,sep=',',header=None).to_numpy() # ISF matrix\n",
    "filename=root+'118ac_fmax.txt'\n",
    "f_max=pd.read_table(filename,sep=',',header=None).to_numpy() # flow limit\n",
    "n_line = np.size(S_isf,0)\n",
    "S = torch.from_numpy(S_isf).to(device) # ISF\n",
    "f_max = torch.from_numpy(f_max).to(device) # flow limit\n",
    "print(S.shape,f_max.shape,torch.min(f_max))\n",
    "\n",
    "x=np.load(root+'ac118_p10_x_v.npy')\n",
    "y=np.load(root+'ac118_p10_y_v.npy')\n",
    "W=np.load(root+'ac118_p10_w.npy')\n",
    "print(np.max(x),np.min(x),np.max(y),np.min(y))\n",
    "\n",
    "# scaling on voltage\n",
    "vy_deviation = 0.9\n",
    "vy_scale = 100\n",
    "y[:,1,:] = (y[:,1,:] - vy_deviation) * vy_scale\n",
    "print('voltage range(scaled):',np.min(y[:,1,:]),np.max(y[:,1,:]))\n",
    "\n",
    "# scaling on price\n",
    "pi_deviation = 0\n",
    "y[:,0,:] = y[:,0,:] + pi_deviation\n",
    "# filter out extreme points in price\n",
    "y_sort_arg = np.argsort(np.amax(np.abs(y[:,0,:]),axis=0)) # max extreme\n",
    "y_sort_arg1 = np.argsort(np.amin(y[:,0,:],axis=0),axis=0) # min extreme\n",
    "\n",
    "del_idx0 = []\n",
    "del_num = 0\n",
    "for i in range(del_num):\n",
    "  del_idx0.append(y_sort_arg[-i])\n",
    "  del_idx0.append(y_sort_arg1[i])\n",
    "# print(del_idx0)\n",
    "del_idx = [] # keep only non-repeated\n",
    "[del_idx.append(x) for x in del_idx0 if x not in del_idx]\n",
    "del_idx = np.sort(del_idx)\n",
    "# delete extreme points\n",
    "print('price range old:',np.min(y[:,0,:]),np.max(y[:,0,:]))\n",
    "print('voltage range old:',np.min(y[:,1,:]),np.max(y[:,1,:]))\n",
    "# x = np.delete(x, del_idx, axis=2)\n",
    "# y = np.delete(y, del_idx, axis=2)\n",
    "print('price range new:',np.min(y[:,0,:]),np.max(y[:,0,:]))\n",
    "print('voltage range new:',np.min(y[:,1,:]),np.max(y[:,1,:]))\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Hj9_eLfoWQY3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([186, 118]) torch.Size([186, 1]) tensor(72)\n",
      "1654.8 -332.4 949.67 0.94\n",
      "voltage range(scaled): 3.9999999999999925 16.000000000000004\n",
      "price range old: 11.547 949.67\n",
      "voltage range old: 3.9999999999999925 16.000000000000004\n",
      "price range new: 11.547 949.67\n",
      "voltage range new: 3.9999999999999925 16.000000000000004\n",
      "(118, 6, 10000) (118, 2, 10000)\n"
     ]
    }
   ],
   "source": [
    "root = 'C:/Users/USER/Desktop/GNN_OPF_electricity_market-main/GNN_OPF_electricity_market-main/GNN_OPF_electricity_market-main/data_gen/118_data/'\n",
    "# C:\\Users\\USER\\Desktop\\GNN_OPF_electricity_market-main\\GNN_OPF_electricity_market-main\\GNN_OPF_electricity_market-main\\data_gen\\118_data\\118dc_quad_ISF.txt\n",
    "filename = root + '118dc_quad_ISF.txt'\n",
    "\n",
    "S_isf=pd.read_table(filename,sep=',',header=None).to_numpy() # ISF matrix\n",
    "filename=root+'118ac_fmax.txt'\n",
    "f_max=pd.read_table(filename,sep=',',header=None).to_numpy() # flow limit\n",
    "n_line = np.size(S_isf,0)\n",
    "S = torch.from_numpy(S_isf).to(device) # ISF\n",
    "f_max = torch.from_numpy(f_max).to(device) # flow limit\n",
    "print(S.shape,f_max.shape,torch.min(f_max))\n",
    "\n",
    "x=np.load(root+'ac118_p10_x_v.npy')\n",
    "y=np.load(root+'ac118_p10_y_v.npy')\n",
    "W=np.load(root+'ac118_p10_w.npy')\n",
    "print(np.max(x),np.min(x),np.max(y),np.min(y))\n",
    "\n",
    "# scaling on voltage\n",
    "vy_deviation = 0.9\n",
    "vy_scale = 100\n",
    "y[:,1,:] = (y[:,1,:] - vy_deviation) * vy_scale\n",
    "print('voltage range(scaled):',np.min(y[:,1,:]),np.max(y[:,1,:]))\n",
    "\n",
    "# scaling on price\n",
    "pi_deviation = 0\n",
    "y[:,0,:] = y[:,0,:] + pi_deviation\n",
    "# filter out extreme points in price\n",
    "y_sort_arg = np.argsort(np.amax(np.abs(y[:,0,:]),axis=0)) # max extreme\n",
    "y_sort_arg1 = np.argsort(np.amin(y[:,0,:],axis=0),axis=0) # min extreme\n",
    "\n",
    "del_idx0 = []\n",
    "del_num = 0\n",
    "for i in range(del_num):\n",
    "  del_idx0.append(y_sort_arg[-i])\n",
    "  del_idx0.append(y_sort_arg1[i])\n",
    "# print(del_idx0)\n",
    "del_idx = [] # keep only non-repeated\n",
    "[del_idx.append(x) for x in del_idx0 if x not in del_idx]\n",
    "del_idx = np.sort(del_idx)\n",
    "# delete extreme points\n",
    "print('price range old:',np.min(y[:,0,:]),np.max(y[:,0,:]))\n",
    "print('voltage range old:',np.min(y[:,1,:]),np.max(y[:,1,:]))\n",
    "# x = np.delete(x, del_idx, axis=2)\n",
    "# y = np.delete(y, del_idx, axis=2)\n",
    "print('price range new:',np.min(y[:,0,:]),np.max(y[:,0,:]))\n",
    "print('voltage range new:',np.min(y[:,1,:]),np.max(y[:,1,:]))\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "YcR42RBbdZqZ"
   },
   "outputs": [],
   "source": [
    "n_sample=y.shape[-1]\n",
    "n_bus=y.shape[0]\n",
    "x_total=x.transpose((1,0,2)).reshape(-1,x.shape[-1])\n",
    "y_total=y.transpose((1,0,2)).reshape(-1,y.shape[-1])\n",
    "x_train,x_test,y_train,y_test=train_test_split(x_total.T,y_total.T,test_size=0.2)\n",
    "\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        self.x=torch.from_numpy(x).float()\n",
    "        self.y=torch.from_numpy(y).float()\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "          idx=idx.tolist()\n",
    "        return self.x[idx],self.y[idx]\n",
    "    \n",
    "batch_size=512\n",
    "#512\n",
    "params={'batch_size': batch_size,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 0}\n",
    "train=Dataset(x_train,y_train)\n",
    "train_set=torch.utils.data.DataLoader(train,**params)\n",
    "val=Dataset(x_test,y_test)\n",
    "val_set=torch.utils.data.DataLoader(val,**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 708])\n",
      "Train Mean: 6.445371627807617\n",
      "Train Std: 3.6904561519622803\n",
      "Validation Mean: 6.4100422859191895\n",
      "Validation Std: 3.681206226348877\n"
     ]
    }
   ],
   "source": [
    "# 資料分析\n",
    "# 從資料集中建立 DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 取第一個 batch 進行分析\n",
    "train_batch_x, _ = next(iter(train_loader))\n",
    "val_batch_x, _ = next(iter(val_loader))\n",
    "\n",
    "# 計算均值和標準差\n",
    "print(train_batch_x.shape)\n",
    "train_mean, train_std = train_batch_x.mean(dim=0), train_batch_x.std(dim=0)\n",
    "val_mean, val_std = val_batch_x.mean(dim=0), val_batch_x.std(dim=0)\n",
    "\n",
    "mean_value_train = train_mean.mean().item()\n",
    "print('Train Mean:', mean_value_train)\n",
    "std_value_train = train_std.mean().item()\n",
    "print('Train Std:', std_value_train)\n",
    "mean_value_val = val_mean.mean().item()\n",
    "print('Validation Mean:', mean_value_val)\n",
    "std_value_val = val_std.mean().item()\n",
    "print('Validation Std:', std_value_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "JN9gN9BnCNim"
   },
   "outputs": [],
   "source": [
    "# fig2 = plt.figure(figsize=(8,4))\n",
    "# flat_list = list(np.concatenate(y[:,n_sample:]).flat)\n",
    "# flat_list3 = list(np.concatenate(y[:,:n_sample]).flat)\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.hist(flat_list,bins = 100,label = 'voltage')\n",
    "# plt.subplot(1,2,2)\n",
    "# # plt.hist(flat_list3,range=[-2000, 2000],bins = 100,label = 'price')\n",
    "# plt.hist(flat_list3,bins = 100,label = 'price')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "iOyWpG_4yCtz"
   },
   "outputs": [],
   "source": [
    "\n",
    "class dnn(torch.nn.Module):\n",
    "  def __init__(self,shape):\n",
    "    super(dnn,self).__init__()\n",
    "    layers=[]\n",
    "    for idx in range(len(shape)-2):\n",
    "      layers.extend([\n",
    "        nn.Linear(shape[idx],shape[idx+1]),\n",
    "        nn.BatchNorm1d(shape[idx+1]),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "      ])\n",
    "    layers+=[nn.Linear(shape[-2],shape[-1])]\n",
    "    self.features=nn.Sequential(*layers)\n",
    "    for temp in self.features:\n",
    "      if type(temp)==nn.Linear:\n",
    "        torch.nn.init.normal_(temp.weight,mean=0,std=1)\n",
    "  def forward(self,x): return self.features(x)\n",
    "#net=dnn([n_bus*6,n_bus*10,n_bus*10,n_bus*10,n_bus*2]).to(device)\n",
    "#print('number of params: %d'%(sum(temp.numel() for temp in net.parameters() if temp.requires_grad)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_bus: 118\n",
      "Total Parameters: 3,909,576\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                 [-1, 1180]         836,620\n",
      "       BatchNorm1d-2                 [-1, 1180]           2,360\n",
      "              ReLU-3                 [-1, 1180]               0\n",
      "           Dropout-4                 [-1, 1180]               0\n",
      "            Linear-5                 [-1, 1180]       1,393,580\n",
      "       BatchNorm1d-6                 [-1, 1180]           2,360\n",
      "              ReLU-7                 [-1, 1180]               0\n",
      "           Dropout-8                 [-1, 1180]               0\n",
      "            Linear-9                 [-1, 1180]       1,393,580\n",
      "      BatchNorm1d-10                 [-1, 1180]           2,360\n",
      "             ReLU-11                 [-1, 1180]               0\n",
      "          Dropout-12                 [-1, 1180]               0\n",
      "           Linear-13                  [-1, 236]         278,716\n",
      "================================================================\n",
      "Total params: 3,909,576\n",
      "Trainable params: 3,909,576\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.11\n",
      "Params size (MB): 14.91\n",
      "Estimated Total Size (MB): 15.03\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# class dnn(torch.nn.Module):\n",
    "#   def __init__(self,shape,dropout=0):\n",
    "#     super(dnn,self).__init__()\n",
    "#     layers=[]\n",
    "#     for idx in range(len(shape)-2):\n",
    "#       layers.extend([\n",
    "#         nn.Linear(shape[idx],shape[idx+1]),\n",
    "#         nn.ReLU(),\n",
    "#         nn.BatchNorm1d(shape[idx+1]),\n",
    "#         nn.Dropout(dropout)\n",
    "#       ])\n",
    "#     layers.append(nn.Linear(shape[-2],shape[-1]))\n",
    "#     self.features=nn.Sequential(*layers)\n",
    "#     # initialize\n",
    "#     for temp in self.features:\n",
    "#       if type(temp)==nn.Linear:\n",
    "#         torch.nn.init.normal_(temp.weight,mean=0,std=1)\n",
    "#   def forward(self,x):\n",
    "#     return self.features(x)\n",
    "# net=dnn([118*6,118*5,118*5,118*10,118*10,118*10,118*10,118]).to(device)\n",
    "# print('number of params: %d'%(sum(temp.numel() for temp in net.parameters() if temp.requires_grad)))\n",
    "from torchsummary import summary\n",
    "class dnn(torch.nn.Module):\n",
    "    def __init__(self, shape):\n",
    "        super(dnn, self).__init__()\n",
    "        layers = []\n",
    "        for idx in range(len(shape) - 2):\n",
    "            layers.extend([\n",
    "                nn.Linear(shape[idx], shape[idx+1]),\n",
    "                nn.BatchNorm1d(num_features=shape[idx+1]),  # 確保 num_features 正確\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5),\n",
    "            ])\n",
    "        layers.append(nn.Linear(shape[-2], shape[-1]))  # 最後一層\n",
    "        self.features = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # 保證 batch_size 在第一維\n",
    "        return self.features(x)\n",
    "\n",
    "# 測試模型\n",
    "print('n_bus:',n_bus)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net=dnn([n_bus*6,n_bus*10,n_bus*10,n_bus*10,n_bus*2]).to(device) #source code\n",
    "# net = dnn([n_bus*6, n_bus*5, n_bus*5, n_bus*10, n_bus*10, n_bus*10, n_bus*10, n_bus*2]).to(device)#my code\n",
    "\n",
    "\n",
    "# 打印總參數數量\n",
    "total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "\n",
    "summary(net, (1, n_bus*6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "GEQ-MDKOTqW1"
   },
   "outputs": [],
   "source": [
    "# threshold function for p_g\n",
    "class my_gen_pred_binary(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(my_gen_pred_binary,self).__init__()\n",
    "  def forward(self,x,thresh):\n",
    "    right_thresh=thresh.clone().detach().requires_grad_(True).double()\n",
    "    left_thresh=torch.tensor(0).double()\n",
    "    x=x.double()\n",
    "    output = torch.sigmoid(left_thresh - x)\n",
    "    output = torch.mul(output,left_thresh - x) + x\n",
    "    output = torch.sigmoid(output - right_thresh)\n",
    "    output = torch.mul(output,output - right_thresh) + right_thresh\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9mYCMszHaosA"
   },
   "outputs": [],
   "source": [
    "## params needed for S calculation\n",
    "# line parameters\n",
    "filename1 = root+'ieee118_lineloc.txt'\n",
    "filename2 = root+'ieee118_lineparams.txt'\n",
    "filename3 = root+'ieee118_Bmat.txt'\n",
    "# incidence info\n",
    "line_loc = pd.read_table(filename1,sep=',',header=None).to_numpy()\n",
    "# r, x, shunt, S_max\n",
    "line_params = pd.read_table(filename2,sep=',',header=None).to_numpy()\n",
    "B_mat=pd.read_table(filename3,sep=',',header=None).to_numpy()\n",
    "B_r = np.delete(B_mat,68,axis=0)\n",
    "B_r = np.delete(B_r,68,axis=1)\n",
    "Br_inv = np.linalg.inv(B_r)\n",
    "\n",
    "R_line = line_params[:,0].copy()\n",
    "X_line = line_params[:,1].copy()\n",
    "\n",
    "B_shunt = line_params[:,2].copy()\n",
    "\n",
    "Z_line = R_line + 1j * X_line \n",
    "Y_line = 1 / Z_line\n",
    "\n",
    "G_line = np.real(Y_line)\n",
    "B_line = np.imag(Y_line)\n",
    "\n",
    "# transformer indicator\n",
    "a = (R_line > 0).astype(int)\n",
    "\n",
    "# params to tensor and GPU\n",
    "G_line_tensor = torch.from_numpy(G_line).to(device) # conductance\n",
    "B_line_tensor = torch.from_numpy(B_line).to(device) # susceptance\n",
    "B_shunt_tensor = torch.from_numpy(B_shunt/2).to(device) # conductance\n",
    "Br_inv_tensor = torch.from_numpy(Br_inv).to(device) # reduced Bbus matrix\n",
    "a_tensor = torch.from_numpy(a).double().to(device) # line/transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Go4bwoEmoi0D"
   },
   "outputs": [],
   "source": [
    "class loss_func:\n",
    "    def __init__(self,s_max,G_line,B_line,B_shunt,Br_inv,a,line_loc):\n",
    "      self.s=s_max\n",
    "      self.g=G_line\n",
    "      self.b=B_line\n",
    "      self.c=B_shunt\n",
    "      self.r=Br_inv\n",
    "      self.a=a\n",
    "      self.mse=nn.MSELoss() # MSE loss\n",
    "      self.lmda1=torch.tensor(10).to(device) # V MSE \n",
    "      self.lmda2=torch.tensor(1).to(device) # pi MSE \n",
    "      self.lmda3=torch.tensor(0.1).to(device) # v l_inf\n",
    "      self.lmda4=torch.tensor(0.1).to(device) # s feasibility\n",
    "      self.lmda5=torch.tensor(0.01).to(device) # pi l_inf\n",
    "      self.line_loc=line_loc\n",
    "      self.binary_cell=my_gen_pred_binary()\n",
    "    def calc(self,pred,label,x,feas):\n",
    "      mse_p=self.mse(pred[:,:n_bus],label[:,:n_bus])\n",
    "      mse_v=self.mse(pred[:,n_bus:],label[:,n_bus:])\n",
    "      linf_p=(pred[:,:n_bus]-label[:,:n_bus]).norm(p=float('inf'))\n",
    "      linf_v=(pred[:,n_bus:]-label[:,n_bus:]).norm(p=float('inf'))\n",
    "      if feas==False:\n",
    "        return self.lmda1*mse_v+self.lmda2*mse_p+self.lmda3*linf_v+self.lmda5*linf_p\n",
    "      label_pred=pred[:,:n_bus]\n",
    "      p_max=x[:,:n_bus*1]-x[:,n_bus*1:n_bus*2]\n",
    "      quadratic_b=x[:,n_bus*4:n_bus*5]\n",
    "      quadratic_a=x[:,n_bus*5:n_bus*6]\n",
    "      quadratic_center=(label_pred-quadratic_b)/(quadratic_a+1e-5)/2\n",
    "      p_inj=self.binary_cell(quadratic_center,p_max)\n",
    "      bus_inj=p_inj+x[:,n_bus:n_bus*2]\n",
    "      p_inj_r=torch.cat((bus_inj[:,:68],bus_inj[:,69:]),1)/100\n",
    "      theta0=torch.matmul(self.r,p_inj_r.T)\n",
    "      ref_ang=torch.zeros(1,theta0.shape[1]).to(device)\n",
    "      theta=torch.cat([theta0[:68,:],ref_ang,theta0[68:,:]],0)\n",
    "      v_pred=(pred[:,n_bus:].transpose(0,1))*0.01+0.9\n",
    "      \n",
    "      # s penalty\n",
    "      theta1=theta[self.line_loc[:,0]-1,:]\n",
    "      theta2=theta[self.line_loc[:,1]-1,:]\n",
    "      V1=(v_pred[self.line_loc[:,0]-1,:]).double()\n",
    "      V2=(v_pred[self.line_loc[:,1]-1,:]).double()\n",
    "      f_p=(self.a*self.g*(V1*V1).T)-self.a*((V1*V2).T)*(self.g*torch.cos(theta1-theta2).T+self.b*torch.sin(theta1-theta2).T)\n",
    "      f_p=f_p.T\n",
    "      f_q=-self.a*(V1.T)*(self.a*V1.T)*(self.b+self.c/2)+self.a*((V1*V2).T)*(self.b*torch.cos(theta1-theta2).T-self.g*torch.sin(theta1-theta2).T)\n",
    "      f_q=f_q.T\n",
    "      s_pred=torch.sqrt(f_p*f_p+f_q*f_q+1e-5)*100\n",
    "      s_penalty=torch.sigmoid(s_pred-self.s)+torch.sigmoid(-s_pred-self.s)\n",
    "      s_total=torch.sum(s_penalty)\n",
    "\n",
    "      # sji penalty\n",
    "      theta1=theta[self.line_loc[:,1]-1,:]\n",
    "      theta2=theta[self.line_loc[:,0]-1,:]\n",
    "      V1=(v_pred[self.line_loc[:,1]-1,:]).double()\n",
    "      V2=(v_pred[self.line_loc[:,0]-1,:]).double() \n",
    "      fji_p=(self.a*self.g*(V1*V1).T)-self.a*((V1*V2).T)*(self.g*torch.cos(theta1-theta2).T+self.b*torch.sin(theta1-theta2).T)\n",
    "      fji_p=fji_p.T\n",
    "      fji_q=-self.a*(V1.T)*(self.a*V1.T)*(self.b+self.c/2)+self.a*((V1*V2).T)*(self.b*torch.cos(theta1-theta2).T-self.g*torch.sin(theta1-theta2).T)\n",
    "      fji_q=fji_q.T\n",
    "      sji_pred=torch.sqrt(fji_p*fji_p+fji_q*fji_q+1e-5)*100\n",
    "      sji_penalty=torch.sigmoid(sji_pred-self.s)+torch.sigmoid(-sji_pred-self.s)\n",
    "      sji_total=torch.sum(sji_penalty)\n",
    "\n",
    "      return self.lmda1*mse_v+self.lmda2*mse_p+self.lmda3*linf_v+self.lmda5*linf_p+self.lmda4*s_total+self.lmda4*sji_total\n",
    "my_loss=loss_func(f_max,G_line_tensor,B_line_tensor,B_shunt_tensor,Br_inv_tensor,a_tensor,line_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cold start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\AC_OPF\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Training loss: 402.8495 | Learning Rate: 0.001000\n",
      "Epoch 0 | Validation loss: 403.4891 | Learning Rate: 0.001000\n",
      "Epoch 1 | Training loss: 387.2406 | Learning Rate: 0.001000\n",
      "Epoch 2 | Training loss: 369.6128 | Learning Rate: 0.001000\n",
      "Epoch 3 | Training loss: 348.4320 | Learning Rate: 0.001000\n",
      "Epoch 4 | Training loss: 326.6127 | Learning Rate: 0.001000\n",
      "Epoch 5 | Training loss: 304.7817 | Learning Rate: 0.001000\n",
      "Epoch 5 | Validation loss: 265.2620 | Learning Rate: 0.001000\n",
      "Epoch 6 | Training loss: 282.1331 | Learning Rate: 0.001000\n",
      "Epoch 7 | Training loss: 259.1316 | Learning Rate: 0.001000\n",
      "Epoch 8 | Training loss: 236.2010 | Learning Rate: 0.001000\n",
      "Epoch 9 | Training loss: 213.4964 | Learning Rate: 0.001000\n",
      "Epoch 10 | Training loss: 191.0159 | Learning Rate: 0.001000\n",
      "Epoch 10 | Validation loss: 169.7049 | Learning Rate: 0.001000\n",
      "Epoch 11 | Training loss: 169.9681 | Learning Rate: 0.001000\n",
      "Epoch 12 | Training loss: 149.2010 | Learning Rate: 0.001000\n",
      "Epoch 13 | Training loss: 129.9170 | Learning Rate: 0.001000\n",
      "Epoch 14 | Training loss: 112.3402 | Learning Rate: 0.001000\n",
      "Epoch 15 | Training loss: 95.7681 | Learning Rate: 0.001000\n",
      "Epoch 15 | Validation loss: 114.3033 | Learning Rate: 0.001000\n",
      "Epoch 16 | Training loss: 81.0346 | Learning Rate: 0.001000\n",
      "Epoch 17 | Training loss: 67.9403 | Learning Rate: 0.001000\n",
      "Epoch 18 | Training loss: 56.1165 | Learning Rate: 0.001000\n",
      "Epoch 19 | Training loss: 45.9698 | Learning Rate: 0.001000\n",
      "Epoch 20 | Training loss: 37.6446 | Learning Rate: 0.001000\n",
      "Epoch 20 | Validation loss: 34.9859 | Learning Rate: 0.001000\n",
      "Epoch 21 | Training loss: 30.2434 | Learning Rate: 0.001000\n",
      "Epoch 22 | Training loss: 24.3959 | Learning Rate: 0.001000\n",
      "Epoch 23 | Training loss: 19.6191 | Learning Rate: 0.001000\n",
      "Epoch 24 | Training loss: 15.6251 | Learning Rate: 0.001000\n",
      "Epoch 25 | Training loss: 12.7181 | Learning Rate: 0.001000\n",
      "Epoch 25 | Validation loss: 26.5952 | Learning Rate: 0.001000\n",
      "Epoch 26 | Training loss: 10.3883 | Learning Rate: 0.001000\n",
      "Epoch 27 | Training loss: 8.4084 | Learning Rate: 0.001000\n",
      "Epoch 28 | Training loss: 6.8461 | Learning Rate: 0.001000\n",
      "Epoch 29 | Training loss: 5.9526 | Learning Rate: 0.001000\n",
      "Epoch 30 | Training loss: 5.0591 | Learning Rate: 0.001000\n",
      "Epoch 30 | Validation loss: 3.6929 | Learning Rate: 0.001000\n",
      "Epoch 31 | Training loss: 4.3171 | Learning Rate: 0.001000\n",
      "Epoch 32 | Training loss: 3.9911 | Learning Rate: 0.001000\n",
      "Epoch 33 | Training loss: 3.5825 | Learning Rate: 0.001000\n",
      "Epoch 34 | Training loss: 3.3885 | Learning Rate: 0.001000\n",
      "Epoch 35 | Training loss: 3.0000 | Learning Rate: 0.001000\n",
      "Epoch 35 | Validation loss: 2.2359 | Learning Rate: 0.001000\n",
      "Epoch 36 | Training loss: 2.8952 | Learning Rate: 0.001000\n",
      "Epoch 37 | Training loss: 2.7898 | Learning Rate: 0.001000\n",
      "Epoch 38 | Training loss: 2.6523 | Learning Rate: 0.001000\n",
      "Epoch 39 | Training loss: 2.5318 | Learning Rate: 0.001000\n",
      "Epoch 40 | Training loss: 2.3960 | Learning Rate: 0.001000\n",
      "Epoch 40 | Validation loss: 1.7258 | Learning Rate: 0.001000\n",
      "Epoch 41 | Training loss: 2.5735 | Learning Rate: 0.001000\n",
      "Epoch 42 | Training loss: 2.3149 | Learning Rate: 0.001000\n",
      "Epoch 43 | Training loss: 2.2900 | Learning Rate: 0.001000\n",
      "Epoch 44 | Training loss: 2.1122 | Learning Rate: 0.001000\n",
      "Epoch 45 | Training loss: 2.0905 | Learning Rate: 0.001000\n",
      "Epoch 45 | Validation loss: 1.9246 | Learning Rate: 0.001000\n",
      "Epoch 46 | Training loss: 2.0634 | Learning Rate: 0.001000\n",
      "Epoch 47 | Training loss: 1.9936 | Learning Rate: 0.001000\n",
      "Epoch 48 | Training loss: 2.0328 | Learning Rate: 0.001000\n",
      "Epoch 49 | Training loss: 2.0508 | Learning Rate: 0.001000\n",
      "Epoch 50 | Training loss: 1.8834 | Learning Rate: 0.001000\n",
      "Epoch 50 | Validation loss: 4.9983 | Learning Rate: 0.001000\n",
      "Epoch 51 | Training loss: 2.0104 | Learning Rate: 0.001000\n",
      "Epoch 52 | Training loss: 1.8661 | Learning Rate: 0.001000\n",
      "Epoch 53 | Training loss: 1.8985 | Learning Rate: 0.001000\n",
      "Epoch 54 | Training loss: 1.9652 | Learning Rate: 0.001000\n",
      "Epoch 55 | Training loss: 1.8470 | Learning Rate: 0.001000\n",
      "Epoch 55 | Validation loss: 1.8241 | Learning Rate: 0.001000\n",
      "Epoch 56 | Training loss: 1.9228 | Learning Rate: 0.001000\n",
      "Epoch 57 | Training loss: 1.8249 | Learning Rate: 0.001000\n",
      "Epoch 58 | Training loss: 1.8487 | Learning Rate: 0.001000\n",
      "Epoch 59 | Training loss: 1.7317 | Learning Rate: 0.001000\n",
      "Epoch 60 | Training loss: 1.8119 | Learning Rate: 0.001000\n",
      "Epoch 60 | Validation loss: 1.7002 | Learning Rate: 0.001000\n",
      "Epoch 61 | Training loss: 1.7949 | Learning Rate: 0.001000\n",
      "Epoch 62 | Training loss: 1.7854 | Learning Rate: 0.001000\n",
      "Epoch 63 | Training loss: 1.7157 | Learning Rate: 0.001000\n",
      "Epoch 64 | Training loss: 1.7735 | Learning Rate: 0.001000\n",
      "Epoch 65 | Training loss: 1.7326 | Learning Rate: 0.001000\n",
      "Epoch 65 | Validation loss: 2.0221 | Learning Rate: 0.001000\n",
      "Epoch 66 | Training loss: 1.7585 | Learning Rate: 0.001000\n",
      "Epoch 67 | Training loss: 1.7127 | Learning Rate: 0.001000\n",
      "Epoch 68 | Training loss: 1.7431 | Learning Rate: 0.001000\n",
      "Epoch 69 | Training loss: 1.7602 | Learning Rate: 0.001000\n",
      "Epoch 70 | Training loss: 1.6662 | Learning Rate: 0.001000\n",
      "Epoch 70 | Validation loss: 2.4237 | Learning Rate: 0.001000\n",
      "Epoch 71 | Training loss: 1.6369 | Learning Rate: 0.001000\n",
      "Epoch 72 | Training loss: 1.6517 | Learning Rate: 0.001000\n",
      "Epoch 73 | Training loss: 1.5122 | Learning Rate: 0.001000\n",
      "Epoch 74 | Training loss: 1.5791 | Learning Rate: 0.001000\n",
      "Epoch 75 | Training loss: 1.5981 | Learning Rate: 0.001000\n",
      "Epoch 75 | Validation loss: 1.8509 | Learning Rate: 0.001000\n",
      "Epoch 76 | Training loss: 1.5903 | Learning Rate: 0.001000\n",
      "Epoch 77 | Training loss: 1.6039 | Learning Rate: 0.001000\n",
      "Epoch 78 | Training loss: 1.5600 | Learning Rate: 0.001000\n",
      "Epoch 79 | Training loss: 1.6492 | Learning Rate: 0.001000\n",
      "Epoch 80 | Training loss: 1.5663 | Learning Rate: 0.001000\n",
      "Epoch 80 | Validation loss: 1.4724 | Learning Rate: 0.001000\n",
      "Epoch 81 | Training loss: 1.5108 | Learning Rate: 0.001000\n",
      "Epoch 82 | Training loss: 1.5556 | Learning Rate: 0.001000\n",
      "Epoch 83 | Training loss: 1.5777 | Learning Rate: 0.001000\n",
      "Epoch 84 | Training loss: 1.5202 | Learning Rate: 0.001000\n",
      "Epoch 85 | Training loss: 1.4665 | Learning Rate: 0.001000\n",
      "Epoch 85 | Validation loss: 3.4843 | Learning Rate: 0.001000\n",
      "Epoch 86 | Training loss: 1.5194 | Learning Rate: 0.001000\n",
      "Epoch 87 | Training loss: 1.4640 | Learning Rate: 0.001000\n",
      "Epoch 88 | Training loss: 1.4827 | Learning Rate: 0.001000\n",
      "Epoch 89 | Training loss: 1.5187 | Learning Rate: 0.001000\n",
      "Epoch 90 | Training loss: 1.5344 | Learning Rate: 0.001000\n",
      "Epoch 90 | Validation loss: 2.5509 | Learning Rate: 0.001000\n",
      "Epoch 91 | Training loss: 1.5711 | Learning Rate: 0.001000\n",
      "Epoch 92 | Training loss: 1.5821 | Learning Rate: 0.001000\n",
      "Epoch 93 | Training loss: 1.4916 | Learning Rate: 0.001000\n",
      "Epoch 94 | Training loss: 1.4647 | Learning Rate: 0.001000\n",
      "Epoch 95 | Training loss: 1.4307 | Learning Rate: 0.001000\n",
      "Epoch 95 | Validation loss: 1.8227 | Learning Rate: 0.001000\n",
      "Epoch 96 | Training loss: 1.4099 | Learning Rate: 0.001000\n",
      "Epoch 97 | Training loss: 1.4713 | Learning Rate: 0.001000\n",
      "Epoch 98 | Training loss: 1.4327 | Learning Rate: 0.001000\n",
      "Epoch 99 | Training loss: 1.4375 | Learning Rate: 0.001000\n",
      "Epoch 100 | Training loss: 1.4832 | Learning Rate: 0.001000\n",
      "Epoch 100 | Validation loss: 1.7713 | Learning Rate: 0.001000\n",
      "Epoch 101 | Training loss: 1.4671 | Learning Rate: 0.001000\n",
      "Epoch 102 | Training loss: 1.4455 | Learning Rate: 0.001000\n",
      "Epoch 103 | Training loss: 1.4016 | Learning Rate: 0.001000\n",
      "Epoch 104 | Training loss: 1.3787 | Learning Rate: 0.001000\n",
      "Epoch 105 | Training loss: 1.3994 | Learning Rate: 0.001000\n",
      "Epoch 105 | Validation loss: 1.7503 | Learning Rate: 0.001000\n",
      "Epoch 106 | Training loss: 1.4788 | Learning Rate: 0.001000\n",
      "Epoch 107 | Training loss: 1.4040 | Learning Rate: 0.001000\n",
      "Epoch 108 | Training loss: 1.3737 | Learning Rate: 0.001000\n",
      "Epoch 109 | Training loss: 1.4100 | Learning Rate: 0.001000\n",
      "Epoch 110 | Training loss: 1.4224 | Learning Rate: 0.001000\n",
      "Epoch 110 | Validation loss: 3.3625 | Learning Rate: 0.001000\n",
      "Epoch 111 | Training loss: 1.3879 | Learning Rate: 0.000900\n",
      "Epoch 112 | Training loss: 1.4929 | Learning Rate: 0.000900\n",
      "Epoch 113 | Training loss: 1.3863 | Learning Rate: 0.000900\n",
      "Epoch 114 | Training loss: 1.3930 | Learning Rate: 0.000900\n",
      "Epoch 115 | Training loss: 1.3338 | Learning Rate: 0.000900\n",
      "Epoch 115 | Validation loss: 1.5640 | Learning Rate: 0.000900\n",
      "Epoch 116 | Training loss: 1.3931 | Learning Rate: 0.000900\n",
      "Epoch 117 | Training loss: 1.3530 | Learning Rate: 0.000900\n",
      "Epoch 118 | Training loss: 1.3627 | Learning Rate: 0.000900\n",
      "Epoch 119 | Training loss: 1.3944 | Learning Rate: 0.000900\n",
      "Epoch 120 | Training loss: 1.3599 | Learning Rate: 0.000900\n",
      "Epoch 120 | Validation loss: 1.7188 | Learning Rate: 0.000900\n",
      "Epoch 121 | Training loss: 1.3477 | Learning Rate: 0.000900\n",
      "Epoch 122 | Training loss: 1.3687 | Learning Rate: 0.000900\n",
      "Epoch 123 | Training loss: 1.3362 | Learning Rate: 0.000900\n",
      "Epoch 124 | Training loss: 1.3388 | Learning Rate: 0.000900\n",
      "Epoch 125 | Training loss: 1.3175 | Learning Rate: 0.000900\n",
      "Epoch 125 | Validation loss: 1.5544 | Learning Rate: 0.000900\n",
      "Epoch 126 | Training loss: 1.2981 | Learning Rate: 0.000900\n",
      "Epoch 127 | Training loss: 1.3904 | Learning Rate: 0.000900\n",
      "Epoch 128 | Training loss: 1.3924 | Learning Rate: 0.000900\n",
      "Epoch 129 | Training loss: 1.3007 | Learning Rate: 0.000900\n",
      "Epoch 130 | Training loss: 1.3058 | Learning Rate: 0.000900\n",
      "Epoch 130 | Validation loss: 1.6447 | Learning Rate: 0.000900\n",
      "Epoch 131 | Training loss: 1.4252 | Learning Rate: 0.000900\n",
      "Epoch 132 | Training loss: 1.3138 | Learning Rate: 0.000900\n",
      "Epoch 133 | Training loss: 1.3171 | Learning Rate: 0.000900\n",
      "Epoch 134 | Training loss: 1.3993 | Learning Rate: 0.000900\n",
      "Epoch 135 | Training loss: 1.2793 | Learning Rate: 0.000900\n",
      "Epoch 135 | Validation loss: 1.5945 | Learning Rate: 0.000900\n",
      "Epoch 136 | Training loss: 1.3005 | Learning Rate: 0.000900\n",
      "Epoch 137 | Training loss: 1.2761 | Learning Rate: 0.000900\n",
      "Epoch 138 | Training loss: 1.3192 | Learning Rate: 0.000900\n",
      "Epoch 139 | Training loss: 1.2468 | Learning Rate: 0.000900\n",
      "Epoch 140 | Training loss: 1.2652 | Learning Rate: 0.000900\n",
      "Epoch 140 | Validation loss: 1.8754 | Learning Rate: 0.000900\n",
      "Epoch 141 | Training loss: 1.2922 | Learning Rate: 0.000810\n",
      "Epoch 142 | Training loss: 1.3202 | Learning Rate: 0.000810\n",
      "Epoch 143 | Training loss: 1.2476 | Learning Rate: 0.000810\n",
      "Epoch 144 | Training loss: 1.2904 | Learning Rate: 0.000810\n",
      "Epoch 145 | Training loss: 1.2396 | Learning Rate: 0.000810\n",
      "Epoch 145 | Validation loss: 1.5681 | Learning Rate: 0.000810\n",
      "Epoch 146 | Training loss: 1.2453 | Learning Rate: 0.000810\n",
      "Epoch 147 | Training loss: 1.3045 | Learning Rate: 0.000810\n",
      "Epoch 148 | Training loss: 1.2726 | Learning Rate: 0.000810\n",
      "Epoch 149 | Training loss: 1.2652 | Learning Rate: 0.000810\n",
      "Epoch 150 | Training loss: 1.2358 | Learning Rate: 0.000810\n",
      "Epoch 150 | Validation loss: 1.8677 | Learning Rate: 0.000810\n",
      "Epoch 151 | Training loss: 1.1906 | Learning Rate: 0.000810\n",
      "Epoch 152 | Training loss: 1.2540 | Learning Rate: 0.000810\n",
      "Epoch 153 | Training loss: 1.2554 | Learning Rate: 0.000810\n",
      "Epoch 154 | Training loss: 1.2661 | Learning Rate: 0.000810\n",
      "Epoch 155 | Training loss: 1.2494 | Learning Rate: 0.000810\n",
      "Epoch 155 | Validation loss: 1.5963 | Learning Rate: 0.000810\n",
      "Epoch 156 | Training loss: 1.2697 | Learning Rate: 0.000810\n",
      "Epoch 157 | Training loss: 1.2369 | Learning Rate: 0.000810\n",
      "Epoch 158 | Training loss: 1.2526 | Learning Rate: 0.000810\n",
      "Epoch 159 | Training loss: 1.2426 | Learning Rate: 0.000810\n",
      "Epoch 160 | Training loss: 1.2055 | Learning Rate: 0.000810\n",
      "Epoch 160 | Validation loss: 1.3764 | Learning Rate: 0.000810\n",
      "Epoch 161 | Training loss: 1.2401 | Learning Rate: 0.000810\n",
      "Epoch 162 | Training loss: 1.2333 | Learning Rate: 0.000810\n",
      "Epoch 163 | Training loss: 1.2300 | Learning Rate: 0.000810\n",
      "Epoch 164 | Training loss: 1.2411 | Learning Rate: 0.000810\n",
      "Epoch 165 | Training loss: 1.1947 | Learning Rate: 0.000810\n",
      "Epoch 165 | Validation loss: 1.7971 | Learning Rate: 0.000810\n",
      "Epoch 166 | Training loss: 1.1917 | Learning Rate: 0.000810\n",
      "Epoch 167 | Training loss: 1.1917 | Learning Rate: 0.000810\n",
      "Epoch 168 | Training loss: 1.2168 | Learning Rate: 0.000810\n",
      "Epoch 169 | Training loss: 1.2378 | Learning Rate: 0.000810\n",
      "Epoch 170 | Training loss: 1.2099 | Learning Rate: 0.000810\n",
      "Epoch 170 | Validation loss: 1.8784 | Learning Rate: 0.000810\n",
      "Epoch 171 | Training loss: 1.2499 | Learning Rate: 0.000810\n",
      "Epoch 172 | Training loss: 1.2032 | Learning Rate: 0.000810\n",
      "Epoch 173 | Training loss: 1.2083 | Learning Rate: 0.000810\n",
      "Epoch 174 | Training loss: 1.2288 | Learning Rate: 0.000810\n",
      "Epoch 175 | Training loss: 1.2208 | Learning Rate: 0.000810\n",
      "Epoch 175 | Validation loss: 1.5553 | Learning Rate: 0.000810\n",
      "Epoch 176 | Training loss: 1.2004 | Learning Rate: 0.000810\n",
      "Epoch 177 | Training loss: 1.2179 | Learning Rate: 0.000810\n",
      "Epoch 178 | Training loss: 1.1826 | Learning Rate: 0.000810\n",
      "Epoch 179 | Training loss: 1.2244 | Learning Rate: 0.000810\n",
      "Epoch 180 | Training loss: 1.2124 | Learning Rate: 0.000810\n",
      "Epoch 180 | Validation loss: 1.6273 | Learning Rate: 0.000810\n",
      "Epoch 181 | Training loss: 1.1791 | Learning Rate: 0.000810\n",
      "Epoch 182 | Training loss: 1.1959 | Learning Rate: 0.000810\n",
      "Epoch 183 | Training loss: 1.2083 | Learning Rate: 0.000810\n",
      "Epoch 184 | Training loss: 1.1959 | Learning Rate: 0.000810\n",
      "Epoch 185 | Training loss: 1.1847 | Learning Rate: 0.000810\n",
      "Epoch 185 | Validation loss: 1.3880 | Learning Rate: 0.000810\n",
      "Epoch 186 | Training loss: 1.1970 | Learning Rate: 0.000810\n",
      "Epoch 187 | Training loss: 1.1761 | Learning Rate: 0.000810\n",
      "Epoch 188 | Training loss: 1.1699 | Learning Rate: 0.000810\n",
      "Epoch 189 | Training loss: 1.1677 | Learning Rate: 0.000810\n",
      "Epoch 190 | Training loss: 1.1932 | Learning Rate: 0.000810\n",
      "Epoch 190 | Validation loss: 2.0841 | Learning Rate: 0.000810\n",
      "Epoch 191 | Training loss: 1.2628 | Learning Rate: 0.000729\n",
      "Epoch 192 | Training loss: 1.1737 | Learning Rate: 0.000729\n",
      "Epoch 193 | Training loss: 1.1889 | Learning Rate: 0.000729\n",
      "Epoch 194 | Training loss: 1.1227 | Learning Rate: 0.000729\n",
      "Epoch 195 | Training loss: 1.1628 | Learning Rate: 0.000729\n",
      "Epoch 195 | Validation loss: 1.7195 | Learning Rate: 0.000729\n",
      "Epoch 196 | Training loss: 1.1415 | Learning Rate: 0.000729\n",
      "Epoch 197 | Training loss: 1.1920 | Learning Rate: 0.000729\n",
      "Epoch 198 | Training loss: 1.1818 | Learning Rate: 0.000729\n",
      "Epoch 199 | Training loss: 1.1622 | Learning Rate: 0.000729\n",
      "Epoch 200 | Training loss: 1.1481 | Learning Rate: 0.000729\n",
      "Epoch 200 | Validation loss: 1.4996 | Learning Rate: 0.000729\n",
      "Epoch 201 | Training loss: 1.1450 | Learning Rate: 0.000729\n",
      "Epoch 202 | Training loss: 1.1664 | Learning Rate: 0.000729\n",
      "Epoch 203 | Training loss: 1.1620 | Learning Rate: 0.000729\n",
      "Epoch 204 | Training loss: 1.1560 | Learning Rate: 0.000729\n",
      "Epoch 205 | Training loss: 1.1425 | Learning Rate: 0.000729\n",
      "Epoch 205 | Validation loss: 1.2083 | Learning Rate: 0.000729\n",
      "Epoch 206 | Training loss: 1.1725 | Learning Rate: 0.000729\n",
      "Epoch 207 | Training loss: 1.1436 | Learning Rate: 0.000729\n",
      "Epoch 208 | Training loss: 1.1719 | Learning Rate: 0.000729\n",
      "Epoch 209 | Training loss: 1.1835 | Learning Rate: 0.000729\n",
      "Epoch 210 | Training loss: 1.1353 | Learning Rate: 0.000729\n",
      "Epoch 210 | Validation loss: 1.2391 | Learning Rate: 0.000729\n",
      "Epoch 211 | Training loss: 1.1403 | Learning Rate: 0.000729\n",
      "Epoch 212 | Training loss: 1.1168 | Learning Rate: 0.000729\n",
      "Epoch 213 | Training loss: 1.1567 | Learning Rate: 0.000729\n",
      "Epoch 214 | Training loss: 1.1156 | Learning Rate: 0.000729\n",
      "Epoch 215 | Training loss: 1.1419 | Learning Rate: 0.000729\n",
      "Epoch 215 | Validation loss: 1.6033 | Learning Rate: 0.000729\n",
      "Epoch 216 | Training loss: 1.0909 | Learning Rate: 0.000729\n",
      "Epoch 217 | Training loss: 1.1690 | Learning Rate: 0.000729\n",
      "Epoch 218 | Training loss: 1.1625 | Learning Rate: 0.000729\n",
      "Epoch 219 | Training loss: 1.1441 | Learning Rate: 0.000729\n",
      "Epoch 220 | Training loss: 1.1356 | Learning Rate: 0.000729\n",
      "Epoch 220 | Validation loss: 1.6190 | Learning Rate: 0.000729\n",
      "Epoch 221 | Training loss: 1.1043 | Learning Rate: 0.000729\n",
      "Epoch 222 | Training loss: 1.1248 | Learning Rate: 0.000729\n",
      "Epoch 223 | Training loss: 1.1039 | Learning Rate: 0.000729\n",
      "Epoch 224 | Training loss: 1.1306 | Learning Rate: 0.000729\n",
      "Epoch 225 | Training loss: 1.1569 | Learning Rate: 0.000729\n",
      "Epoch 225 | Validation loss: 1.3389 | Learning Rate: 0.000729\n",
      "Epoch 226 | Training loss: 1.1802 | Learning Rate: 0.000729\n",
      "Epoch 227 | Training loss: 1.1105 | Learning Rate: 0.000729\n",
      "Epoch 228 | Training loss: 1.1005 | Learning Rate: 0.000729\n",
      "Epoch 229 | Training loss: 1.0910 | Learning Rate: 0.000729\n",
      "Epoch 230 | Training loss: 1.1167 | Learning Rate: 0.000729\n",
      "Epoch 230 | Validation loss: 1.8530 | Learning Rate: 0.000729\n",
      "Epoch 231 | Training loss: 1.1274 | Learning Rate: 0.000729\n",
      "Epoch 232 | Training loss: 1.0719 | Learning Rate: 0.000729\n",
      "Epoch 233 | Training loss: 1.0911 | Learning Rate: 0.000729\n",
      "Epoch 234 | Training loss: 1.1195 | Learning Rate: 0.000729\n",
      "Epoch 235 | Training loss: 1.1332 | Learning Rate: 0.000729\n",
      "Epoch 235 | Validation loss: 1.9768 | Learning Rate: 0.000729\n",
      "Epoch 236 | Training loss: 1.0788 | Learning Rate: 0.000656\n",
      "Epoch 237 | Training loss: 1.1231 | Learning Rate: 0.000656\n",
      "Epoch 238 | Training loss: 1.1084 | Learning Rate: 0.000656\n",
      "Epoch 239 | Training loss: 1.1070 | Learning Rate: 0.000656\n",
      "Epoch 240 | Training loss: 1.0864 | Learning Rate: 0.000656\n",
      "Epoch 240 | Validation loss: 1.4566 | Learning Rate: 0.000656\n",
      "Epoch 241 | Training loss: 1.0796 | Learning Rate: 0.000656\n",
      "Epoch 242 | Training loss: 1.0885 | Learning Rate: 0.000656\n",
      "Epoch 243 | Training loss: 1.0689 | Learning Rate: 0.000656\n",
      "Epoch 244 | Training loss: 1.0810 | Learning Rate: 0.000656\n",
      "Epoch 245 | Training loss: 1.1306 | Learning Rate: 0.000656\n",
      "Epoch 245 | Validation loss: 1.3018 | Learning Rate: 0.000656\n",
      "Epoch 246 | Training loss: 1.1294 | Learning Rate: 0.000656\n",
      "Epoch 247 | Training loss: 1.0902 | Learning Rate: 0.000656\n",
      "Epoch 248 | Training loss: 1.0802 | Learning Rate: 0.000656\n",
      "Epoch 249 | Training loss: 1.0921 | Learning Rate: 0.000656\n",
      "Epoch 250 | Training loss: 1.0827 | Learning Rate: 0.000656\n",
      "Epoch 250 | Validation loss: 1.6554 | Learning Rate: 0.000656\n",
      "Epoch 251 | Training loss: 1.1176 | Learning Rate: 0.000656\n",
      "Epoch 252 | Training loss: 1.1253 | Learning Rate: 0.000656\n",
      "Epoch 253 | Training loss: 1.1077 | Learning Rate: 0.000656\n",
      "Epoch 254 | Training loss: 1.0782 | Learning Rate: 0.000656\n",
      "Epoch 255 | Training loss: 1.0941 | Learning Rate: 0.000656\n",
      "Epoch 255 | Validation loss: 1.3657 | Learning Rate: 0.000656\n",
      "Epoch 256 | Training loss: 1.0618 | Learning Rate: 0.000656\n",
      "Epoch 257 | Training loss: 1.0831 | Learning Rate: 0.000656\n",
      "Epoch 258 | Training loss: 1.0713 | Learning Rate: 0.000656\n",
      "Epoch 259 | Training loss: 1.0394 | Learning Rate: 0.000656\n",
      "Epoch 260 | Training loss: 1.0626 | Learning Rate: 0.000656\n",
      "Epoch 260 | Validation loss: 1.3666 | Learning Rate: 0.000656\n",
      "Epoch 261 | Training loss: 1.0848 | Learning Rate: 0.000656\n",
      "Epoch 262 | Training loss: 1.1100 | Learning Rate: 0.000656\n",
      "Epoch 263 | Training loss: 1.0937 | Learning Rate: 0.000656\n",
      "Epoch 264 | Training loss: 1.0617 | Learning Rate: 0.000656\n",
      "Epoch 265 | Training loss: 1.0895 | Learning Rate: 0.000656\n",
      "Epoch 265 | Validation loss: 1.5951 | Learning Rate: 0.000656\n",
      "Epoch 266 | Training loss: 1.1075 | Learning Rate: 0.000590\n",
      "Epoch 267 | Training loss: 1.0807 | Learning Rate: 0.000590\n",
      "Epoch 268 | Training loss: 1.0632 | Learning Rate: 0.000590\n",
      "Epoch 269 | Training loss: 1.0532 | Learning Rate: 0.000590\n",
      "Epoch 270 | Training loss: 1.0729 | Learning Rate: 0.000590\n",
      "Epoch 270 | Validation loss: 1.2812 | Learning Rate: 0.000590\n",
      "Epoch 271 | Training loss: 1.0472 | Learning Rate: 0.000590\n",
      "Epoch 272 | Training loss: 1.0740 | Learning Rate: 0.000590\n",
      "Epoch 273 | Training loss: 1.0433 | Learning Rate: 0.000590\n",
      "Epoch 274 | Training loss: 1.0488 | Learning Rate: 0.000590\n",
      "Epoch 275 | Training loss: 1.0644 | Learning Rate: 0.000590\n",
      "Epoch 275 | Validation loss: 1.2371 | Learning Rate: 0.000590\n",
      "Epoch 276 | Training loss: 1.0642 | Learning Rate: 0.000590\n",
      "Epoch 277 | Training loss: 1.0641 | Learning Rate: 0.000590\n",
      "Epoch 278 | Training loss: 1.0706 | Learning Rate: 0.000590\n",
      "Epoch 279 | Training loss: 1.0448 | Learning Rate: 0.000590\n",
      "Epoch 280 | Training loss: 1.0845 | Learning Rate: 0.000590\n",
      "Epoch 280 | Validation loss: 1.3394 | Learning Rate: 0.000590\n",
      "Epoch 281 | Training loss: 1.0761 | Learning Rate: 0.000590\n",
      "Epoch 282 | Training loss: 1.0956 | Learning Rate: 0.000590\n",
      "Epoch 283 | Training loss: 1.0716 | Learning Rate: 0.000590\n",
      "Epoch 284 | Training loss: 1.0697 | Learning Rate: 0.000590\n",
      "Epoch 285 | Training loss: 1.0699 | Learning Rate: 0.000590\n",
      "Epoch 285 | Validation loss: 1.3939 | Learning Rate: 0.000590\n",
      "Epoch 286 | Training loss: 1.0484 | Learning Rate: 0.000590\n",
      "Epoch 287 | Training loss: 1.0497 | Learning Rate: 0.000590\n",
      "Epoch 288 | Training loss: 1.0392 | Learning Rate: 0.000590\n",
      "Epoch 289 | Training loss: 1.0335 | Learning Rate: 0.000590\n",
      "Epoch 290 | Training loss: 1.0393 | Learning Rate: 0.000590\n",
      "Epoch 290 | Validation loss: 1.3342 | Learning Rate: 0.000590\n",
      "Epoch 291 | Training loss: 1.0162 | Learning Rate: 0.000590\n",
      "Epoch 292 | Training loss: 1.0450 | Learning Rate: 0.000590\n",
      "Epoch 293 | Training loss: 1.0768 | Learning Rate: 0.000590\n",
      "Epoch 294 | Training loss: 1.0545 | Learning Rate: 0.000590\n",
      "Epoch 295 | Training loss: 1.0607 | Learning Rate: 0.000590\n",
      "Epoch 295 | Validation loss: 1.5334 | Learning Rate: 0.000590\n",
      "Epoch 296 | Training loss: 1.0343 | Learning Rate: 0.000531\n",
      "Epoch 297 | Training loss: 1.0552 | Learning Rate: 0.000531\n",
      "Epoch 298 | Training loss: 1.0514 | Learning Rate: 0.000531\n",
      "Epoch 299 | Training loss: 1.0265 | Learning Rate: 0.000531\n",
      "Training time: 125.6585s\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "path = root + 'data_118_quad/gnn_trained_ac118.pickle'\n",
    "try:\n",
    "    net.load_state_dict(torch.load(path))\n",
    "    print('params loaded')\n",
    "except:\n",
    "    print('cold start')\n",
    "\n",
    "# 初始學習率\n",
    "lr = 0.001\n",
    "factor = 0.9\n",
    "patience = 5\n",
    "optimizer = optim.AdamW(net.parameters(), lr)\n",
    "\n",
    "# Learning Rate Scheduler: ReduceLROnPlateau\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#     optimizer, \n",
    "#     mode='min',       # 監測 loss，越小越好\n",
    "#     factor=0.9,       # 學習率變成原來的 90%\n",
    "#     patience=5,       # 5 個 epoch 沒有改善才降低學習率\n",
    "#     verbose=True,     # 會輸出學習率變更訊息\n",
    "#     min_lr=1e-6       # 學習率最低不低於 1e-6\n",
    "# )\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min',       # 監測 loss\n",
    "    factor=factor,       # 學習率變成 50% (下降更多)\n",
    "    patience=patience,       # 3 個 epoch 沒有改善就降低學習率\n",
    "    verbose=True,     \n",
    "    min_lr=1e-6       \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "lr_list = []\n",
    "\n",
    "t0 = time.time()\n",
    "max_epochs = 300\n",
    "eval_epoch = 5\n",
    "\n",
    "tolerance = 5  # 早停耐心值\n",
    "min_delta = 1e-3\n",
    "previous = float('inf')\n",
    "\n",
    "feas = False  # 加入可行性標記\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    total_loss = 0.0\n",
    "    for local_batch, local_label in train_set:\n",
    "        optimizer.zero_grad()\n",
    "        local_batch, local_label = local_batch.to(device), local_label.to(device)\n",
    "        logits = net(local_batch)\n",
    "        loss = my_loss.calc(logits, local_label, local_batch, feas)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_set.dataset)\n",
    "    train_loss.append(avg_loss)\n",
    "    \n",
    "    lr_list.append(scheduler.optimizer.param_groups[0]['lr'])\n",
    "    print(f\"Epoch {epoch} | Training loss: {avg_loss:.4f} | Learning Rate: {scheduler.optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    if epoch % eval_epoch == 0:\n",
    "        net.eval()\n",
    "        total_loss = 0.0\n",
    "        for local_batch, local_label in val_set:\n",
    "            local_batch, local_label = local_batch.to(device), local_label.to(device)\n",
    "            logits = net(local_batch)\n",
    "            loss = my_loss.calc(logits, local_label, local_batch, feas)\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(val_set.dataset)\n",
    "        val_loss.append([epoch, avg_loss])\n",
    "        print(f\"Epoch {epoch} | Validation loss: {avg_loss:.4f} | Learning Rate: {scheduler.optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        # 更新 learning rate\n",
    "        scheduler.step(avg_loss)\n",
    "        \n",
    "        # 早停機制\n",
    "        # if epoch:\n",
    "        #     if previous - avg_loss < min_delta:\n",
    "        #         tolerance -= 1\n",
    "        #     if tolerance == 0:\n",
    "        #         print(\"Early stopping triggered\")\n",
    "        #         break\n",
    "        previous = avg_loss\n",
    "        net.train()\n",
    "\n",
    "        final_epoch = epoch\n",
    "\n",
    "t1 = time.time()\n",
    "print(f\"Training time: {t1 - t0:.4f}s\")\n",
    "#印出 lr validation loss training loss 圖\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 60 300\n",
      "✅ 輸出內容已儲存到 C:\\Users\\USER\\Desktop\\GNN_OPF_electricity_market-main\\output_loss_fig\\trainingloss_gunplot\\02181633.txt\n",
      "✅ Plots generated successfully.\n",
      "📂 Loss plot: C:/Users/USER/Desktop/GNN_OPF_electricity_market-main/output_loss_fig/trainingloss_gunplot/loss_plot_02181633.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# **設定輸出目錄**\n",
    "output_dir = r'C:\\Users\\USER\\Desktop\\GNN_OPF_electricity_market-main\\output_loss_fig\\trainingloss_gunplot'\n",
    "timestamp = datetime.now().strftime('%m%d%H%M')\n",
    "output_file = os.path.join(output_dir, f'{timestamp}.txt')\n",
    "\n",
    "# **確保目錄存在**\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(len(train_loss), len(val_loss), len(lr_list))  # **確認長度**\n",
    "\n",
    "# **儲存數據到 txt 檔案**\n",
    "with open(output_file, 'w') as file:\n",
    "    for i in range(len(train_loss)):\n",
    "        if i % 5 == 0 and (i // 5) < len(val_loss):  # **確保 val_loss 沒有超出索引**\n",
    "            val = val_loss[i // 5]  # **正確存入數值**\n",
    "        else:\n",
    "            val = \"NaN\"  # **改成 NaN 讓 Gnuplot 忽略**\n",
    "        file.write(f\"{i} {train_loss[i]} {val[1]} {lr_list[i]}\\n\")\n",
    "\n",
    "print(f\"✅ 輸出內容已儲存到 {output_file}\")\n",
    "\n",
    "# **修正 Windows 路徑格式**\n",
    "gnuplot_file = output_file.replace(\"\\\\\", \"/\")  # **確保 Gnuplot 能識別路徑**\n",
    "loss_plot_path = os.path.join(output_dir, f'loss_plot_{timestamp}.png').replace(\"\\\\\", \"/\")\n",
    "lr_plot_path = os.path.join(output_dir, f'lr_plot_{timestamp}.png').replace(\"\\\\\", \"/\")\n",
    "\n",
    "# **定義 gnuplot 腳本**\n",
    "gnuplot_script = f\"\"\"\n",
    "set terminal pngcairo enhanced font 'Arial,12' size 800,600\n",
    "set output \"{loss_plot_path}\"\n",
    "\n",
    "set title \"Training Loss & Validation Loss\"\n",
    "set xlabel \"Step\"\n",
    "set ylabel \"Value\"\n",
    "set grid\n",
    "set key outside\n",
    "\n",
    "# **確保 Validation Loss 顯示**\n",
    "plot \"{gnuplot_file}\" using 1:2 with lines title \"Train Loss\" linecolor rgb \"blue\", \\\n",
    "     \"{gnuplot_file}\" using 1:3 with lines title \"Val Loss\" linecolor rgb \"red\"\n",
    "   \n",
    "\n",
    "# **第二張圖：Learning Rate**\n",
    "set output \"{lr_plot_path}\"\n",
    "set title \"Learning Rate\"\n",
    "set xlabel \"Step\"\n",
    "set ylabel \"Learning Rate\"\n",
    "set grid\n",
    "set key outside\n",
    "\n",
    "# **繪製 Learning Rate**\n",
    "plot \"{gnuplot_file}\" using 1:4 with lines title \"Learning Rate\" linecolor rgb \"green\" lw 2\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# **執行 gnuplot**\n",
    "try:\n",
    "    result = subprocess.run([\"gnuplot\"], input=gnuplot_script, text=True, check=True,\n",
    "                            stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    if result.stderr:\n",
    "        print(\"⚠️ Gnuplot Error:\", result.stderr)\n",
    "    else:\n",
    "        print(f\"✅ Plots generated successfully.\\n📂 Loss plot: {loss_plot_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ Error: Gnuplot is not installed or not in PATH.\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"❌ Gnuplot execution failed:\\n{e.stderr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AvUTphUSzqzA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Desktop\\GNN_OPF_electricity_market-main\\GNN_OPF_electricity_market-main\\GNN_OPF_electricity_market-main\\data_gen\\118_data\\FCNN_model\\dnn_02181633.pickle\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "timestamp=datetime.now().strftime('%m%d%H%M')\n",
    "path = 'C:\\\\Users\\\\USER\\\\Desktop\\\\GNN_OPF_electricity_market-main\\\\GNN_OPF_electricity_market-main\\\\GNN_OPF_electricity_market-main\\\\data_gen\\\\118_data\\\\FCNN_model\\\\'\n",
    "\n",
    "path = path+'dnn_%s.pickle'%(timestamp)\n",
    "if feas==False: path.replace('feas','')\n",
    "print(path)\n",
    "torch.save(net.state_dict(),path)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "L3M4jmjkscK_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "val_len = len(val_loss)\n",
    "print(val_len)\n",
    "val_plt = np.zeros((2,val_len))\n",
    "for i in range(val_len):\n",
    "  val_plt[0,i] = val_loss[i][0]\n",
    "  val_plt[1,i] = val_loss[i][1]\n",
    "\n",
    "plt.figure()\n",
    "plot_idx = np.arange(np.size(train_loss))\n",
    "plt.plot(plot_idx[5:-1],train_loss[5:-1],lw=2,label='training loss')\n",
    "plt.plot(val_plt[0,1:],val_plt[1,1:],lw=2,label='validation loss')\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "\n",
    "# 在圖的右上角標示學習率\n",
    "plt.text(0.95, 0.95, f'Learning Rate: {lr:.6f}\\nbatch size: {batch_size}', \n",
    "         transform=plt.gca().transAxes, fontsize=10, ha='right', va='top', color='red')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 儲存圖片到指定資料夾（需提供路徑）\n",
    "timestamp=datetime.now().strftime('%m%d%H%M')\n",
    "output_path = r'C:\\Users\\USER\\Desktop\\GNN_OPF_electricity_market-main\\output_loss_fig'\n",
    "output_path = output_path + '/118ac_loss_fig_%s.png' % (timestamp)\n",
    "plt.savefig(output_path, dpi=300, bbox_inches='tight')  # 儲存為 PNG 格式，解析度 300 dpi\n",
    "\n",
    "\n",
    "plt.show(block=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9uGUm4rsco3"
   },
   "source": [
    "# Evaluate the model w/ validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "4u6001UQ2pN3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset size: torch.Size([2000, 708])\n",
      "Number of validation set:  2000\n",
      "torch.Size([2000, 236])\n"
     ]
    }
   ],
   "source": [
    "n_test = np.size(x_test,0)\n",
    "x_test_feed = torch.from_numpy(x_test).float()\n",
    "x_test_feed = x_test_feed#.transpose(1,2)\n",
    "x_test_feed = x_test_feed.to(device)\n",
    "print('Validation dataset size:',x_test_feed.shape)\n",
    "print('Number of validation set: ',n_test)\n",
    "y_pred = net(x_test_feed)\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnNbSGYoXS3J"
   },
   "source": [
    "* Visualization\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKZQaqwJkn3P"
   },
   "source": [
    " - Visualize errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "7-JCo0KmXwm3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 236) (2000, 236)\n"
     ]
    }
   ],
   "source": [
    "y_pred1 = y_pred.cpu().detach()\n",
    "y_pred1 = torch.squeeze(y_pred1,1).numpy()#.transpose()\n",
    "print(y_test.shape,y_pred1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "NOeXUzrd9h9y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 2, 2000) (2000, 236)\n"
     ]
    }
   ],
   "source": [
    "# x=np.reshape(x,(x.shape[0]*x.shape[1],x.shape[2])) # reshape by samples not dim1\n",
    "# y=np.reshape(y,(y.shape[0]*y.shape[1],y.shape[2]))\n",
    "# print(x_pre.shape,y_pre.shape)\n",
    "\n",
    "y_pred_temp = y_pred1.copy().transpose()\n",
    "# y_pred2=np.reshape(y_pred2,(y_pre.shape[0],y_pre.shape[1],n_test))\n",
    "y_pred2=np.zeros([y.shape[0],y.shape[1],n_test])\n",
    "y_pred2[:,0,:]=y_pred_temp[:n_bus,:]\n",
    "y_pred2[:,1,:]=y_pred_temp[n_bus:,:]\n",
    "print(y_pred2.shape,y_pred1.shape)\n",
    "\n",
    "y_test_temp = y_test.copy().transpose()\n",
    "# y_pred2=np.reshape(y_pred2,(y_pre.shape[0],y_pre.shape[1],n_test))\n",
    "y_test2=np.zeros([y.shape[0],y.shape[1],n_test])\n",
    "y_test2[:,0,:]=y_test_temp[:n_bus,:]\n",
    "y_test2[:,1,:]=y_test_temp[n_bus:,:]\n",
    "y_pred1 = y_pred2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "2eVE-t3nl0Cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 2, 2000) (118, 2, 2000)\n"
     ]
    }
   ],
   "source": [
    "# recover the original p.u. scale\n",
    "# vy_deviation) * vy_scale\n",
    "y_pred1[:,1,:] = y_pred1[:,1,:] / vy_scale + vy_deviation\n",
    "y_test2[:,1,:] = y_test2[:,1,:] / vy_scale + vy_deviation\n",
    "print(y_test2.shape,y_pred1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "FUVl5LXeknC4"
   },
   "outputs": [],
   "source": [
    "n_test = np.size(y_test2,2)\n",
    "err_L2 = np.zeros(n_test)\n",
    "err_Linf = np.zeros(n_test)\n",
    "for i in range(n_test):\n",
    "  err_L2[i] = np.linalg.norm(y_test2[:,0,i] - y_pred1[:,0,i]) / np.linalg.norm(y_test2[:,0,i])\n",
    "  err_Linf[i] = np.max(np.abs(y_test2[:,0,i] - y_pred1[:,0,i])) / np.max(np.abs(y_test2[:,0,i]))\n",
    "\n",
    "err_L2_v = np.zeros(n_test)\n",
    "err_Linf_v = np.zeros(n_test)\n",
    "for i in range(n_test):\n",
    "  err_L2_v[i] = np.linalg.norm(y_test2[:,1,i] - y_pred1[:,1,i]) / np.linalg.norm(y_test2[:,1,i])\n",
    "  err_Linf_v[i] = np.max(np.abs(y_test2[:,1,i] - y_pred1[:,1,i])) / np.max(np.abs(y_test2[:,1,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "79xFCVXklkLb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price L2 mean: 0.05730480558669382 L_inf mean: 0.0650073898220493\n",
      "Voltage L2 mean: 0.006139498481707258 L_inf mean: 0.01688125367586567\n"
     ]
    }
   ],
   "source": [
    "err_L2_mean = np.mean(err_L2)\n",
    "err_Linf_mean = np.mean(err_Linf)\n",
    "print('Price L2 mean:', err_L2_mean,'L_inf mean:', err_Linf_mean )\n",
    "err_L2_mean_v = np.mean(err_L2_v)\n",
    "err_Linf_mean_v = np.mean(err_Linf_v)\n",
    "print('Voltage L2 mean:', err_L2_mean_v,'L_inf mean:', err_Linf_mean_v )\n",
    "\n",
    "# fig2 = plt.figure(figsize=(16, 16))\n",
    "# plt.subplot(2, 2, 1)\n",
    "# # plt.hist(np.abs(ga),bins = 10)\n",
    "# plt.plot(err_L2,'bo',markersize=0.5,label = 'L2 error')\n",
    "# plt.plot(err_Linf,'r^',markersize=0.5,label = 'Linf error')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('sample index')\n",
    "# plt.ylabel('error')\n",
    "# plt.title('Normalized sample error')\n",
    "# plt.grid(True)\n",
    "# # error histogram\n",
    "# plt.subplot(2, 2, 2)\n",
    "# plt.hist(err_L2, bins = 50, facecolor='b', alpha=0.75,label = 'L2 error')\n",
    "# plt.hist(err_Linf, bins = 50, facecolor='g', alpha=0.75,label = 'Linf error')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('sample error value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('Histogram of L_2 error')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)\n",
    "\n",
    "# plt.subplot(2, 2, 3)\n",
    "# # plt.hist(np.abs(ga),bins = 10)\n",
    "# plt.plot(err_L2_v,'bo',markersize=0.5,label = 'L2 error')\n",
    "# plt.plot(err_Linf_v,'r^',markersize=0.5,label = 'Linf error')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('sample index')\n",
    "# plt.ylabel('error')\n",
    "# plt.title('Normalized sample error')\n",
    "# plt.grid(True)\n",
    "# # error histogram\n",
    "# plt.subplot(2, 2, 4)\n",
    "# plt.hist(err_L2_v, bins = 50, facecolor='b', alpha=0.75,label = 'L2 error')\n",
    "# plt.hist(err_Linf_v, bins = 50, facecolor='g', alpha=0.75,label = 'Linf error')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('sample error value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('Histogram of L_2 error')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "6sUddh-uGg_p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 2000) (118, 2000)\n",
      "true range: 1.06 0.94\n",
      "predicted range 1.0861692237854004 0.9342292428016663\n"
     ]
    }
   ],
   "source": [
    "print(y_pred1[:,1,:n_test].shape,y_test2[:,1,:n_test].shape)\n",
    "print('true range:',np.max(y_test2[:,1,:n_test]),np.min(y_test2[:,1,:n_test]))\n",
    "print('predicted range',np.max(y_pred1[:,1,:n_test]),np.min(y_pred1[:,1,:n_test]))\n",
    "\n",
    "# fig3 = plt.figure(figsize=(16, 8))\n",
    "# flat_list1 = list(np.concatenate(y_test2[:,1,:n_test]).flat)\n",
    "# flat_list2 = list(np.concatenate(y_pred1[:,1,:n_test]).flat)\n",
    "# plt.hist(flat_list1,bins = 100,label = 'true')\n",
    "\n",
    "# plt.hist(flat_list2,bins = 100,label = 'pred')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "jhfImaPsjKYq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 6, 10000) 10000\n"
     ]
    }
   ],
   "source": [
    "print(x.shape,n_sample)\n",
    "\n",
    "x_new = np.zeros([x.shape[0],x.shape[1],n_sample])\n",
    "for i in range(x.shape[1]):\n",
    "  x_new[:,i,:] = x_total[n_bus*i:n_bus*(i+1),:]\n",
    "\n",
    "y_new = np.zeros([y.shape[0],y.shape[1],n_sample])\n",
    "for i in range(y.shape[1]):\n",
    "  y_new[:,i,:] = y_total[n_bus*i:n_bus*(i+1),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9x7neMNj7n3"
   },
   "source": [
    "# Predict generation using $\\pi$\n",
    "* Using predicted $\\pi$ and find the active constraints in $p_G(i)$\n",
    "* For inactive $p_G(i)$ consider other methods like power flow balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Kr29K04j2KTN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 10000)\n",
      "<class 'numpy.ndarray'> 118 [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117]\n"
     ]
    }
   ],
   "source": [
    "gen_limit0 = x_new[:,4,:].copy() # lin cost\n",
    "print(gen_limit0.shape)\n",
    "\n",
    "gen_idx = []\n",
    "gen_idx = np.arange(n_bus)\n",
    "# for i in range(n_bus):\n",
    "#   if gen_limit0[i,0] > 0:\n",
    "#     gen_idx.append(i)\n",
    "print(type(gen_idx),len(gen_idx),gen_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "gHmx9lMDXzpM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 2, 10000) (236, 10000)\n"
     ]
    }
   ],
   "source": [
    "n_sample=x_total.shape[-1]\n",
    "x_feed = torch.from_numpy(x_total.T).float()\n",
    "y_pred1=net(x_feed.to(device)).cpu().detach().numpy().T\n",
    "y_pred_temp = y_pred1.copy()\n",
    "y_pred2=np.zeros([y.shape[0],y.shape[1],n_sample])\n",
    "y_pred2[:,0,:]=y_pred_temp[:n_bus,:]\n",
    "y_pred2[:,1,:]=y_pred_temp[n_bus:,:]\n",
    "print(y_pred2.shape,y_pred1.shape)\n",
    "y_pred1 = y_pred2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "GPEHF2L91bGv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0004269638061522585\n",
      "0.784000000000006\n",
      "0.1744418088183922\n",
      "0.24221453285700786\n"
     ]
    }
   ],
   "source": [
    "gen_cost0 = x_new[:,4,:].copy()\n",
    "lmp_data = y_new[:,0,:].copy()\n",
    "quadratic_a = x_new[:,5,:].copy()\n",
    "profit_pred = y_pred1[:,0,:] - gen_cost0\n",
    "print(np.min(np.abs(profit_pred)))\n",
    "profit_true = lmp_data - gen_cost0\n",
    "print(np.min(np.abs(profit_true)))\n",
    "profit_pred=(y_pred1[:,0,:]-gen_cost0)/(quadratic_a+1e-10)/2\n",
    "profit_true=(lmp_data-gen_cost0)/(quadratic_a+1e-10)/2\n",
    "print(np.min(np.abs(profit_pred)))\n",
    "print(np.min(np.abs(profit_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "uiHWtU5OLc1x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 10000) (118, 10000)\n",
      "0.044130859416306935\n"
     ]
    }
   ],
   "source": [
    "print(profit_pred.shape,profit_true.shape)\n",
    "profit_err = profit_true - profit_pred\n",
    "profit_err_l2 = np.zeros([n_sample,1])\n",
    "\n",
    "for i in range(n_sample):\n",
    "  profit_err_l2[i] = np.linalg.norm(profit_err[:,i])/np.linalg.norm(profit_true[:,i])\n",
    "print(np.mean(profit_err_l2))\n",
    "\n",
    "# fig5 = plt.figure(figsize=(16, 8))\n",
    "# # error histogram\n",
    "# plt.hist(profit_err_l2, bins = 50, facecolor='b', alpha=0.75,label = 'L2 error')\n",
    "# # plt.hist(err_linf_new, bins = 50, facecolor='g', alpha=0.75,label = 'Linf error')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('sample error value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('Histogram of L_2 error')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "ZbHchRQd_g8-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1180000,)\n",
      "-743.7443171448433 -2.9391182643149665\n"
     ]
    }
   ],
   "source": [
    "p_pred_sort = np.reshape(profit_pred,n_bus*n_sample)\n",
    "p_true_sort = np.reshape(profit_true,n_bus*n_sample)\n",
    "print(p_pred_sort.shape)\n",
    "print(np.min(p_pred_sort),np.min(p_true_sort))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "TYsSdNGp-OLP"
   },
   "outputs": [],
   "source": [
    "# fig2 = plt.figure(figsize=(8, 8))\n",
    "# plt.hist(p_pred_sort, bins = 50, facecolor='b', alpha=0.75,label = 'pred. profit')\n",
    "# plt.hist(p_true_sort, bins = 50, facecolor='g', alpha=0.75,label = 'true profit')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('profit histogram')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "1J9j5tT9p_f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3146256.0048756977 2764601.835122853\n",
      "3146256.0048756977 43260060.56350001 3146256.0048756977\n"
     ]
    }
   ],
   "source": [
    "# x = [load, gen_cost, gen_lim]\n",
    "binary_thres_true = 1e-5\n",
    "binary_thres = x_new[:,0,:].copy() # upper\n",
    "binary_thres_lo = x_new[:,1,:].copy() # lower\n",
    "gen_pred_binary_full = np.zeros((n_bus,n_sample))\n",
    "gen_true_binary_full = np.zeros((n_bus,n_sample))\n",
    "\n",
    "for i in range(n_sample):\n",
    "  for j in range(len(gen_idx)):\n",
    "    # predicted generator limit\n",
    "    if profit_pred[gen_idx[j],i] > binary_thres[gen_idx[j],i]:\n",
    "      gen_pred_binary_full[gen_idx[j],i] = binary_thres[gen_idx[j],i]\n",
    "    elif profit_pred[gen_idx[j],i] < binary_thres_lo[gen_idx[j],i]:\n",
    "      gen_pred_binary_full[gen_idx[j],i] = binary_thres_lo[gen_idx[j],i]\n",
    "    else:\n",
    "      gen_pred_binary_full[gen_idx[j],i] = profit_pred[gen_idx[j],i]\n",
    "    # true generator limit\n",
    "    if profit_true[gen_idx[j],i] > binary_thres[gen_idx[j],i]:\n",
    "      gen_true_binary_full[gen_idx[j],i] = binary_thres[gen_idx[j],i]\n",
    "    elif profit_true[gen_idx[j],i] < binary_thres_lo[gen_idx[j],i]:\n",
    "      gen_true_binary_full[gen_idx[j],i] = binary_thres_lo[gen_idx[j],i]\n",
    "    else:\n",
    "      gen_true_binary_full[gen_idx[j],i] = profit_true[gen_idx[j],i]\n",
    "\n",
    "gen_inj=gen_pred_binary_full\n",
    "gen_inj_true=gen_true_binary_full\n",
    "# nodal injection\n",
    "load0 = -x_new[:,1,:].copy() # load file\n",
    "p_inj = gen_inj #- load0\n",
    "p_inj_true = gen_inj_true #- load0\n",
    "print(np.sum(p_inj),np.sum(gen_inj_true))\n",
    "print(np.sum(p_inj),np.sum(load0),np.sum(gen_inj))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WAgqdRPjAONm"
   },
   "source": [
    "## injection accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "J46pLAw2AQor"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 10000) (118, 10000)\n",
      "mean p_inj l2 err: 0.09582456125126432\n"
     ]
    }
   ],
   "source": [
    "print(p_inj_true.shape,p_inj.shape)\n",
    "p_inj_true_sort = np.reshape(p_inj_true,n_bus*n_sample)\n",
    "p_inj_sort = np.reshape(p_inj,n_bus*n_sample)\n",
    "p_err = np.zeros(n_sample)\n",
    "for i in range(n_sample):\n",
    "  p_err[i] = np.linalg.norm(p_inj_true[:,i]-p_inj[:,i]) / np.linalg.norm(p_inj_true[:,i])\n",
    "print('mean p_inj l2 err:',np.mean(p_err))\n",
    "# fig3 = plt.figure(figsize=(16, 8))\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.hist(p_inj_sort, bins = 50, facecolor='b', alpha=0.75,label = 'pred. injection')\n",
    "# plt.hist(p_inj_true_sort, bins = 50, facecolor='g', alpha=0.75,label = 'true injection')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('injection histogram')\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.hist(p_err, bins = 50, facecolor='b', alpha=0.75,label = 'injection err')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('error histogram')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)s_binary\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DaN7u4xeGIom"
   },
   "source": [
    "* Calculate flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "YT4sgn_n79MI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(186, 1) (186, 10000) (186, 10000)\n",
      "15764 13334\n",
      "0.008475268817204302 0.007168817204301075\n",
      "186 10000 (186, 10000)\n"
     ]
    }
   ],
   "source": [
    "filename=root+'118ac_fmax.txt'\n",
    "f_max1=pd.read_table(filename,sep=',',header=None).to_numpy() # flow limit\n",
    "\n",
    "n_line = np.size(S_isf,0)\n",
    "flow_est = np.zeros((n_line,n_sample))\n",
    "flow_est0 = np.zeros((n_line,n_sample))\n",
    "\n",
    "f_binary = np.zeros((n_line,n_sample))\n",
    "f_binary0 = np.zeros((n_line,n_sample))\n",
    "\n",
    "# for i in range(n_sample):\n",
    "flow_est = np.dot(S_isf,p_inj)\n",
    "flow_est0 = np.dot(S_isf,p_inj_true)\n",
    "# f_max\n",
    "# f_max_numpy = f_max.cpu().detach().numpy()\n",
    "f_max_numpy = f_max1.copy()\n",
    "f_binary = (np.abs(flow_est)-f_max_numpy > 0)\n",
    "f_binary0 = (np.abs(flow_est0)-f_max_numpy > 0)\n",
    "\n",
    "print(f_max_numpy.shape,flow_est.shape,flow_est0.shape)\n",
    "f_tot_sample = n_line * n_sample\n",
    "print(np.sum(f_binary),np.sum(f_binary0))\n",
    "print(np.sum(f_binary)/f_tot_sample,np.sum(f_binary0)/f_tot_sample)\n",
    "print(n_line,n_sample,flow_est.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "VyvDpKhyQj0M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174.0339649672112 39.64401431411352\n",
      "1.1602264331147414 0.2642934287607568\n"
     ]
    }
   ],
   "source": [
    "# soft threshold\n",
    "f_err_est = np.abs(flow_est)-f_max_numpy\n",
    "f_err_true = np.abs(flow_est0)-f_max_numpy\n",
    "\n",
    "f_err_est = np.maximum(np.abs(flow_est)-f_max_numpy,0) # identify violations\n",
    "f_err_true = np.maximum(np.abs(flow_est0)-f_max_numpy,0)\n",
    "\n",
    "print(np.max(f_err_est),np.max(f_err_true))\n",
    "print(np.max(f_err_est/f_max_numpy),np.max(f_err_true/f_max_numpy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "7iuX7tN2a2Cp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12055 10451\n",
      "0.0064811827956989245 0.005618817204301075\n"
     ]
    }
   ],
   "source": [
    "f_binary_soft = (np.abs(flow_est)-f_max_numpy > 0.1*(f_max_numpy))\n",
    "f_binary0_soft = (np.abs(flow_est0)-f_max_numpy > 0.1*(f_max_numpy))\n",
    "print(np.sum(f_binary_soft),np.sum(f_binary0_soft))\n",
    "print(np.sum(f_binary_soft)/f_tot_sample,np.sum(f_binary0_soft)/f_tot_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "SYl9pxnOUUQF"
   },
   "outputs": [],
   "source": [
    "f_pred_sort = np.reshape(f_err_est/f_max_numpy,n_line*n_sample)\n",
    "f_true_sort = np.reshape(f_err_true/f_max_numpy,n_line*n_sample)\n",
    "\n",
    "# fig2 = plt.figure(figsize=(16, 8))\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.hist(f_pred_sort, bins = 10, facecolor='b', alpha=0.75,label = 'pred. f')\n",
    "# plt.hist(f_true_sort, bins = 10, facecolor='g', alpha=0.75,label = 'true f')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('percentage')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('flow violation level histogram')\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.hist(f_true_sort, bins = 10, facecolor='g', alpha=0.75,label = 'true f')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('percentage')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('flow violation level histogram')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "YglgTpWVRLri"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sample pred: 7\n",
      "max line pred: 8915\n",
      "max sample true: 3\n",
      "max line true: 10000\n"
     ]
    }
   ],
   "source": [
    "f_line = np.sum(f_binary,0)\n",
    "f_samp = np.sum(f_binary,1)\n",
    "print('max sample pred:',np.max(f_line))\n",
    "print('max line pred:',np.max(f_samp))\n",
    "\n",
    "f_line0  = np.sum(f_binary0,0)\n",
    "f_samp0 = np.sum(f_binary0,1)\n",
    "print('max sample true:',np.max(f_line0))\n",
    "print('max line true:',np.max(f_samp0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZQnarHmPl35"
   },
   "source": [
    "# Check objective optimality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "BZjhp-CgQaDz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07596602783941205\n"
     ]
    }
   ],
   "source": [
    "gen_cost_pred = np.zeros((n_bus,n_sample))\n",
    "gen_cost_true = np.zeros((n_bus,n_sample))\n",
    "objective_err = np.zeros(n_sample)\n",
    "\n",
    "gen_cost_pred = np.multiply(np.multiply(p_inj,p_inj),quadratic_a) + np.multiply(p_inj,gen_cost0)\n",
    "gen_cost_true = np.multiply(np.multiply(p_inj_true,p_inj_true),quadratic_a) + np.multiply(p_inj_true,gen_cost0)\n",
    "\n",
    "objective_err = np.sum(np.abs(gen_cost_true-gen_cost_pred),axis=0) / np.sum(gen_cost_true,axis=0)\n",
    "print(np.mean(objective_err))\n",
    "\n",
    "# fig6 = plt.figure(figsize=(16, 8))\n",
    "# # error histogram\n",
    "# plt.hist(objective_err, bins = 50, facecolor='b', alpha=0.75,label = 'L2 error')\n",
    "# # plt.hist(err_linf_new, bins = 50, facecolor='g', alpha=0.75,label = 'Linf error')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('sample error value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('Histogram of L_2 error')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdrsAWpYo0-w"
   },
   "source": [
    "## injection accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "mArjSN-So0-x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 10000) (118, 10000)\n",
      "mean p_inj l2 err: 0.09582456125126432\n"
     ]
    }
   ],
   "source": [
    "print(p_inj_true.shape,p_inj.shape)\n",
    "\n",
    "p_inj_true_sort = np.reshape(p_inj_true,n_bus*n_sample)\n",
    "p_inj_sort = np.reshape(p_inj,n_bus*n_sample)\n",
    "\n",
    "p_err = np.zeros(n_sample)\n",
    "for i in range(n_sample):\n",
    "  p_err[i] = np.linalg.norm(p_inj_true[:,i]-p_inj[:,i]) / np.linalg.norm(p_inj_true[:,i])\n",
    "\n",
    "print('mean p_inj l2 err:',np.mean(p_err))\n",
    "# fig3 = plt.figure(figsize=(16, 8))\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.hist(p_inj_sort, bins = 50, facecolor='b', alpha=0.75,label = 'pred. injection')\n",
    "# plt.hist(p_inj_true_sort, bins = 50, facecolor='g', alpha=0.75,label = 'true injection')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('injection histogram')s_binary\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.hist(p_err, bins = 50, facecolor='b', alpha=0.75,label = 'injection err')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('error histogram')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujU84tOSpqsy"
   },
   "source": [
    "# Test AC feasibility\n",
    "* P in actual value, V in p.u.\n",
    "* Use P to recover $\\theta$, or solve $\\theta$ and Q for PF\n",
    "$$ Q_m = V_m \\sum_{n=1}^N V_n \\left(G_{mn}\\sin\\theta_{mn} - B_{mn}\\cos\\theta_{mn} \\right) $$\n",
    "calculate $Q_{mn}$ directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "Of6mEXF4puDN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 118) (118, 118)\n",
      "(117, 117) (118, 10000) (118, 10000)\n",
      "(118, 10000) (118, 10000)\n"
     ]
    }
   ],
   "source": [
    "# Bbus and B_r inverse\n",
    "filename1 = root+'ieee118_Bbus.txt'\n",
    "Bbus=pd.read_table(filename1,sep=',',header=None).to_numpy()\n",
    "B_r = np.delete(Bbus,68,axis=0)\n",
    "B_r = np.delete(B_r,68,axis=1)\n",
    "Br_inv = np.linalg.inv(B_r)\n",
    "\n",
    "# Y = G + jB\n",
    "filename1 = root+'ieee118_Gmat.txt'\n",
    "G_mat=pd.read_table(filename1,sep=',',header=None).to_numpy()\n",
    "filename1 = root+'ieee118_Bmat.txt'\n",
    "B_mat=pd.read_table(filename1,sep=',',header=None).to_numpy()\n",
    "print(G_mat.shape,B_mat.shape)\n",
    "\n",
    "# line parameters\n",
    "filename1 = root+'ieee118_lineloc.txt'\n",
    "line_loc = pd.read_table(filename1,sep=',',header=None).to_numpy()\n",
    "\n",
    "# load line params\n",
    "filename1 = root+'ieee118_lineparams.txt'\n",
    "line_params = pd.read_table(filename1,sep=',',header=None).to_numpy()\n",
    "R_line = line_params[:,0].copy()\n",
    "X_line = line_params[:,1].copy()\n",
    "B_shunt = line_params[:,2].copy()\n",
    "Z_line = R_line + 1j * X_line \n",
    "Y_line = 1 / Z_line\n",
    "G_line = np.real(Y_line)\n",
    "B_line = np.imag(Y_line)\n",
    "# P_inj w/out reference bus in p.u.\n",
    "p_inj_r = np.delete(p_inj,68,axis=0) / 100\n",
    "p_inj_true_r = np.delete(p_inj_true,68,axis=0) / 100\n",
    "p_inj_pu = p_inj / 100\n",
    "p_inj_true_pu = p_inj_true / 100\n",
    "print(Br_inv.shape,p_inj.shape,p_inj_true.shape)#p_inj_true\n",
    "\n",
    "theta0 = np.matmul(Br_inv,p_inj_r)\n",
    "theta_true0 = np.matmul(Br_inv,p_inj_true_r)\n",
    "theta = np.insert(theta0,68,0,axis = 0)\n",
    "theta_true = np.insert(theta_true0,68,0,axis = 0)\n",
    "print(theta.shape,theta_true.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47173524296823127 -0.9678369074831753\n",
      "2.7803011534120627 -9.166735486002148\n"
     ]
    }
   ],
   "source": [
    "print(np.max(theta),np.min(theta))\n",
    "math.sin(math.pi/6)\n",
    "print(G_line[0],B_line[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(186, 10000)\n",
      "1.091443099975586 0.9323860096931458 (118, 10000)\n"
     ]
    }
   ],
   "source": [
    "# Calculate real and reactive flow\n",
    "f_p = np.zeros((n_line,n_sample))\n",
    "f_q = np.zeros((n_line,n_sample))\n",
    "fji_p = np.zeros((n_line,n_sample))\n",
    "fji_q = np.zeros((n_line,n_sample))\n",
    "print(f_q.shape)\n",
    "\n",
    "v_pred = y_pred1[:,1,:].copy()\n",
    "v_pred = v_pred / vy_scale + vy_deviation\n",
    "print(np.max(v_pred),np.min(v_pred),v_pred.shape)\n",
    "\n",
    "theta1 = theta[line_loc[:,0]-1,:]\n",
    "theta2 = theta[line_loc[:,1]-1,:]\n",
    "V1 = v_pred[line_loc[:,0]-1,:]\n",
    "V2 = v_pred[line_loc[:,1]-1,:] \n",
    "f_p=(a*G_line*(V1*V1).T)-a*((V1*V2).T)*(G_line*np.cos(theta1-theta2).T+B_line*np.sin(theta1-theta2).T)\n",
    "f_p=f_p.T\n",
    "f_q=-a*(V1.T)*(a*V1.T)*(B_line+B_shunt/2)+a*((V1*V2).T)*(B_line*np.cos(theta1-theta2).T-G_line*np.sin(theta1-theta2).T)\n",
    "f_q=f_q.T\n",
    "\n",
    "theta1 = theta[line_loc[:,1]-1,:]\n",
    "theta2 = theta[line_loc[:,0]-1,:]\n",
    "V1 = v_pred[line_loc[:,1]-1,:]\n",
    "V2 = v_pred[line_loc[:,0]-1,:]\n",
    "fji_p=(a*G_line*(V1*V1).T)-a*((V1*V2).T)*(G_line*np.cos(theta1-theta2).T+B_line*np.sin(theta1-theta2).T)\n",
    "fji_p=fji_p.T\n",
    "fji_q=-a*(V1.T)*(a*V1.T)*(B_line+B_shunt/2)+a*((V1*V2).T)*(B_line*np.cos(theta1-theta2).T-G_line*np.sin(theta1-theta2).T)\n",
    "fji_q=fji_q.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "gKfcrbTSVMeO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0428536227460423 -1.4338526208346138\n",
      "15.948691902078052 151.0\n"
     ]
    }
   ],
   "source": [
    "s_pred = np.sqrt(f_p*f_p+f_q*f_q)*100\n",
    "sji_pred = np.sqrt(fji_p*fji_p+fji_q*fji_q)*100\n",
    "print(np.max(f_q),np.min(f_q))\n",
    "flow_est.shape\n",
    "print(np.mean(s_pred[0,:]),np.mean(f_max_numpy[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "YO4__LJ2brLu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20435\n",
      "hard violation rate: 0.010986559139784946\n",
      "13811\n",
      "0.007425268817204301\n"
     ]
    }
   ],
   "source": [
    "sij_binary = (np.abs(s_pred)-f_max_numpy[:n_line] > 0)\n",
    "sji_binary = (np.abs(sji_pred)-f_max_numpy[:n_line] > 0)\n",
    "s_binary = np.maximum(sij_binary,sji_binary)\n",
    "print(np.sum(s_binary))#,np.sum(f_binary0))\n",
    "print('hard violation rate:',np.sum(s_binary)/n_sample/n_line)#,np.sum(f_binary0)/f_tot_sample)\n",
    "s_binary_soft = (np.abs(s_pred)-f_max_numpy[:n_line] > 0.1*(f_max_numpy[:n_line]))\n",
    "print(np.sum(s_binary_soft))#,np.sum(f_binary0_soft))\n",
    "print(np.sum(s_binary_soft)/n_sample/n_line)#,np.sum(f_binary0_soft)/f_tot_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "Ty0WpBdfJwMR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S violation level:\n",
      "hard: 0.010986559139784946\n",
      "mean: 0.002877655302614395\n",
      "median: 0.0\n",
      "max: 1.181699710423837\n",
      "std: 0.03571860289369648\n",
      "p99: 0.02364705608820588\n",
      "f violation level:\n",
      "hard: 0.008475268817204302 0.007168817204301075\n",
      "mean: 0.0026511273559789044\n",
      "median: 0.0\n",
      "max: 1.1602264331147414\n",
      "std: 0.036370701645125125\n",
      "p99: 0.0\n"
     ]
    }
   ],
   "source": [
    "# violation level\n",
    "sij_violation = np.abs(s_pred)-f_max_numpy[:n_line] #/ f_max_numpy\n",
    "sij_violation_level = np.maximum(sij_violation,0)\n",
    "sji_violation = np.abs(sji_pred)-f_max_numpy[:n_line] #/ f_max_numpy\n",
    "sji_violation_level = np.maximum(sji_violation,0)\n",
    "s_violation_level = np.maximum(sij_violation_level,sji_violation_level)\n",
    "s_violation_level = np.divide(s_violation_level,f_max_numpy[:n_line])\n",
    "s_vio_lvl = np.reshape(s_violation_level,n_line*n_sample)\n",
    "\n",
    "print('S violation level:')\n",
    "print('hard:',np.sum(s_binary)/f_tot_sample)\n",
    "print('mean:',np.mean(s_vio_lvl))\n",
    "print('median:',np.median(s_vio_lvl))\n",
    "print('max:',np.max(s_vio_lvl))\n",
    "print('std:',np.std(s_vio_lvl))\n",
    "print('p99:',np.percentile(s_vio_lvl,99))\n",
    "\n",
    "f_violation = np.abs(flow_est)-f_max_numpy #/ f_max_numpy\n",
    "f_violation_level = np.maximum(f_violation,0)\n",
    "f_violation_level = np.divide(f_violation_level,f_max_numpy)\n",
    "f_vio_lvl = np.reshape(f_violation_level,n_line*n_sample)\n",
    "\n",
    "print('f violation level:')\n",
    "print('hard:',np.sum(f_binary)/f_tot_sample,np.sum(f_binary0)/f_tot_sample)\n",
    "print('mean:',np.mean(f_vio_lvl))\n",
    "print('median:',np.median(f_vio_lvl))\n",
    "print('max:',np.max(f_vio_lvl))\n",
    "print('std:',np.std(f_vio_lvl))\n",
    "print('p99:',np.percentile(f_vio_lvl,99))\n",
    "\n",
    "# fig4 = plt.figure(figsize=(6,4))\n",
    "# plt.hist(s_vio_lvl, bins = 50, facecolor='b', alpha=0.75,label = 's violation')\n",
    "# plt.hist(f_vio_lvl, bins = 50, facecolor='r', alpha=0.75,label = 'f violation')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('violation level')\n",
    "# plt.ylabel('frequency')\n",
    "# # plt.title('injection histogram')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "kWXEpj-ryjbJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price L2 mean: 0.05730480558669382 L_inf mean: 0.0650073898220493\n",
      "std: 0.0383416399532124\n",
      "Voltage L2 mean: 0.006139498481707258 L_inf mean: 0.01688125367586567\n",
      "std: 0.003476186314642975\n"
     ]
    }
   ],
   "source": [
    "# err_L2_mean = np.mean(err_L2)\n",
    "# err_Linf_mean = np.mean(err_Linf)\n",
    "print('Price L2 mean:', err_L2_mean,'L_inf mean:', err_Linf_mean )\n",
    "print('std:',np.std(err_L2))\n",
    "# err_L2_mean_v = np.mean(err_L2_v)\n",
    "# err_Linf_mean_v = np.mean(err_Linf_v)\n",
    "print('Voltage L2 mean:', err_L2_mean_v,'L_inf mean:', err_Linf_mean_v )\n",
    "print('std:',np.std(err_L2_v))\n",
    "\n",
    "params = (sum(temp.numel() for temp in net.parameters() if temp.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "輸出內容已儲存到 C:\\Users\\USER\\Desktop\\GNN_OPF_electricity_market-main\\output_loss_fig/118ac_output_02181633.txt\n"
     ]
    }
   ],
   "source": [
    "import os   \n",
    "# 建立輸出內容字串\n",
    "output_content = f\"\"\"\n",
    "date: {timestamp}\n",
    "final_epoch: {final_epoch}\n",
    "training time: {t1 - t0:.4e}s\n",
    "\n",
    "factor: {factor}\n",
    "patience: {patience}\n",
    "\n",
    "start learning rate: {lr:.4e}\n",
    "batch size: {batch_size}\n",
    "params number: {params:.4e}\n",
    "\n",
    "Price L2 mean: {err_L2_mean:.4e} \n",
    "Price std: {np.std(err_L2):.4e}\n",
    "\n",
    "Voltage L2 mean: {err_L2_mean_v:.4e} \n",
    "Voltage std: {np.std(err_L2_v):.4e}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# 設定儲存路徑和檔名\n",
    "output_dir = r'C:\\Users\\USER\\Desktop\\GNN_OPF_electricity_market-main\\output_loss_fig'\n",
    "timestamp = datetime.now().strftime('%m%d%H%M')\n",
    "output_file = f'{output_dir}/118ac_output_{timestamp}.txt'\n",
    "\n",
    "# 確保目錄存在\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 將內容寫入 txt 檔案\n",
    "with open(output_file, 'w') as file:\n",
    "    file.write(output_content)\n",
    "\n",
    "print(f\"輸出內容已儲存到 {output_file}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "118ac_feasdnn0417.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "AC_OPF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
