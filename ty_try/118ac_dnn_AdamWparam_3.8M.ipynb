{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JKLovDeXoCCP"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import math\n",
    "from datetime import datetime\n",
    "root=''\n",
    "# try:\n",
    "#   from google.colab import drive\n",
    "#   drive.mount('/content/drive')\n",
    "#   root='./drive/MyDrive/gnn/data/'\n",
    "# except:\n",
    "#   pass\n",
    "# device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# print(torch.cuda.get_device_name(0))\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# 檢查是否有可用的 GPU，否則使用 CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([186, 118]) torch.Size([186, 1]) tensor(72)\n",
      "1654.8 -332.4 949.67 0.94\n",
      "voltage range(scaled): 3.9999999999999925 16.000000000000004\n",
      "price range old: 11.547 949.67\n",
      "voltage range old: 3.9999999999999925 16.000000000000004\n",
      "price range new: 11.547 949.67\n",
      "voltage range new: 3.9999999999999925 16.000000000000004\n",
      "(118, 6, 10000) (118, 2, 10000)\n"
     ]
    }
   ],
   "source": [
    "root = 'C:/Users/USER/Desktop/GNN_OPF_electricity_market-main/GNN_OPF_electricity_market-main/GNN_OPF_electricity_market-main/data_gen/118_data/'\n",
    "# C:\\Users\\USER\\Desktop\\GNN_OPF_electricity_market-main\\GNN_OPF_electricity_market-main\\GNN_OPF_electricity_market-main\\data_gen\\118_data\\118dc_quad_ISF.txt\n",
    "filename = root + '118dc_quad_ISF.txt'\n",
    "\n",
    "S_isf=pd.read_table(filename,sep=',',header=None).to_numpy() # ISF matrix\n",
    "filename=root+'118ac_fmax.txt'\n",
    "f_max=pd.read_table(filename,sep=',',header=None).to_numpy() # flow limit\n",
    "n_line = np.size(S_isf,0)\n",
    "S = torch.from_numpy(S_isf).to(device) # ISF\n",
    "f_max = torch.from_numpy(f_max).to(device) # flow limit\n",
    "print(S.shape,f_max.shape,torch.min(f_max))\n",
    "\n",
    "x=np.load(root+'ac118_p10_x_v.npy')\n",
    "y=np.load(root+'ac118_p10_y_v.npy')\n",
    "W=np.load(root+'ac118_p10_w.npy')\n",
    "print(np.max(x),np.min(x),np.max(y),np.min(y))\n",
    "\n",
    "# scaling on voltage\n",
    "vy_deviation = 0.9\n",
    "vy_scale = 100\n",
    "y[:,1,:] = (y[:,1,:] - vy_deviation) * vy_scale\n",
    "print('voltage range(scaled):',np.min(y[:,1,:]),np.max(y[:,1,:]))\n",
    "\n",
    "# scaling on price\n",
    "pi_deviation = 0\n",
    "y[:,0,:] = y[:,0,:] + pi_deviation\n",
    "# filter out extreme points in price\n",
    "y_sort_arg = np.argsort(np.amax(np.abs(y[:,0,:]),axis=0)) # max extreme\n",
    "y_sort_arg1 = np.argsort(np.amin(y[:,0,:],axis=0),axis=0) # min extreme\n",
    "\n",
    "del_idx0 = []\n",
    "del_num = 0\n",
    "for i in range(del_num):\n",
    "  del_idx0.append(y_sort_arg[-i])\n",
    "  del_idx0.append(y_sort_arg1[i])\n",
    "# print(del_idx0)\n",
    "del_idx = [] # keep only non-repeated\n",
    "[del_idx.append(x) for x in del_idx0 if x not in del_idx]\n",
    "del_idx = np.sort(del_idx)\n",
    "# delete extreme points\n",
    "print('price range old:',np.min(y[:,0,:]),np.max(y[:,0,:]))\n",
    "print('voltage range old:',np.min(y[:,1,:]),np.max(y[:,1,:]))\n",
    "# x = np.delete(x, del_idx, axis=2)\n",
    "# y = np.delete(y, del_idx, axis=2)\n",
    "print('price range new:',np.min(y[:,0,:]),np.max(y[:,0,:]))\n",
    "print('voltage range new:',np.min(y[:,1,:]),np.max(y[:,1,:]))\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Hj9_eLfoWQY3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([186, 118]) torch.Size([186, 1]) tensor(72)\n",
      "1654.8 -332.4 949.67 0.94\n",
      "voltage range(scaled): 3.9999999999999925 16.000000000000004\n",
      "price range old: 11.547 949.67\n",
      "voltage range old: 3.9999999999999925 16.000000000000004\n",
      "price range new: 11.547 949.67\n",
      "voltage range new: 3.9999999999999925 16.000000000000004\n",
      "(118, 6, 10000) (118, 2, 10000)\n"
     ]
    }
   ],
   "source": [
    "root = 'C:/Users/USER/Desktop/GNN_OPF_electricity_market-main/GNN_OPF_electricity_market-main/GNN_OPF_electricity_market-main/data_gen/118_data/'\n",
    "# C:\\Users\\USER\\Desktop\\GNN_OPF_electricity_market-main\\GNN_OPF_electricity_market-main\\GNN_OPF_electricity_market-main\\data_gen\\118_data\\118dc_quad_ISF.txt\n",
    "filename = root + '118dc_quad_ISF.txt'\n",
    "\n",
    "S_isf=pd.read_table(filename,sep=',',header=None).to_numpy() # ISF matrix\n",
    "filename=root+'118ac_fmax.txt'\n",
    "f_max=pd.read_table(filename,sep=',',header=None).to_numpy() # flow limit\n",
    "n_line = np.size(S_isf,0)\n",
    "S = torch.from_numpy(S_isf).to(device) # ISF\n",
    "f_max = torch.from_numpy(f_max).to(device) # flow limit\n",
    "print(S.shape,f_max.shape,torch.min(f_max))\n",
    "\n",
    "x=np.load(root+'ac118_p10_x_v.npy')\n",
    "y=np.load(root+'ac118_p10_y_v.npy')\n",
    "W=np.load(root+'ac118_p10_w.npy')\n",
    "print(np.max(x),np.min(x),np.max(y),np.min(y))\n",
    "\n",
    "# scaling on voltage\n",
    "vy_deviation = 0.9\n",
    "vy_scale = 100\n",
    "y[:,1,:] = (y[:,1,:] - vy_deviation) * vy_scale\n",
    "print('voltage range(scaled):',np.min(y[:,1,:]),np.max(y[:,1,:]))\n",
    "\n",
    "# scaling on price\n",
    "pi_deviation = 0\n",
    "y[:,0,:] = y[:,0,:] + pi_deviation\n",
    "# filter out extreme points in price\n",
    "y_sort_arg = np.argsort(np.amax(np.abs(y[:,0,:]),axis=0)) # max extreme\n",
    "y_sort_arg1 = np.argsort(np.amin(y[:,0,:],axis=0),axis=0) # min extreme\n",
    "\n",
    "del_idx0 = []\n",
    "del_num = 0\n",
    "for i in range(del_num):\n",
    "  del_idx0.append(y_sort_arg[-i])\n",
    "  del_idx0.append(y_sort_arg1[i])\n",
    "# print(del_idx0)\n",
    "del_idx = [] # keep only non-repeated\n",
    "[del_idx.append(x) for x in del_idx0 if x not in del_idx]\n",
    "del_idx = np.sort(del_idx)\n",
    "# delete extreme points\n",
    "print('price range old:',np.min(y[:,0,:]),np.max(y[:,0,:]))\n",
    "print('voltage range old:',np.min(y[:,1,:]),np.max(y[:,1,:]))\n",
    "# x = np.delete(x, del_idx, axis=2)\n",
    "# y = np.delete(y, del_idx, axis=2)\n",
    "print('price range new:',np.min(y[:,0,:]),np.max(y[:,0,:]))\n",
    "print('voltage range new:',np.min(y[:,1,:]),np.max(y[:,1,:]))\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "YcR42RBbdZqZ"
   },
   "outputs": [],
   "source": [
    "n_sample=y.shape[-1]\n",
    "n_bus=y.shape[0]\n",
    "x_total=x.transpose((1,0,2)).reshape(-1,x.shape[-1])\n",
    "y_total=y.transpose((1,0,2)).reshape(-1,y.shape[-1])\n",
    "x_train,x_test,y_train,y_test=train_test_split(x_total.T,y_total.T,test_size=0.2)\n",
    "\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        self.x=torch.from_numpy(x).float()\n",
    "        self.y=torch.from_numpy(y).float()\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "          idx=idx.tolist()\n",
    "        return self.x[idx],self.y[idx]\n",
    "    \n",
    "batch_size=512\n",
    "#512\n",
    "params={'batch_size': batch_size,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 0}\n",
    "train=Dataset(x_train,y_train)\n",
    "train_set=torch.utils.data.DataLoader(train,**params)\n",
    "val=Dataset(x_test,y_test)\n",
    "val_set=torch.utils.data.DataLoader(val,**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 708])\n",
      "Train Mean: 6.4420881271362305\n",
      "Train Std: 3.662130832672119\n",
      "Validation Mean: 6.458369731903076\n",
      "Validation Std: 3.657360792160034\n"
     ]
    }
   ],
   "source": [
    "# 資料分析\n",
    "# 從資料集中建立 DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 取第一個 batch 進行分析\n",
    "train_batch_x, _ = next(iter(train_loader))\n",
    "val_batch_x, _ = next(iter(val_loader))\n",
    "\n",
    "# 計算均值和標準差\n",
    "print(train_batch_x.shape)\n",
    "train_mean, train_std = train_batch_x.mean(dim=0), train_batch_x.std(dim=0)\n",
    "val_mean, val_std = val_batch_x.mean(dim=0), val_batch_x.std(dim=0)\n",
    "\n",
    "mean_value_train = train_mean.mean().item()\n",
    "print('Train Mean:', mean_value_train)\n",
    "std_value_train = train_std.mean().item()\n",
    "print('Train Std:', std_value_train)\n",
    "mean_value_val = val_mean.mean().item()\n",
    "print('Validation Mean:', mean_value_val)\n",
    "std_value_val = val_std.mean().item()\n",
    "print('Validation Std:', std_value_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "JN9gN9BnCNim"
   },
   "outputs": [],
   "source": [
    "# fig2 = plt.figure(figsize=(8,4))\n",
    "# flat_list = list(np.concatenate(y[:,n_sample:]).flat)\n",
    "# flat_list3 = list(np.concatenate(y[:,:n_sample]).flat)\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.hist(flat_list,bins = 100,label = 'voltage')\n",
    "# plt.subplot(1,2,2)\n",
    "# # plt.hist(flat_list3,range=[-2000, 2000],bins = 100,label = 'price')\n",
    "# plt.hist(flat_list3,bins = 100,label = 'price')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "iOyWpG_4yCtz"
   },
   "outputs": [],
   "source": [
    "\n",
    "class dnn(torch.nn.Module):\n",
    "  def __init__(self,shape):\n",
    "    super(dnn,self).__init__()\n",
    "    layers=[]\n",
    "    for idx in range(len(shape)-2):\n",
    "      layers.extend([\n",
    "        nn.Linear(shape[idx],shape[idx+1]),\n",
    "        nn.BatchNorm1d(shape[idx+1]),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "      ])\n",
    "    layers+=[nn.Linear(shape[-2],shape[-1])]\n",
    "    self.features=nn.Sequential(*layers)\n",
    "    for temp in self.features:\n",
    "      if type(temp)==nn.Linear:\n",
    "        torch.nn.init.normal_(temp.weight,mean=0,std=1)\n",
    "  def forward(self,x): return self.features(x)\n",
    "#net=dnn([n_bus*6,n_bus*10,n_bus*10,n_bus*10,n_bus*2]).to(device)\n",
    "#print('number of params: %d'%(sum(temp.numel() for temp in net.parameters() if temp.requires_grad)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_bus: 118\n",
      "Total Parameters: 5,935,636\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 590]         418,310\n",
      "       BatchNorm1d-2                  [-1, 590]           1,180\n",
      "              ReLU-3                  [-1, 590]               0\n",
      "           Dropout-4                  [-1, 590]               0\n",
      "            Linear-5                  [-1, 590]         348,690\n",
      "       BatchNorm1d-6                  [-1, 590]           1,180\n",
      "              ReLU-7                  [-1, 590]               0\n",
      "           Dropout-8                  [-1, 590]               0\n",
      "            Linear-9                 [-1, 1180]         697,380\n",
      "      BatchNorm1d-10                 [-1, 1180]           2,360\n",
      "             ReLU-11                 [-1, 1180]               0\n",
      "          Dropout-12                 [-1, 1180]               0\n",
      "           Linear-13                 [-1, 1180]       1,393,580\n",
      "      BatchNorm1d-14                 [-1, 1180]           2,360\n",
      "             ReLU-15                 [-1, 1180]               0\n",
      "          Dropout-16                 [-1, 1180]               0\n",
      "           Linear-17                 [-1, 1180]       1,393,580\n",
      "      BatchNorm1d-18                 [-1, 1180]           2,360\n",
      "             ReLU-19                 [-1, 1180]               0\n",
      "          Dropout-20                 [-1, 1180]               0\n",
      "           Linear-21                 [-1, 1180]       1,393,580\n",
      "      BatchNorm1d-22                 [-1, 1180]           2,360\n",
      "             ReLU-23                 [-1, 1180]               0\n",
      "          Dropout-24                 [-1, 1180]               0\n",
      "           Linear-25                  [-1, 236]         278,716\n",
      "================================================================\n",
      "Total params: 5,935,636\n",
      "Trainable params: 5,935,636\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.18\n",
      "Params size (MB): 22.64\n",
      "Estimated Total Size (MB): 22.83\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# class dnn(torch.nn.Module):\n",
    "#   def __init__(self,shape,dropout=0):\n",
    "#     super(dnn,self).__init__()\n",
    "#     layers=[]\n",
    "#     for idx in range(len(shape)-2):\n",
    "#       layers.extend([\n",
    "#         nn.Linear(shape[idx],shape[idx+1]),\n",
    "#         nn.ReLU(),\n",
    "#         nn.BatchNorm1d(shape[idx+1]),\n",
    "#         nn.Dropout(dropout)\n",
    "#       ])\n",
    "#     layers.append(nn.Linear(shape[-2],shape[-1]))\n",
    "#     self.features=nn.Sequential(*layers)\n",
    "#     # initialize\n",
    "#     for temp in self.features:\n",
    "#       if type(temp)==nn.Linear:\n",
    "#         torch.nn.init.normal_(temp.weight,mean=0,std=1)\n",
    "#   def forward(self,x):\n",
    "#     return self.features(x)\n",
    "# net=dnn([118*6,118*5,118*5,118*10,118*10,118*10,118*10,118]).to(device)\n",
    "# print('number of params: %d'%(sum(temp.numel() for temp in net.parameters() if temp.requires_grad)))\n",
    "from torchsummary import summary\n",
    "class dnn(torch.nn.Module):\n",
    "    def __init__(self, shape):\n",
    "        super(dnn, self).__init__()\n",
    "        layers = []\n",
    "        for idx in range(len(shape) - 2):\n",
    "            layers.extend([\n",
    "                nn.Linear(shape[idx], shape[idx+1]),\n",
    "                nn.BatchNorm1d(num_features=shape[idx+1]),  # 確保 num_features 正確\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5),\n",
    "            ])\n",
    "        layers.append(nn.Linear(shape[-2], shape[-1]))  # 最後一層\n",
    "        self.features = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # 保證 batch_size 在第一維\n",
    "        return self.features(x)\n",
    "\n",
    "# 測試模型\n",
    "print('n_bus:',n_bus)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net=dnn([n_bus*6,n_bus*10,n_bus*10,n_bus*10,n_bus*2]).to(device) #source code\n",
    "#net = dnn([n_bus*6, n_bus*5, n_bus*5, n_bus*10, n_bus*10, n_bus*10, n_bus*10, n_bus*2]).to(device)#my code\n",
    "\n",
    "\n",
    "# 打印總參數數量\n",
    "total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "\n",
    "summary(net, (1, n_bus*6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "GEQ-MDKOTqW1"
   },
   "outputs": [],
   "source": [
    "# threshold function for p_g\n",
    "class my_gen_pred_binary(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(my_gen_pred_binary,self).__init__()\n",
    "  def forward(self,x,thresh):\n",
    "    right_thresh=thresh.clone().detach().requires_grad_(True).double()\n",
    "    left_thresh=torch.tensor(0).double()\n",
    "    x=x.double()\n",
    "    output = torch.sigmoid(left_thresh - x)\n",
    "    output = torch.mul(output,left_thresh - x) + x\n",
    "    output = torch.sigmoid(output - right_thresh)\n",
    "    output = torch.mul(output,output - right_thresh) + right_thresh\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9mYCMszHaosA"
   },
   "outputs": [],
   "source": [
    "## params needed for S calculation\n",
    "# line parameters\n",
    "filename1 = root+'ieee118_lineloc.txt'\n",
    "filename2 = root+'ieee118_lineparams.txt'\n",
    "filename3 = root+'ieee118_Bmat.txt'\n",
    "# incidence info\n",
    "line_loc = pd.read_table(filename1,sep=',',header=None).to_numpy()\n",
    "# r, x, shunt, S_max\n",
    "line_params = pd.read_table(filename2,sep=',',header=None).to_numpy()\n",
    "B_mat=pd.read_table(filename3,sep=',',header=None).to_numpy()\n",
    "B_r = np.delete(B_mat,68,axis=0)\n",
    "B_r = np.delete(B_r,68,axis=1)\n",
    "Br_inv = np.linalg.inv(B_r)\n",
    "\n",
    "R_line = line_params[:,0].copy()\n",
    "X_line = line_params[:,1].copy()\n",
    "\n",
    "B_shunt = line_params[:,2].copy()\n",
    "\n",
    "Z_line = R_line + 1j * X_line \n",
    "Y_line = 1 / Z_line\n",
    "\n",
    "G_line = np.real(Y_line)\n",
    "B_line = np.imag(Y_line)\n",
    "\n",
    "# transformer indicator\n",
    "a = (R_line > 0).astype(int)\n",
    "\n",
    "# params to tensor and GPU\n",
    "G_line_tensor = torch.from_numpy(G_line).to(device) # conductance\n",
    "B_line_tensor = torch.from_numpy(B_line).to(device) # susceptance\n",
    "B_shunt_tensor = torch.from_numpy(B_shunt/2).to(device) # conductance\n",
    "Br_inv_tensor = torch.from_numpy(Br_inv).to(device) # reduced Bbus matrix\n",
    "a_tensor = torch.from_numpy(a).double().to(device) # line/transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Go4bwoEmoi0D"
   },
   "outputs": [],
   "source": [
    "class loss_func:\n",
    "    def __init__(self,s_max,G_line,B_line,B_shunt,Br_inv,a,line_loc):\n",
    "      self.s=s_max\n",
    "      self.g=G_line\n",
    "      self.b=B_line\n",
    "      self.c=B_shunt\n",
    "      self.r=Br_inv\n",
    "      self.a=a\n",
    "      self.mse=nn.MSELoss() # MSE loss\n",
    "      self.lmda1=torch.tensor(10).to(device) # V MSE \n",
    "      self.lmda2=torch.tensor(1).to(device) # pi MSE \n",
    "      self.lmda3=torch.tensor(0.1).to(device) # v l_inf\n",
    "      self.lmda4=torch.tensor(0.1).to(device) # s feasibility\n",
    "      self.lmda5=torch.tensor(0.01).to(device) # pi l_inf\n",
    "      self.line_loc=line_loc\n",
    "      self.binary_cell=my_gen_pred_binary()\n",
    "    def calc(self,pred,label,x,feas):\n",
    "      mse_p=self.mse(pred[:,:n_bus],label[:,:n_bus])\n",
    "      mse_v=self.mse(pred[:,n_bus:],label[:,n_bus:])\n",
    "      linf_p=(pred[:,:n_bus]-label[:,:n_bus]).norm(p=float('inf'))\n",
    "      linf_v=(pred[:,n_bus:]-label[:,n_bus:]).norm(p=float('inf'))\n",
    "      if feas==False:\n",
    "        return self.lmda1*mse_v+self.lmda2*mse_p+self.lmda3*linf_v+self.lmda5*linf_p\n",
    "      label_pred=pred[:,:n_bus]\n",
    "      p_max=x[:,:n_bus*1]-x[:,n_bus*1:n_bus*2]\n",
    "      quadratic_b=x[:,n_bus*4:n_bus*5]\n",
    "      quadratic_a=x[:,n_bus*5:n_bus*6]\n",
    "      quadratic_center=(label_pred-quadratic_b)/(quadratic_a+1e-5)/2\n",
    "      p_inj=self.binary_cell(quadratic_center,p_max)\n",
    "      bus_inj=p_inj+x[:,n_bus:n_bus*2]\n",
    "      p_inj_r=torch.cat((bus_inj[:,:68],bus_inj[:,69:]),1)/100\n",
    "      theta0=torch.matmul(self.r,p_inj_r.T)\n",
    "      ref_ang=torch.zeros(1,theta0.shape[1]).to(device)\n",
    "      theta=torch.cat([theta0[:68,:],ref_ang,theta0[68:,:]],0)\n",
    "      v_pred=(pred[:,n_bus:].transpose(0,1))*0.01+0.9\n",
    "      \n",
    "      # s penalty\n",
    "      theta1=theta[self.line_loc[:,0]-1,:]\n",
    "      theta2=theta[self.line_loc[:,1]-1,:]\n",
    "      V1=(v_pred[self.line_loc[:,0]-1,:]).double()\n",
    "      V2=(v_pred[self.line_loc[:,1]-1,:]).double()\n",
    "      f_p=(self.a*self.g*(V1*V1).T)-self.a*((V1*V2).T)*(self.g*torch.cos(theta1-theta2).T+self.b*torch.sin(theta1-theta2).T)\n",
    "      f_p=f_p.T\n",
    "      f_q=-self.a*(V1.T)*(self.a*V1.T)*(self.b+self.c/2)+self.a*((V1*V2).T)*(self.b*torch.cos(theta1-theta2).T-self.g*torch.sin(theta1-theta2).T)\n",
    "      f_q=f_q.T\n",
    "      s_pred=torch.sqrt(f_p*f_p+f_q*f_q+1e-5)*100\n",
    "      s_penalty=torch.sigmoid(s_pred-self.s)+torch.sigmoid(-s_pred-self.s)\n",
    "      s_total=torch.sum(s_penalty)\n",
    "\n",
    "      # sji penalty\n",
    "      theta1=theta[self.line_loc[:,1]-1,:]\n",
    "      theta2=theta[self.line_loc[:,0]-1,:]\n",
    "      V1=(v_pred[self.line_loc[:,1]-1,:]).double()\n",
    "      V2=(v_pred[self.line_loc[:,0]-1,:]).double() \n",
    "      fji_p=(self.a*self.g*(V1*V1).T)-self.a*((V1*V2).T)*(self.g*torch.cos(theta1-theta2).T+self.b*torch.sin(theta1-theta2).T)\n",
    "      fji_p=fji_p.T\n",
    "      fji_q=-self.a*(V1.T)*(self.a*V1.T)*(self.b+self.c/2)+self.a*((V1*V2).T)*(self.b*torch.cos(theta1-theta2).T-self.g*torch.sin(theta1-theta2).T)\n",
    "      fji_q=fji_q.T\n",
    "      sji_pred=torch.sqrt(fji_p*fji_p+fji_q*fji_q+1e-5)*100\n",
    "      sji_penalty=torch.sigmoid(sji_pred-self.s)+torch.sigmoid(-sji_pred-self.s)\n",
    "      sji_total=torch.sum(sji_penalty)\n",
    "\n",
    "      return self.lmda1*mse_v+self.lmda2*mse_p+self.lmda3*linf_v+self.lmda5*linf_p+self.lmda4*s_total+self.lmda4*sji_total\n",
    "my_loss=loss_func(f_max,G_line_tensor,B_line_tensor,B_shunt_tensor,Br_inv_tensor,a_tensor,line_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cold start\n",
      "Epoch 0 | Training loss: 403.3938 | Learning Rate: 0.001000\n",
      "Epoch 0 | Validation loss: 407.0385 | Learning Rate: 0.001000\n",
      "Epoch 1 | Training loss: 388.1000 | Learning Rate: 0.001000\n",
      "Epoch 2 | Training loss: 373.2499 | Learning Rate: 0.001000\n",
      "Epoch 3 | Training loss: 357.4983 | Learning Rate: 0.001000\n",
      "Epoch 4 | Training loss: 340.1894 | Learning Rate: 0.001000\n",
      "Epoch 5 | Training loss: 318.1922 | Learning Rate: 0.001000\n",
      "Epoch 5 | Validation loss: 257.5825 | Learning Rate: 0.001000\n",
      "Epoch 6 | Training loss: 287.9529 | Learning Rate: 0.001000\n",
      "Epoch 7 | Training loss: 261.5910 | Learning Rate: 0.001000\n",
      "Epoch 8 | Training loss: 237.3741 | Learning Rate: 0.001000\n",
      "Epoch 9 | Training loss: 213.4976 | Learning Rate: 0.001000\n",
      "Epoch 10 | Training loss: 190.5202 | Learning Rate: 0.001000\n",
      "Epoch 10 | Validation loss: 175.1895 | Learning Rate: 0.001000\n",
      "Epoch 11 | Training loss: 168.2565 | Learning Rate: 0.001000\n",
      "Epoch 12 | Training loss: 147.7542 | Learning Rate: 0.001000\n",
      "Epoch 13 | Training loss: 127.6928 | Learning Rate: 0.001000\n",
      "Epoch 14 | Training loss: 109.6045 | Learning Rate: 0.001000\n",
      "Epoch 15 | Training loss: 93.3204 | Learning Rate: 0.001000\n",
      "Epoch 15 | Validation loss: 92.0805 | Learning Rate: 0.001000\n",
      "Epoch 16 | Training loss: 78.4947 | Learning Rate: 0.001000\n",
      "Epoch 17 | Training loss: 65.1244 | Learning Rate: 0.001000\n",
      "Epoch 18 | Training loss: 53.4061 | Learning Rate: 0.001000\n",
      "Epoch 19 | Training loss: 43.2974 | Learning Rate: 0.001000\n",
      "Epoch 20 | Training loss: 34.5992 | Learning Rate: 0.001000\n",
      "Epoch 20 | Validation loss: 40.9256 | Learning Rate: 0.001000\n",
      "Epoch 21 | Training loss: 27.3372 | Learning Rate: 0.001000\n",
      "Epoch 22 | Training loss: 21.5753 | Learning Rate: 0.001000\n",
      "Epoch 23 | Training loss: 17.1014 | Learning Rate: 0.001000\n",
      "Epoch 24 | Training loss: 13.5269 | Learning Rate: 0.001000\n",
      "Epoch 25 | Training loss: 10.8117 | Learning Rate: 0.001000\n",
      "Epoch 25 | Validation loss: 15.9022 | Learning Rate: 0.001000\n",
      "Epoch 26 | Training loss: 8.6839 | Learning Rate: 0.001000\n",
      "Epoch 27 | Training loss: 7.0968 | Learning Rate: 0.001000\n",
      "Epoch 28 | Training loss: 5.9112 | Learning Rate: 0.001000\n",
      "Epoch 29 | Training loss: 4.9974 | Learning Rate: 0.001000\n",
      "Epoch 30 | Training loss: 4.3564 | Learning Rate: 0.001000\n",
      "Epoch 30 | Validation loss: 3.0326 | Learning Rate: 0.001000\n",
      "Epoch 31 | Training loss: 3.9246 | Learning Rate: 0.001000\n",
      "Epoch 32 | Training loss: 3.4728 | Learning Rate: 0.001000\n",
      "Epoch 33 | Training loss: 3.3571 | Learning Rate: 0.001000\n",
      "Epoch 34 | Training loss: 3.0662 | Learning Rate: 0.001000\n",
      "Epoch 35 | Training loss: 2.9052 | Learning Rate: 0.001000\n",
      "Epoch 35 | Validation loss: 2.0728 | Learning Rate: 0.001000\n",
      "Epoch 36 | Training loss: 2.8570 | Learning Rate: 0.001000\n",
      "Epoch 37 | Training loss: 2.7104 | Learning Rate: 0.001000\n",
      "Epoch 38 | Training loss: 2.5651 | Learning Rate: 0.001000\n",
      "Epoch 39 | Training loss: 2.5427 | Learning Rate: 0.001000\n",
      "Epoch 40 | Training loss: 2.4659 | Learning Rate: 0.001000\n",
      "Epoch 40 | Validation loss: 1.5592 | Learning Rate: 0.001000\n",
      "Epoch 41 | Training loss: 2.3551 | Learning Rate: 0.001000\n",
      "Epoch 42 | Training loss: 2.3056 | Learning Rate: 0.001000\n",
      "Epoch 43 | Training loss: 2.2713 | Learning Rate: 0.001000\n",
      "Epoch 44 | Training loss: 2.3038 | Learning Rate: 0.001000\n",
      "Epoch 45 | Training loss: 2.1766 | Learning Rate: 0.001000\n",
      "Epoch 45 | Validation loss: 1.4225 | Learning Rate: 0.001000\n",
      "Epoch 46 | Training loss: 2.2210 | Learning Rate: 0.001000\n",
      "Epoch 47 | Training loss: 2.1814 | Learning Rate: 0.001000\n",
      "Epoch 48 | Training loss: 2.1776 | Learning Rate: 0.001000\n",
      "Epoch 49 | Training loss: 2.1942 | Learning Rate: 0.001000\n",
      "Epoch 50 | Training loss: 2.1502 | Learning Rate: 0.001000\n",
      "Epoch 50 | Validation loss: 1.3386 | Learning Rate: 0.001000\n",
      "Epoch 51 | Training loss: 2.1943 | Learning Rate: 0.001000\n",
      "Epoch 52 | Training loss: 2.1176 | Learning Rate: 0.001000\n",
      "Epoch 53 | Training loss: 2.0945 | Learning Rate: 0.001000\n",
      "Epoch 54 | Training loss: 2.1045 | Learning Rate: 0.001000\n",
      "Epoch 55 | Training loss: 2.0730 | Learning Rate: 0.001000\n",
      "Epoch 55 | Validation loss: 1.3565 | Learning Rate: 0.001000\n",
      "Epoch 56 | Training loss: 2.1069 | Learning Rate: 0.001000\n",
      "Epoch 57 | Training loss: 2.1256 | Learning Rate: 0.001000\n",
      "Epoch 58 | Training loss: 1.9932 | Learning Rate: 0.001000\n",
      "Epoch 59 | Training loss: 2.0999 | Learning Rate: 0.001000\n",
      "Epoch 60 | Training loss: 2.1592 | Learning Rate: 0.001000\n",
      "Epoch 60 | Validation loss: 1.3327 | Learning Rate: 0.001000\n",
      "Epoch 61 | Training loss: 2.0260 | Learning Rate: 0.001000\n",
      "Epoch 62 | Training loss: 2.0274 | Learning Rate: 0.001000\n",
      "Epoch 63 | Training loss: 2.0596 | Learning Rate: 0.001000\n",
      "Epoch 64 | Training loss: 2.0127 | Learning Rate: 0.001000\n",
      "Epoch 65 | Training loss: 1.9966 | Learning Rate: 0.001000\n",
      "Epoch 65 | Validation loss: 1.3556 | Learning Rate: 0.001000\n",
      "Epoch 66 | Training loss: 2.0646 | Learning Rate: 0.001000\n",
      "Epoch 67 | Training loss: 2.0035 | Learning Rate: 0.001000\n",
      "Epoch 68 | Training loss: 2.0248 | Learning Rate: 0.001000\n",
      "Epoch 69 | Training loss: 1.9696 | Learning Rate: 0.001000\n",
      "Epoch 70 | Training loss: 1.9959 | Learning Rate: 0.001000\n",
      "Epoch 70 | Validation loss: 1.5805 | Learning Rate: 0.001000\n",
      "Epoch 71 | Training loss: 1.9962 | Learning Rate: 0.001000\n",
      "Epoch 72 | Training loss: 1.9921 | Learning Rate: 0.001000\n",
      "Epoch 73 | Training loss: 2.0224 | Learning Rate: 0.001000\n",
      "Epoch 74 | Training loss: 1.9834 | Learning Rate: 0.001000\n",
      "Epoch 75 | Training loss: 1.8922 | Learning Rate: 0.001000\n",
      "Epoch 75 | Validation loss: 1.3268 | Learning Rate: 0.001000\n",
      "Epoch 76 | Training loss: 1.9240 | Learning Rate: 0.001000\n",
      "Epoch 77 | Training loss: 1.9579 | Learning Rate: 0.001000\n",
      "Epoch 78 | Training loss: 1.9612 | Learning Rate: 0.001000\n",
      "Epoch 79 | Training loss: 1.9026 | Learning Rate: 0.001000\n",
      "Epoch 80 | Training loss: 1.9503 | Learning Rate: 0.001000\n",
      "Epoch 80 | Validation loss: 1.3415 | Learning Rate: 0.001000\n",
      "Epoch 81 | Training loss: 1.9708 | Learning Rate: 0.001000\n",
      "Epoch 82 | Training loss: 1.9918 | Learning Rate: 0.001000\n",
      "Epoch 83 | Training loss: 2.0452 | Learning Rate: 0.001000\n",
      "Epoch 84 | Training loss: 1.9267 | Learning Rate: 0.001000\n",
      "Epoch 85 | Training loss: 1.9266 | Learning Rate: 0.001000\n",
      "Epoch 85 | Validation loss: 1.3314 | Learning Rate: 0.001000\n",
      "Epoch 86 | Training loss: 1.9498 | Learning Rate: 0.001000\n",
      "Epoch 87 | Training loss: 1.8722 | Learning Rate: 0.001000\n",
      "Epoch 88 | Training loss: 1.9158 | Learning Rate: 0.001000\n",
      "Epoch 89 | Training loss: 1.9520 | Learning Rate: 0.001000\n",
      "Epoch 90 | Training loss: 1.9909 | Learning Rate: 0.001000\n",
      "Epoch 90 | Validation loss: 1.2932 | Learning Rate: 0.001000\n",
      "Epoch 91 | Training loss: 1.9264 | Learning Rate: 0.001000\n",
      "Epoch 92 | Training loss: 1.9068 | Learning Rate: 0.001000\n",
      "Epoch 93 | Training loss: 1.9003 | Learning Rate: 0.001000\n",
      "Epoch 94 | Training loss: 1.8952 | Learning Rate: 0.001000\n",
      "Epoch 95 | Training loss: 1.9031 | Learning Rate: 0.001000\n",
      "Epoch 95 | Validation loss: 1.4943 | Learning Rate: 0.001000\n",
      "Epoch 96 | Training loss: 1.9467 | Learning Rate: 0.001000\n",
      "Epoch 97 | Training loss: 1.9214 | Learning Rate: 0.001000\n",
      "Epoch 98 | Training loss: 1.9410 | Learning Rate: 0.001000\n",
      "Epoch 99 | Training loss: 1.8907 | Learning Rate: 0.001000\n",
      "Epoch 100 | Training loss: 1.8151 | Learning Rate: 0.001000\n",
      "Epoch 100 | Validation loss: 1.3220 | Learning Rate: 0.001000\n",
      "Epoch 101 | Training loss: 1.8735 | Learning Rate: 0.001000\n",
      "Epoch 102 | Training loss: 1.8214 | Learning Rate: 0.001000\n",
      "Epoch 103 | Training loss: 1.8694 | Learning Rate: 0.001000\n",
      "Epoch 104 | Training loss: 1.8672 | Learning Rate: 0.001000\n",
      "Epoch 105 | Training loss: 1.8936 | Learning Rate: 0.001000\n",
      "Epoch 105 | Validation loss: 1.3624 | Learning Rate: 0.001000\n",
      "Epoch 106 | Training loss: 1.8528 | Learning Rate: 0.001000\n",
      "Epoch 107 | Training loss: 1.8368 | Learning Rate: 0.001000\n",
      "Epoch 108 | Training loss: 1.8361 | Learning Rate: 0.001000\n",
      "Epoch 109 | Training loss: 1.8680 | Learning Rate: 0.001000\n",
      "Epoch 110 | Training loss: 1.8255 | Learning Rate: 0.001000\n",
      "Epoch 110 | Validation loss: 1.3724 | Learning Rate: 0.001000\n",
      "Epoch 111 | Training loss: 1.7647 | Learning Rate: 0.001000\n",
      "Epoch 112 | Training loss: 1.7624 | Learning Rate: 0.001000\n",
      "Epoch 113 | Training loss: 1.7033 | Learning Rate: 0.001000\n",
      "Epoch 114 | Training loss: 1.7637 | Learning Rate: 0.001000\n",
      "Epoch 115 | Training loss: 1.8612 | Learning Rate: 0.001000\n",
      "Epoch 115 | Validation loss: 1.4985 | Learning Rate: 0.001000\n",
      "Epoch 116 | Training loss: 1.7830 | Learning Rate: 0.001000\n",
      "Epoch 117 | Training loss: 1.7510 | Learning Rate: 0.001000\n",
      "Epoch 118 | Training loss: 1.7420 | Learning Rate: 0.001000\n",
      "Epoch 119 | Training loss: 1.7327 | Learning Rate: 0.001000\n",
      "Epoch 120 | Training loss: 1.7705 | Learning Rate: 0.001000\n",
      "Epoch 120 | Validation loss: 1.2549 | Learning Rate: 0.001000\n",
      "Epoch 121 | Training loss: 1.7257 | Learning Rate: 0.001000\n",
      "Epoch 122 | Training loss: 1.7232 | Learning Rate: 0.001000\n",
      "Epoch 123 | Training loss: 1.7449 | Learning Rate: 0.001000\n",
      "Epoch 124 | Training loss: 1.7001 | Learning Rate: 0.001000\n",
      "Epoch 125 | Training loss: 1.6939 | Learning Rate: 0.001000\n",
      "Epoch 125 | Validation loss: 2.3127 | Learning Rate: 0.001000\n",
      "Epoch 126 | Training loss: 1.6773 | Learning Rate: 0.001000\n",
      "Epoch 127 | Training loss: 1.7385 | Learning Rate: 0.001000\n",
      "Epoch 128 | Training loss: 1.7378 | Learning Rate: 0.001000\n",
      "Epoch 129 | Training loss: 1.7382 | Learning Rate: 0.001000\n",
      "Epoch 130 | Training loss: 1.7126 | Learning Rate: 0.001000\n",
      "Epoch 130 | Validation loss: 1.2760 | Learning Rate: 0.001000\n",
      "Epoch 131 | Training loss: 1.6521 | Learning Rate: 0.001000\n",
      "Epoch 132 | Training loss: 1.6786 | Learning Rate: 0.001000\n",
      "Epoch 133 | Training loss: 1.6594 | Learning Rate: 0.001000\n",
      "Epoch 134 | Training loss: 1.6547 | Learning Rate: 0.001000\n",
      "Epoch 135 | Training loss: 1.6539 | Learning Rate: 0.001000\n",
      "Epoch 135 | Validation loss: 1.5972 | Learning Rate: 0.001000\n",
      "Epoch 136 | Training loss: 1.6893 | Learning Rate: 0.001000\n",
      "Epoch 137 | Training loss: 1.6567 | Learning Rate: 0.001000\n",
      "Epoch 138 | Training loss: 1.6183 | Learning Rate: 0.001000\n",
      "Epoch 139 | Training loss: 1.6389 | Learning Rate: 0.001000\n",
      "Epoch 140 | Training loss: 1.6194 | Learning Rate: 0.001000\n",
      "Epoch 140 | Validation loss: 2.3364 | Learning Rate: 0.001000\n",
      "Epoch 141 | Training loss: 1.5394 | Learning Rate: 0.001000\n",
      "Epoch 142 | Training loss: 1.6088 | Learning Rate: 0.001000\n",
      "Epoch 143 | Training loss: 1.6325 | Learning Rate: 0.001000\n",
      "Epoch 144 | Training loss: 1.6408 | Learning Rate: 0.001000\n",
      "Epoch 145 | Training loss: 1.6184 | Learning Rate: 0.001000\n",
      "Epoch 145 | Validation loss: 2.2610 | Learning Rate: 0.001000\n",
      "Epoch 146 | Training loss: 1.5812 | Learning Rate: 0.001000\n",
      "Epoch 147 | Training loss: 1.6125 | Learning Rate: 0.001000\n",
      "Epoch 148 | Training loss: 1.6393 | Learning Rate: 0.001000\n",
      "Epoch 149 | Training loss: 1.5957 | Learning Rate: 0.001000\n",
      "Epoch 150 | Training loss: 1.5402 | Learning Rate: 0.001000\n",
      "Epoch 150 | Validation loss: 1.4573 | Learning Rate: 0.001000\n",
      "Epoch 151 | Training loss: 1.5811 | Learning Rate: 0.001000\n",
      "Epoch 152 | Training loss: 1.5923 | Learning Rate: 0.001000\n",
      "Epoch 153 | Training loss: 1.5992 | Learning Rate: 0.001000\n",
      "Epoch 154 | Training loss: 1.5965 | Learning Rate: 0.001000\n",
      "Epoch 155 | Training loss: 1.6203 | Learning Rate: 0.001000\n",
      "Epoch 155 | Validation loss: 1.8645 | Learning Rate: 0.001000\n",
      "Epoch 156 | Training loss: 1.6533 | Learning Rate: 0.001000\n",
      "Epoch 157 | Training loss: 1.5760 | Learning Rate: 0.001000\n",
      "Epoch 158 | Training loss: 1.5382 | Learning Rate: 0.001000\n",
      "Epoch 159 | Training loss: 1.5805 | Learning Rate: 0.001000\n",
      "Epoch 160 | Training loss: 1.5605 | Learning Rate: 0.001000\n",
      "Epoch 160 | Validation loss: 1.3244 | Learning Rate: 0.001000\n",
      "Epoch 161 | Training loss: 1.5430 | Learning Rate: 0.001000\n",
      "Epoch 162 | Training loss: 1.5127 | Learning Rate: 0.001000\n",
      "Epoch 163 | Training loss: 1.5297 | Learning Rate: 0.001000\n",
      "Epoch 164 | Training loss: 1.4999 | Learning Rate: 0.001000\n",
      "Epoch 165 | Training loss: 1.4920 | Learning Rate: 0.001000\n",
      "Epoch 165 | Validation loss: 1.7818 | Learning Rate: 0.001000\n",
      "Epoch 166 | Training loss: 1.5298 | Learning Rate: 0.001000\n",
      "Epoch 167 | Training loss: 1.5222 | Learning Rate: 0.001000\n",
      "Epoch 168 | Training loss: 1.4571 | Learning Rate: 0.001000\n",
      "Epoch 169 | Training loss: 1.4838 | Learning Rate: 0.001000\n",
      "Epoch 170 | Training loss: 1.4641 | Learning Rate: 0.001000\n",
      "Epoch 170 | Validation loss: 1.3354 | Learning Rate: 0.001000\n",
      "Epoch 171 | Training loss: 1.4914 | Learning Rate: 0.001000\n",
      "Epoch 172 | Training loss: 1.4222 | Learning Rate: 0.001000\n",
      "Epoch 173 | Training loss: 1.4770 | Learning Rate: 0.001000\n",
      "Epoch 174 | Training loss: 1.4999 | Learning Rate: 0.001000\n",
      "Epoch 175 | Training loss: 1.4465 | Learning Rate: 0.001000\n",
      "Epoch 175 | Validation loss: 1.8408 | Learning Rate: 0.001000\n",
      "Epoch 176 | Training loss: 1.4235 | Learning Rate: 0.001000\n",
      "Epoch 177 | Training loss: 1.4768 | Learning Rate: 0.001000\n",
      "Epoch 178 | Training loss: 1.4604 | Learning Rate: 0.001000\n",
      "Epoch 179 | Training loss: 1.4784 | Learning Rate: 0.001000\n",
      "Epoch 180 | Training loss: 1.4332 | Learning Rate: 0.001000\n",
      "Epoch 180 | Validation loss: 3.1682 | Learning Rate: 0.001000\n",
      "Epoch 181 | Training loss: 1.4173 | Learning Rate: 0.001000\n",
      "Epoch 182 | Training loss: 1.4429 | Learning Rate: 0.001000\n",
      "Epoch 183 | Training loss: 1.4273 | Learning Rate: 0.001000\n",
      "Epoch 184 | Training loss: 1.4232 | Learning Rate: 0.001000\n",
      "Epoch 185 | Training loss: 1.4338 | Learning Rate: 0.001000\n",
      "Epoch 185 | Validation loss: 1.7667 | Learning Rate: 0.001000\n",
      "Epoch 186 | Training loss: 1.4095 | Learning Rate: 0.001000\n",
      "Epoch 187 | Training loss: 1.3993 | Learning Rate: 0.001000\n",
      "Epoch 188 | Training loss: 1.4421 | Learning Rate: 0.001000\n",
      "Epoch 189 | Training loss: 1.4356 | Learning Rate: 0.001000\n",
      "Epoch 190 | Training loss: 1.4680 | Learning Rate: 0.001000\n",
      "Epoch 190 | Validation loss: 1.5285 | Learning Rate: 0.001000\n",
      "Epoch 191 | Training loss: 1.4271 | Learning Rate: 0.001000\n",
      "Epoch 192 | Training loss: 1.3765 | Learning Rate: 0.001000\n",
      "Epoch 193 | Training loss: 1.3969 | Learning Rate: 0.001000\n",
      "Epoch 194 | Training loss: 1.4287 | Learning Rate: 0.001000\n",
      "Epoch 195 | Training loss: 1.4668 | Learning Rate: 0.001000\n",
      "Epoch 195 | Validation loss: 1.6443 | Learning Rate: 0.001000\n",
      "Epoch 196 | Training loss: 1.3671 | Learning Rate: 0.001000\n",
      "Epoch 197 | Training loss: 1.3780 | Learning Rate: 0.001000\n",
      "Epoch 198 | Training loss: 1.3999 | Learning Rate: 0.001000\n",
      "Epoch 199 | Training loss: 1.3569 | Learning Rate: 0.001000\n",
      "Epoch 200 | Training loss: 1.3781 | Learning Rate: 0.001000\n",
      "Epoch 200 | Validation loss: 3.5654 | Learning Rate: 0.001000\n",
      "Epoch 201 | Training loss: 1.4120 | Learning Rate: 0.001000\n",
      "Epoch 202 | Training loss: 1.3823 | Learning Rate: 0.001000\n",
      "Epoch 203 | Training loss: 1.3478 | Learning Rate: 0.001000\n",
      "Epoch 204 | Training loss: 1.3635 | Learning Rate: 0.001000\n",
      "Epoch 205 | Training loss: 1.3740 | Learning Rate: 0.001000\n",
      "Epoch 205 | Validation loss: 3.5534 | Learning Rate: 0.001000\n",
      "Epoch 206 | Training loss: 1.3343 | Learning Rate: 0.001000\n",
      "Epoch 207 | Training loss: 1.3264 | Learning Rate: 0.001000\n",
      "Epoch 208 | Training loss: 1.3317 | Learning Rate: 0.001000\n",
      "Epoch 209 | Training loss: 1.3503 | Learning Rate: 0.001000\n",
      "Epoch 210 | Training loss: 1.3503 | Learning Rate: 0.001000\n",
      "Epoch 210 | Validation loss: 1.9511 | Learning Rate: 0.001000\n",
      "Epoch 211 | Training loss: 1.3433 | Learning Rate: 0.001000\n",
      "Epoch 212 | Training loss: 1.3243 | Learning Rate: 0.001000\n",
      "Epoch 213 | Training loss: 1.3398 | Learning Rate: 0.001000\n",
      "Epoch 214 | Training loss: 1.2966 | Learning Rate: 0.001000\n",
      "Epoch 215 | Training loss: 1.3277 | Learning Rate: 0.001000\n",
      "Epoch 215 | Validation loss: 1.8673 | Learning Rate: 0.001000\n",
      "Epoch 216 | Training loss: 1.3101 | Learning Rate: 0.001000\n",
      "Epoch 217 | Training loss: 1.3078 | Learning Rate: 0.001000\n",
      "Epoch 218 | Training loss: 1.2888 | Learning Rate: 0.001000\n",
      "Epoch 219 | Training loss: 1.3011 | Learning Rate: 0.001000\n",
      "Epoch 220 | Training loss: 1.3172 | Learning Rate: 0.001000\n",
      "Epoch 220 | Validation loss: 1.3054 | Learning Rate: 0.001000\n",
      "Epoch 221 | Training loss: 1.3161 | Learning Rate: 0.001000\n",
      "Epoch 222 | Training loss: 1.3078 | Learning Rate: 0.001000\n",
      "Epoch 223 | Training loss: 1.2859 | Learning Rate: 0.001000\n",
      "Epoch 224 | Training loss: 1.3305 | Learning Rate: 0.001000\n",
      "Epoch 225 | Training loss: 1.2742 | Learning Rate: 0.001000\n",
      "Epoch 225 | Validation loss: 3.3321 | Learning Rate: 0.001000\n",
      "Epoch 226 | Training loss: 1.3427 | Learning Rate: 0.001000\n",
      "Epoch 227 | Training loss: 1.3756 | Learning Rate: 0.001000\n",
      "Epoch 228 | Training loss: 1.3238 | Learning Rate: 0.001000\n",
      "Epoch 229 | Training loss: 1.3247 | Learning Rate: 0.001000\n",
      "Epoch 230 | Training loss: 1.3106 | Learning Rate: 0.001000\n",
      "Epoch 230 | Validation loss: 1.2854 | Learning Rate: 0.001000\n",
      "Epoch 231 | Training loss: 1.2545 | Learning Rate: 0.001000\n",
      "Epoch 232 | Training loss: 1.3164 | Learning Rate: 0.001000\n",
      "Epoch 233 | Training loss: 1.3262 | Learning Rate: 0.001000\n",
      "Epoch 234 | Training loss: 1.3049 | Learning Rate: 0.001000\n",
      "Epoch 235 | Training loss: 1.2422 | Learning Rate: 0.001000\n",
      "Epoch 235 | Validation loss: 1.0984 | Learning Rate: 0.001000\n",
      "Epoch 236 | Training loss: 1.2587 | Learning Rate: 0.001000\n",
      "Epoch 237 | Training loss: 1.2539 | Learning Rate: 0.001000\n",
      "Epoch 238 | Training loss: 1.2530 | Learning Rate: 0.001000\n",
      "Epoch 239 | Training loss: 1.2850 | Learning Rate: 0.001000\n",
      "Epoch 240 | Training loss: 1.2226 | Learning Rate: 0.001000\n",
      "Epoch 240 | Validation loss: 2.0583 | Learning Rate: 0.001000\n",
      "Epoch 241 | Training loss: 1.2484 | Learning Rate: 0.001000\n",
      "Epoch 242 | Training loss: 1.2509 | Learning Rate: 0.001000\n",
      "Epoch 243 | Training loss: 1.2248 | Learning Rate: 0.001000\n",
      "Epoch 244 | Training loss: 1.2354 | Learning Rate: 0.001000\n",
      "Epoch 245 | Training loss: 1.2197 | Learning Rate: 0.001000\n",
      "Epoch 245 | Validation loss: 1.0559 | Learning Rate: 0.001000\n",
      "Epoch 246 | Training loss: 1.2324 | Learning Rate: 0.001000\n",
      "Epoch 247 | Training loss: 1.2695 | Learning Rate: 0.001000\n",
      "Epoch 248 | Training loss: 1.2480 | Learning Rate: 0.001000\n",
      "Epoch 249 | Training loss: 1.2464 | Learning Rate: 0.001000\n",
      "Epoch 250 | Training loss: 1.1979 | Learning Rate: 0.001000\n",
      "Epoch 250 | Validation loss: 2.2298 | Learning Rate: 0.001000\n",
      "Epoch 251 | Training loss: 1.2308 | Learning Rate: 0.001000\n",
      "Epoch 252 | Training loss: 1.2218 | Learning Rate: 0.001000\n",
      "Epoch 253 | Training loss: 1.2149 | Learning Rate: 0.001000\n",
      "Epoch 254 | Training loss: 1.2325 | Learning Rate: 0.001000\n",
      "Epoch 255 | Training loss: 1.2482 | Learning Rate: 0.001000\n",
      "Epoch 255 | Validation loss: 1.6646 | Learning Rate: 0.001000\n",
      "Epoch 256 | Training loss: 1.2355 | Learning Rate: 0.001000\n",
      "Epoch 257 | Training loss: 1.2174 | Learning Rate: 0.001000\n",
      "Epoch 258 | Training loss: 1.2294 | Learning Rate: 0.001000\n",
      "Epoch 259 | Training loss: 1.2348 | Learning Rate: 0.001000\n",
      "Epoch 260 | Training loss: 1.2120 | Learning Rate: 0.001000\n",
      "Epoch 260 | Validation loss: 2.6786 | Learning Rate: 0.001000\n",
      "Epoch 261 | Training loss: 1.1926 | Learning Rate: 0.001000\n",
      "Epoch 262 | Training loss: 1.1802 | Learning Rate: 0.001000\n",
      "Epoch 263 | Training loss: 1.2039 | Learning Rate: 0.001000\n",
      "Epoch 264 | Training loss: 1.2478 | Learning Rate: 0.001000\n",
      "Epoch 265 | Training loss: 1.2152 | Learning Rate: 0.001000\n",
      "Epoch 265 | Validation loss: 1.2902 | Learning Rate: 0.001000\n",
      "Epoch 266 | Training loss: 1.2625 | Learning Rate: 0.001000\n",
      "Epoch 267 | Training loss: 1.2035 | Learning Rate: 0.001000\n",
      "Epoch 268 | Training loss: 1.2021 | Learning Rate: 0.001000\n",
      "Epoch 269 | Training loss: 1.2200 | Learning Rate: 0.001000\n",
      "Epoch 270 | Training loss: 1.2127 | Learning Rate: 0.001000\n",
      "Epoch 270 | Validation loss: 1.7003 | Learning Rate: 0.001000\n",
      "Epoch 271 | Training loss: 1.2023 | Learning Rate: 0.001000\n",
      "Epoch 272 | Training loss: 1.2067 | Learning Rate: 0.001000\n",
      "Epoch 273 | Training loss: 1.1871 | Learning Rate: 0.001000\n",
      "Epoch 274 | Training loss: 1.1719 | Learning Rate: 0.001000\n",
      "Epoch 275 | Training loss: 1.1885 | Learning Rate: 0.001000\n",
      "Epoch 275 | Validation loss: 1.5473 | Learning Rate: 0.001000\n",
      "Epoch 276 | Training loss: 1.1857 | Learning Rate: 0.001000\n",
      "Epoch 277 | Training loss: 1.1642 | Learning Rate: 0.001000\n",
      "Epoch 278 | Training loss: 1.1754 | Learning Rate: 0.001000\n",
      "Epoch 279 | Training loss: 1.2033 | Learning Rate: 0.001000\n",
      "Epoch 280 | Training loss: 1.1640 | Learning Rate: 0.001000\n",
      "Epoch 280 | Validation loss: 1.7487 | Learning Rate: 0.001000\n",
      "Epoch 281 | Training loss: 1.1576 | Learning Rate: 0.001000\n",
      "Epoch 282 | Training loss: 1.1577 | Learning Rate: 0.001000\n",
      "Epoch 283 | Training loss: 1.1722 | Learning Rate: 0.001000\n",
      "Epoch 284 | Training loss: 1.1798 | Learning Rate: 0.001000\n",
      "Epoch 285 | Training loss: 1.2085 | Learning Rate: 0.001000\n",
      "Epoch 285 | Validation loss: 1.1499 | Learning Rate: 0.001000\n",
      "Epoch 286 | Training loss: 1.1704 | Learning Rate: 0.001000\n",
      "Epoch 287 | Training loss: 1.1756 | Learning Rate: 0.001000\n",
      "Epoch 288 | Training loss: 1.1610 | Learning Rate: 0.001000\n",
      "Epoch 289 | Training loss: 1.1538 | Learning Rate: 0.001000\n",
      "Epoch 290 | Training loss: 1.1747 | Learning Rate: 0.001000\n",
      "Epoch 290 | Validation loss: 1.3314 | Learning Rate: 0.001000\n",
      "Epoch 291 | Training loss: 1.1569 | Learning Rate: 0.001000\n",
      "Epoch 292 | Training loss: 1.1720 | Learning Rate: 0.001000\n",
      "Epoch 293 | Training loss: 1.1899 | Learning Rate: 0.001000\n",
      "Epoch 294 | Training loss: 1.1459 | Learning Rate: 0.001000\n",
      "Epoch 295 | Training loss: 1.1738 | Learning Rate: 0.001000\n",
      "Epoch 295 | Validation loss: 1.1940 | Learning Rate: 0.001000\n",
      "Epoch 296 | Training loss: 1.1770 | Learning Rate: 0.001000\n",
      "Epoch 297 | Training loss: 1.1346 | Learning Rate: 0.001000\n",
      "Epoch 298 | Training loss: 1.1309 | Learning Rate: 0.001000\n",
      "Epoch 299 | Training loss: 1.1349 | Learning Rate: 0.001000\n",
      "Epoch 300 | Training loss: 1.1192 | Learning Rate: 0.001000\n",
      "Epoch 300 | Validation loss: 2.1198 | Learning Rate: 0.001000\n",
      "Epoch 301 | Training loss: 1.1824 | Learning Rate: 0.001000\n",
      "Epoch 302 | Training loss: 1.1567 | Learning Rate: 0.001000\n",
      "Epoch 303 | Training loss: 1.1341 | Learning Rate: 0.001000\n",
      "Epoch 304 | Training loss: 1.1507 | Learning Rate: 0.001000\n",
      "Epoch 305 | Training loss: 1.1216 | Learning Rate: 0.001000\n",
      "Epoch 305 | Validation loss: 1.5884 | Learning Rate: 0.001000\n",
      "Epoch 306 | Training loss: 1.1364 | Learning Rate: 0.001000\n",
      "Epoch 307 | Training loss: 1.1678 | Learning Rate: 0.001000\n",
      "Epoch 308 | Training loss: 1.1203 | Learning Rate: 0.001000\n",
      "Epoch 309 | Training loss: 1.1259 | Learning Rate: 0.001000\n",
      "Epoch 310 | Training loss: 1.1669 | Learning Rate: 0.001000\n",
      "Epoch 310 | Validation loss: 6.7645 | Learning Rate: 0.001000\n",
      "Epoch 311 | Training loss: 1.1343 | Learning Rate: 0.001000\n",
      "Epoch 312 | Training loss: 1.1318 | Learning Rate: 0.001000\n",
      "Epoch 313 | Training loss: 1.1774 | Learning Rate: 0.001000\n",
      "Epoch 314 | Training loss: 1.1578 | Learning Rate: 0.001000\n",
      "Epoch 315 | Training loss: 1.1553 | Learning Rate: 0.001000\n",
      "Epoch 315 | Validation loss: 5.1283 | Learning Rate: 0.001000\n",
      "Epoch 316 | Training loss: 1.1514 | Learning Rate: 0.001000\n",
      "Epoch 317 | Training loss: 1.1158 | Learning Rate: 0.001000\n",
      "Epoch 318 | Training loss: 1.1118 | Learning Rate: 0.001000\n",
      "Epoch 319 | Training loss: 1.0938 | Learning Rate: 0.001000\n",
      "Epoch 320 | Training loss: 1.1193 | Learning Rate: 0.001000\n",
      "Epoch 320 | Validation loss: 1.0353 | Learning Rate: 0.001000\n",
      "Epoch 321 | Training loss: 1.1032 | Learning Rate: 0.001000\n",
      "Epoch 322 | Training loss: 1.1348 | Learning Rate: 0.001000\n",
      "Epoch 323 | Training loss: 1.1737 | Learning Rate: 0.001000\n",
      "Epoch 324 | Training loss: 1.1149 | Learning Rate: 0.001000\n",
      "Epoch 325 | Training loss: 1.1201 | Learning Rate: 0.001000\n",
      "Epoch 325 | Validation loss: 1.0787 | Learning Rate: 0.001000\n",
      "Epoch 326 | Training loss: 1.0694 | Learning Rate: 0.001000\n",
      "Epoch 327 | Training loss: 1.0960 | Learning Rate: 0.001000\n",
      "Epoch 328 | Training loss: 1.1179 | Learning Rate: 0.001000\n",
      "Epoch 329 | Training loss: 1.1273 | Learning Rate: 0.001000\n",
      "Epoch 330 | Training loss: 1.0730 | Learning Rate: 0.001000\n",
      "Epoch 330 | Validation loss: 1.1627 | Learning Rate: 0.001000\n",
      "Epoch 331 | Training loss: 1.0793 | Learning Rate: 0.001000\n",
      "Epoch 332 | Training loss: 1.0640 | Learning Rate: 0.001000\n",
      "Epoch 333 | Training loss: 1.0873 | Learning Rate: 0.001000\n",
      "Epoch 334 | Training loss: 1.1121 | Learning Rate: 0.001000\n",
      "Epoch 335 | Training loss: 1.0785 | Learning Rate: 0.001000\n",
      "Epoch 335 | Validation loss: 1.4835 | Learning Rate: 0.001000\n",
      "Epoch 336 | Training loss: 1.0811 | Learning Rate: 0.001000\n",
      "Epoch 337 | Training loss: 1.0845 | Learning Rate: 0.001000\n",
      "Epoch 338 | Training loss: 1.1298 | Learning Rate: 0.001000\n",
      "Epoch 339 | Training loss: 1.1257 | Learning Rate: 0.001000\n",
      "Epoch 340 | Training loss: 1.0872 | Learning Rate: 0.001000\n",
      "Epoch 340 | Validation loss: 1.3706 | Learning Rate: 0.001000\n",
      "Epoch 341 | Training loss: 1.0919 | Learning Rate: 0.001000\n",
      "Epoch 342 | Training loss: 1.0891 | Learning Rate: 0.001000\n",
      "Epoch 343 | Training loss: 1.0703 | Learning Rate: 0.001000\n",
      "Epoch 344 | Training loss: 1.0629 | Learning Rate: 0.001000\n",
      "Epoch 345 | Training loss: 1.1022 | Learning Rate: 0.001000\n",
      "Epoch 345 | Validation loss: 1.0200 | Learning Rate: 0.001000\n",
      "Epoch 346 | Training loss: 1.0880 | Learning Rate: 0.001000\n",
      "Epoch 347 | Training loss: 1.0795 | Learning Rate: 0.001000\n",
      "Epoch 348 | Training loss: 1.0480 | Learning Rate: 0.001000\n",
      "Epoch 349 | Training loss: 1.0735 | Learning Rate: 0.001000\n",
      "Epoch 350 | Training loss: 1.0800 | Learning Rate: 0.001000\n",
      "Epoch 350 | Validation loss: 1.1789 | Learning Rate: 0.001000\n",
      "Epoch 351 | Training loss: 1.1165 | Learning Rate: 0.001000\n",
      "Epoch 352 | Training loss: 1.0838 | Learning Rate: 0.001000\n",
      "Epoch 353 | Training loss: 1.0808 | Learning Rate: 0.001000\n",
      "Epoch 354 | Training loss: 1.0721 | Learning Rate: 0.001000\n",
      "Epoch 355 | Training loss: 1.0660 | Learning Rate: 0.001000\n",
      "Epoch 355 | Validation loss: 1.1336 | Learning Rate: 0.001000\n",
      "Epoch 356 | Training loss: 1.0587 | Learning Rate: 0.001000\n",
      "Epoch 357 | Training loss: 1.0492 | Learning Rate: 0.001000\n",
      "Epoch 358 | Training loss: 1.0730 | Learning Rate: 0.001000\n",
      "Epoch 359 | Training loss: 1.0744 | Learning Rate: 0.001000\n",
      "Epoch 360 | Training loss: 1.0786 | Learning Rate: 0.001000\n",
      "Epoch 360 | Validation loss: 1.0407 | Learning Rate: 0.001000\n",
      "Epoch 361 | Training loss: 1.0551 | Learning Rate: 0.001000\n",
      "Epoch 362 | Training loss: 1.0616 | Learning Rate: 0.001000\n",
      "Epoch 363 | Training loss: 1.0812 | Learning Rate: 0.001000\n",
      "Epoch 364 | Training loss: 1.0523 | Learning Rate: 0.001000\n",
      "Epoch 365 | Training loss: 1.0559 | Learning Rate: 0.001000\n",
      "Epoch 365 | Validation loss: 2.2222 | Learning Rate: 0.001000\n",
      "Epoch 366 | Training loss: 1.0519 | Learning Rate: 0.001000\n",
      "Epoch 367 | Training loss: 1.0216 | Learning Rate: 0.001000\n",
      "Epoch 368 | Training loss: 1.0494 | Learning Rate: 0.001000\n",
      "Epoch 369 | Training loss: 1.0500 | Learning Rate: 0.001000\n",
      "Epoch 370 | Training loss: 1.0683 | Learning Rate: 0.001000\n",
      "Epoch 370 | Validation loss: 1.0289 | Learning Rate: 0.001000\n",
      "Epoch 371 | Training loss: 1.0284 | Learning Rate: 0.001000\n",
      "Epoch 372 | Training loss: 1.0976 | Learning Rate: 0.001000\n",
      "Epoch 373 | Training loss: 1.0947 | Learning Rate: 0.001000\n",
      "Epoch 374 | Training loss: 1.0374 | Learning Rate: 0.001000\n",
      "Epoch 375 | Training loss: 1.0389 | Learning Rate: 0.001000\n",
      "Epoch 375 | Validation loss: 1.0513 | Learning Rate: 0.001000\n",
      "Epoch 376 | Training loss: 1.0810 | Learning Rate: 0.001000\n",
      "Epoch 377 | Training loss: 1.0254 | Learning Rate: 0.001000\n",
      "Epoch 378 | Training loss: 1.0548 | Learning Rate: 0.001000\n",
      "Epoch 379 | Training loss: 1.0616 | Learning Rate: 0.001000\n",
      "Epoch 380 | Training loss: 1.0733 | Learning Rate: 0.001000\n",
      "Epoch 380 | Validation loss: 1.1898 | Learning Rate: 0.001000\n",
      "Epoch 381 | Training loss: 1.0921 | Learning Rate: 0.001000\n",
      "Epoch 382 | Training loss: 1.0838 | Learning Rate: 0.001000\n",
      "Epoch 383 | Training loss: 1.0726 | Learning Rate: 0.001000\n",
      "Epoch 384 | Training loss: 1.0580 | Learning Rate: 0.001000\n",
      "Epoch 385 | Training loss: 1.0814 | Learning Rate: 0.001000\n",
      "Epoch 385 | Validation loss: 1.7856 | Learning Rate: 0.001000\n",
      "Epoch 386 | Training loss: 1.0179 | Learning Rate: 0.001000\n",
      "Epoch 387 | Training loss: 1.0392 | Learning Rate: 0.001000\n",
      "Epoch 388 | Training loss: 1.0672 | Learning Rate: 0.001000\n",
      "Epoch 389 | Training loss: 1.0376 | Learning Rate: 0.001000\n",
      "Epoch 390 | Training loss: 1.0463 | Learning Rate: 0.001000\n",
      "Epoch 390 | Validation loss: 3.4493 | Learning Rate: 0.001000\n",
      "Epoch 391 | Training loss: 1.0163 | Learning Rate: 0.001000\n",
      "Epoch 392 | Training loss: 1.0047 | Learning Rate: 0.001000\n",
      "Epoch 393 | Training loss: 1.0681 | Learning Rate: 0.001000\n",
      "Epoch 394 | Training loss: 1.0433 | Learning Rate: 0.001000\n",
      "Epoch 395 | Training loss: 1.0503 | Learning Rate: 0.001000\n",
      "Epoch 395 | Validation loss: 1.0704 | Learning Rate: 0.001000\n",
      "Epoch 396 | Training loss: 1.0203 | Learning Rate: 0.001000\n",
      "Epoch 397 | Training loss: 1.0334 | Learning Rate: 0.001000\n",
      "Epoch 398 | Training loss: 1.0412 | Learning Rate: 0.001000\n",
      "Epoch 399 | Training loss: 1.0705 | Learning Rate: 0.001000\n",
      "Epoch 400 | Training loss: 1.0391 | Learning Rate: 0.001000\n",
      "Epoch 400 | Validation loss: 1.4367 | Learning Rate: 0.001000\n",
      "Epoch 401 | Training loss: 1.0834 | Learning Rate: 0.001000\n",
      "Epoch 402 | Training loss: 1.0361 | Learning Rate: 0.001000\n",
      "Epoch 403 | Training loss: 1.0232 | Learning Rate: 0.001000\n",
      "Epoch 404 | Training loss: 1.0611 | Learning Rate: 0.001000\n",
      "Epoch 405 | Training loss: 1.0541 | Learning Rate: 0.001000\n",
      "Epoch 405 | Validation loss: 1.2983 | Learning Rate: 0.001000\n",
      "Epoch 406 | Training loss: 1.0143 | Learning Rate: 0.001000\n",
      "Epoch 407 | Training loss: 1.0176 | Learning Rate: 0.001000\n",
      "Epoch 408 | Training loss: 1.0320 | Learning Rate: 0.001000\n",
      "Epoch 409 | Training loss: 1.0255 | Learning Rate: 0.001000\n",
      "Epoch 410 | Training loss: 1.0400 | Learning Rate: 0.001000\n",
      "Epoch 410 | Validation loss: 1.0068 | Learning Rate: 0.001000\n",
      "Epoch 411 | Training loss: 1.0086 | Learning Rate: 0.001000\n",
      "Epoch 412 | Training loss: 1.0257 | Learning Rate: 0.001000\n",
      "Epoch 413 | Training loss: 1.0200 | Learning Rate: 0.001000\n",
      "Epoch 414 | Training loss: 1.0645 | Learning Rate: 0.001000\n",
      "Epoch 415 | Training loss: 1.0573 | Learning Rate: 0.001000\n",
      "Epoch 415 | Validation loss: 1.6993 | Learning Rate: 0.001000\n",
      "Epoch 416 | Training loss: 1.0262 | Learning Rate: 0.001000\n",
      "Epoch 417 | Training loss: 0.9954 | Learning Rate: 0.001000\n",
      "Epoch 418 | Training loss: 1.0292 | Learning Rate: 0.001000\n",
      "Epoch 419 | Training loss: 1.0242 | Learning Rate: 0.001000\n",
      "Epoch 420 | Training loss: 1.0143 | Learning Rate: 0.001000\n",
      "Epoch 420 | Validation loss: 1.6312 | Learning Rate: 0.001000\n",
      "Epoch 421 | Training loss: 1.0045 | Learning Rate: 0.001000\n",
      "Epoch 422 | Training loss: 0.9977 | Learning Rate: 0.001000\n",
      "Epoch 423 | Training loss: 1.0210 | Learning Rate: 0.001000\n",
      "Epoch 424 | Training loss: 1.0071 | Learning Rate: 0.001000\n",
      "Epoch 425 | Training loss: 1.0099 | Learning Rate: 0.001000\n",
      "Epoch 425 | Validation loss: 1.1075 | Learning Rate: 0.001000\n",
      "Epoch 426 | Training loss: 1.0399 | Learning Rate: 0.001000\n",
      "Epoch 427 | Training loss: 0.9951 | Learning Rate: 0.001000\n",
      "Epoch 428 | Training loss: 0.9965 | Learning Rate: 0.001000\n",
      "Epoch 429 | Training loss: 1.0109 | Learning Rate: 0.001000\n",
      "Epoch 430 | Training loss: 1.0137 | Learning Rate: 0.001000\n",
      "Epoch 430 | Validation loss: 1.0882 | Learning Rate: 0.001000\n",
      "Epoch 431 | Training loss: 1.0250 | Learning Rate: 0.001000\n",
      "Epoch 432 | Training loss: 1.0483 | Learning Rate: 0.001000\n",
      "Epoch 433 | Training loss: 1.0195 | Learning Rate: 0.001000\n",
      "Epoch 434 | Training loss: 0.9810 | Learning Rate: 0.001000\n",
      "Epoch 435 | Training loss: 0.9937 | Learning Rate: 0.001000\n",
      "Epoch 435 | Validation loss: 1.0083 | Learning Rate: 0.001000\n",
      "Epoch 436 | Training loss: 1.0087 | Learning Rate: 0.001000\n",
      "Epoch 437 | Training loss: 0.9856 | Learning Rate: 0.001000\n",
      "Epoch 438 | Training loss: 0.9932 | Learning Rate: 0.001000\n",
      "Epoch 439 | Training loss: 0.9892 | Learning Rate: 0.001000\n",
      "Epoch 440 | Training loss: 0.9803 | Learning Rate: 0.001000\n",
      "Epoch 440 | Validation loss: 1.0292 | Learning Rate: 0.001000\n",
      "Epoch 441 | Training loss: 1.0249 | Learning Rate: 0.001000\n",
      "Epoch 442 | Training loss: 1.0016 | Learning Rate: 0.001000\n",
      "Epoch 443 | Training loss: 0.9847 | Learning Rate: 0.001000\n",
      "Epoch 444 | Training loss: 1.0165 | Learning Rate: 0.001000\n",
      "Epoch 445 | Training loss: 0.9901 | Learning Rate: 0.001000\n",
      "Epoch 445 | Validation loss: 1.0995 | Learning Rate: 0.001000\n",
      "Epoch 446 | Training loss: 0.9680 | Learning Rate: 0.001000\n",
      "Epoch 447 | Training loss: 0.9570 | Learning Rate: 0.001000\n",
      "Epoch 448 | Training loss: 1.0131 | Learning Rate: 0.001000\n",
      "Epoch 449 | Training loss: 0.9507 | Learning Rate: 0.001000\n",
      "Epoch 450 | Training loss: 1.0166 | Learning Rate: 0.001000\n",
      "Epoch 450 | Validation loss: 1.3054 | Learning Rate: 0.001000\n",
      "Epoch 451 | Training loss: 1.0136 | Learning Rate: 0.001000\n",
      "Epoch 452 | Training loss: 0.9916 | Learning Rate: 0.001000\n",
      "Epoch 453 | Training loss: 0.9697 | Learning Rate: 0.001000\n",
      "Epoch 454 | Training loss: 1.0255 | Learning Rate: 0.001000\n",
      "Epoch 455 | Training loss: 0.9946 | Learning Rate: 0.001000\n",
      "Epoch 455 | Validation loss: 1.6381 | Learning Rate: 0.001000\n",
      "Epoch 456 | Training loss: 0.9910 | Learning Rate: 0.001000\n",
      "Epoch 457 | Training loss: 1.0066 | Learning Rate: 0.001000\n",
      "Epoch 458 | Training loss: 0.9809 | Learning Rate: 0.001000\n",
      "Epoch 459 | Training loss: 1.0019 | Learning Rate: 0.001000\n",
      "Epoch 460 | Training loss: 1.0285 | Learning Rate: 0.001000\n",
      "Epoch 460 | Validation loss: 1.0055 | Learning Rate: 0.001000\n",
      "Epoch 461 | Training loss: 0.9996 | Learning Rate: 0.001000\n",
      "Epoch 462 | Training loss: 0.9888 | Learning Rate: 0.001000\n",
      "Epoch 463 | Training loss: 1.0081 | Learning Rate: 0.001000\n",
      "Epoch 464 | Training loss: 1.0169 | Learning Rate: 0.001000\n",
      "Epoch 465 | Training loss: 0.9703 | Learning Rate: 0.001000\n",
      "Epoch 465 | Validation loss: 1.0976 | Learning Rate: 0.001000\n",
      "Epoch 466 | Training loss: 0.9720 | Learning Rate: 0.001000\n",
      "Epoch 467 | Training loss: 1.0051 | Learning Rate: 0.001000\n",
      "Epoch 468 | Training loss: 1.0061 | Learning Rate: 0.001000\n",
      "Epoch 469 | Training loss: 1.0042 | Learning Rate: 0.001000\n",
      "Epoch 470 | Training loss: 0.9829 | Learning Rate: 0.001000\n",
      "Epoch 470 | Validation loss: 1.0171 | Learning Rate: 0.001000\n",
      "Epoch 471 | Training loss: 0.9873 | Learning Rate: 0.001000\n",
      "Epoch 472 | Training loss: 0.9751 | Learning Rate: 0.001000\n",
      "Epoch 473 | Training loss: 0.9888 | Learning Rate: 0.001000\n",
      "Epoch 474 | Training loss: 0.9362 | Learning Rate: 0.001000\n",
      "Epoch 475 | Training loss: 0.9621 | Learning Rate: 0.001000\n",
      "Epoch 475 | Validation loss: 1.0712 | Learning Rate: 0.001000\n",
      "Epoch 476 | Training loss: 0.9675 | Learning Rate: 0.001000\n",
      "Epoch 477 | Training loss: 0.9422 | Learning Rate: 0.001000\n",
      "Epoch 478 | Training loss: 0.9672 | Learning Rate: 0.001000\n",
      "Epoch 479 | Training loss: 0.9578 | Learning Rate: 0.001000\n",
      "Epoch 480 | Training loss: 0.9520 | Learning Rate: 0.001000\n",
      "Epoch 480 | Validation loss: 1.0092 | Learning Rate: 0.001000\n",
      "Epoch 481 | Training loss: 0.9843 | Learning Rate: 0.001000\n",
      "Epoch 482 | Training loss: 0.9786 | Learning Rate: 0.001000\n",
      "Epoch 483 | Training loss: 0.9491 | Learning Rate: 0.001000\n",
      "Epoch 484 | Training loss: 0.9689 | Learning Rate: 0.001000\n",
      "Epoch 485 | Training loss: 0.9506 | Learning Rate: 0.001000\n",
      "Epoch 485 | Validation loss: 1.2043 | Learning Rate: 0.001000\n",
      "Epoch 486 | Training loss: 0.9433 | Learning Rate: 0.001000\n",
      "Epoch 487 | Training loss: 0.9525 | Learning Rate: 0.001000\n",
      "Epoch 488 | Training loss: 0.9582 | Learning Rate: 0.001000\n",
      "Epoch 489 | Training loss: 0.9328 | Learning Rate: 0.001000\n",
      "Epoch 490 | Training loss: 0.9915 | Learning Rate: 0.001000\n",
      "Epoch 490 | Validation loss: 1.1622 | Learning Rate: 0.001000\n",
      "Epoch 491 | Training loss: 0.9741 | Learning Rate: 0.001000\n",
      "Epoch 492 | Training loss: 0.9659 | Learning Rate: 0.001000\n",
      "Epoch 493 | Training loss: 1.0042 | Learning Rate: 0.001000\n",
      "Epoch 494 | Training loss: 0.9370 | Learning Rate: 0.001000\n",
      "Epoch 495 | Training loss: 0.9728 | Learning Rate: 0.001000\n",
      "Epoch 495 | Validation loss: 1.1038 | Learning Rate: 0.001000\n",
      "Epoch 496 | Training loss: 1.0075 | Learning Rate: 0.001000\n",
      "Epoch 497 | Training loss: 0.9658 | Learning Rate: 0.001000\n",
      "Epoch 498 | Training loss: 0.9696 | Learning Rate: 0.001000\n",
      "Epoch 499 | Training loss: 0.9278 | Learning Rate: 0.001000\n",
      "Training time: 391.3807s\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "path = root + 'data_118_quad/gnn_trained_ac118.pickle'\n",
    "try:\n",
    "    net.load_state_dict(torch.load(path))\n",
    "    print('params loaded')\n",
    "except:\n",
    "    print('cold start')\n",
    "\n",
    "# 初始學習率\n",
    "lr = 0.001\n",
    "factor = 0.9\n",
    "patience = 5\n",
    "optimizer = optim.AdamW(net.parameters(), lr)\n",
    "\n",
    "# Learning Rate Scheduler: ReduceLROnPlateau\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#     optimizer, \n",
    "#     mode='min',       # 監測 loss，越小越好\n",
    "#     factor=0.9,       # 學習率變成原來的 90%\n",
    "#     patience=5,       # 5 個 epoch 沒有改善才降低學習率\n",
    "#     verbose=True,     # 會輸出學習率變更訊息\n",
    "#     min_lr=1e-6       # 學習率最低不低於 1e-6\n",
    "# )\n",
    "\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#     optimizer, \n",
    "#     mode='min',       # 監測 loss\n",
    "#     factor=factor,       # 學習率變成 50% (下降更多)\n",
    "#     patience=patience,       # 3 個 epoch 沒有改善就降低學習率\n",
    "#     verbose=True,     \n",
    "#     min_lr=1e-6       \n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "lr_list = []\n",
    "\n",
    "t0 = time.time()\n",
    "max_epochs = 500\n",
    "eval_epoch = 5\n",
    "\n",
    "tolerance = 5  # 早停耐心值\n",
    "min_delta = 1e-3\n",
    "previous = float('inf')\n",
    "\n",
    "feas = False  # 加入可行性標記\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    total_loss = 0.0\n",
    "    for local_batch, local_label in train_set:\n",
    "        optimizer.zero_grad()\n",
    "        local_batch, local_label = local_batch.to(device), local_label.to(device)\n",
    "        logits = net(local_batch)\n",
    "        loss = my_loss.calc(logits, local_label, local_batch, feas)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_set.dataset)\n",
    "    train_loss.append(avg_loss)\n",
    "    \n",
    "    # lr_list.append(scheduler.optimizer.param_groups[0]['lr'])\n",
    "    lr_list.append(lr)\n",
    "    # print(f\"Epoch {epoch} | Training loss: {avg_loss:.4f} | Learning Rate: {scheduler.optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    print(f\"Epoch {epoch} | Training loss: {avg_loss:.4f} | Learning Rate: {lr:.6f}\")\n",
    "    \n",
    "    if epoch % eval_epoch == 0:\n",
    "        net.eval()\n",
    "        total_loss = 0.0\n",
    "        for local_batch, local_label in val_set:\n",
    "            local_batch, local_label = local_batch.to(device), local_label.to(device)\n",
    "            logits = net(local_batch)\n",
    "            loss = my_loss.calc(logits, local_label, local_batch, feas)\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(val_set.dataset)\n",
    "        val_loss.append([epoch, avg_loss])\n",
    "        print(f\"Epoch {epoch} | Validation loss: {avg_loss:.4f} | Learning Rate: {lr:.6f}\")\n",
    "        \n",
    "        # 更新 learning rate\n",
    "        #scheduler.step(avg_loss)\n",
    "        \n",
    "        # 早停機制\n",
    "        # if epoch:\n",
    "        #     if previous - avg_loss < min_delta:\n",
    "        #         tolerance -= 1\n",
    "        #     if tolerance == 0:\n",
    "        #         print(\"Early stopping triggered\")\n",
    "        #         break\n",
    "        previous = avg_loss\n",
    "        net.train()\n",
    "\n",
    "        final_epoch = epoch\n",
    "\n",
    "t1 = time.time()\n",
    "print(f\"Training time: {t1 - t0:.4f}s\")\n",
    "#印出 lr validation loss training loss 圖\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 100 500\n",
      "✅ 輸出內容已儲存到 C:\\Users\\USER\\Desktop\\GNN_OPF_electricity_market-main\\output_loss_fig\\trainingloss_gunplot\\02181727.txt\n",
      "⚠️ Gnuplot Error: Warning: empty y range [0.001:0.001], adjusting to [0.00099:0.00101]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# **設定輸出目錄**\n",
    "output_dir = r'C:\\Users\\USER\\Desktop\\GNN_OPF_electricity_market-main\\output_loss_fig\\trainingloss_gunplot'\n",
    "timestamp = datetime.now().strftime('%m%d%H%M')\n",
    "output_file = os.path.join(output_dir, f'{timestamp}.txt')\n",
    "\n",
    "# **確保目錄存在**\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(len(train_loss), len(val_loss), len(lr_list))  # **確認長度**\n",
    "\n",
    "# **儲存數據到 txt 檔案**\n",
    "with open(output_file, 'w') as file:\n",
    "    for i in range(len(train_loss)):\n",
    "        if i % 5 == 0 and (i // 5) < len(val_loss):  # **確保 val_loss 沒有超出索引**\n",
    "            val = val_loss[i // 5]  # **正確存入數值**\n",
    "        else:\n",
    "            val = \"NaN\"  # **改成 NaN 讓 Gnuplot 忽略**\n",
    "        file.write(f\"{i} {train_loss[i]} {val[1]} {lr_list[i]}\\n\")\n",
    "\n",
    "print(f\"✅ 輸出內容已儲存到 {output_file}\")\n",
    "\n",
    "# **修正 Windows 路徑格式**\n",
    "gnuplot_file = output_file.replace(\"\\\\\", \"/\")  # **確保 Gnuplot 能識別路徑**\n",
    "loss_plot_path = os.path.join(output_dir, f'loss_plot_{timestamp}.png').replace(\"\\\\\", \"/\")\n",
    "lr_plot_path = os.path.join(output_dir, f'lr_plot_{timestamp}.png').replace(\"\\\\\", \"/\")\n",
    "\n",
    "# **定義 gnuplot 腳本**\n",
    "gnuplot_script = f\"\"\"\n",
    "set terminal pngcairo enhanced font 'Arial,12' size 800,600\n",
    "set output \"{loss_plot_path}\"\n",
    "\n",
    "set title \"Training Loss & Validation Loss\"\n",
    "set xlabel \"Step\"\n",
    "set ylabel \"Value\"\n",
    "set grid\n",
    "set key outside\n",
    "\n",
    "# **確保 Validation Loss 顯示**\n",
    "plot \"{gnuplot_file}\" using 1:2 with lines title \"Train Loss\" linecolor rgb \"blue\", \\\n",
    "     \"{gnuplot_file}\" using 1:3 with lines title \"Val Loss\" linecolor rgb \"red\"\n",
    "   \n",
    "\n",
    "# **第二張圖：Learning Rate**\n",
    "set output \"{lr_plot_path}\"\n",
    "set title \"Learning Rate\"\n",
    "set xlabel \"Step\"\n",
    "set ylabel \"Learning Rate\"\n",
    "set grid\n",
    "set key outside\n",
    "\n",
    "# **繪製 Learning Rate**\n",
    "plot \"{gnuplot_file}\" using 1:4 with lines title \"Learning Rate\" linecolor rgb \"green\" lw 2\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# **執行 gnuplot**\n",
    "try:\n",
    "    result = subprocess.run([\"gnuplot\"], input=gnuplot_script, text=True, check=True,\n",
    "                            stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    if result.stderr:\n",
    "        print(\"⚠️ Gnuplot Error:\", result.stderr)\n",
    "    else:\n",
    "        print(f\"✅ Plots generated successfully.\\n📂 Loss plot: {loss_plot_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ Error: Gnuplot is not installed or not in PATH.\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"❌ Gnuplot execution failed:\\n{e.stderr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AvUTphUSzqzA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Desktop\\GNN_OPF_electricity_market-main\\GNN_OPF_electricity_market-main\\GNN_OPF_electricity_market-main\\data_gen\\118_data\\FCNN_model\\dnn_02181727.pickle\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "timestamp=datetime.now().strftime('%m%d%H%M')\n",
    "path = 'C:\\\\Users\\\\USER\\\\Desktop\\\\GNN_OPF_electricity_market-main\\\\GNN_OPF_electricity_market-main\\\\GNN_OPF_electricity_market-main\\\\data_gen\\\\118_data\\\\FCNN_model\\\\'\n",
    "\n",
    "path = path+'dnn_%s.pickle'%(timestamp)\n",
    "if feas==False: path.replace('feas','')\n",
    "print(path)\n",
    "torch.save(net.state_dict(),path)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "L3M4jmjkscK_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "val_len = len(val_loss)\n",
    "print(val_len)\n",
    "val_plt = np.zeros((2,val_len))\n",
    "for i in range(val_len):\n",
    "  val_plt[0,i] = val_loss[i][0]\n",
    "  val_plt[1,i] = val_loss[i][1]\n",
    "\n",
    "plt.figure()\n",
    "plot_idx = np.arange(np.size(train_loss))\n",
    "plt.plot(plot_idx[5:-1],train_loss[5:-1],lw=2,label='training loss')\n",
    "plt.plot(val_plt[0,1:],val_plt[1,1:],lw=2,label='validation loss')\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "\n",
    "# 在圖的右上角標示學習率\n",
    "plt.text(0.95, 0.95, f'Learning Rate: {lr:.6f}\\nbatch size: {batch_size}', \n",
    "         transform=plt.gca().transAxes, fontsize=10, ha='right', va='top', color='red')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 儲存圖片到指定資料夾（需提供路徑）\n",
    "timestamp=datetime.now().strftime('%m%d%H%M')\n",
    "output_path = r'C:\\Users\\USER\\Desktop\\GNN_OPF_electricity_market-main\\output_loss_fig'\n",
    "output_path = output_path + '/118ac_loss_fig_%s.png' % (timestamp)\n",
    "plt.savefig(output_path, dpi=300, bbox_inches='tight')  # 儲存為 PNG 格式，解析度 300 dpi\n",
    "\n",
    "\n",
    "plt.show(block=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9uGUm4rsco3"
   },
   "source": [
    "# Evaluate the model w/ validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "4u6001UQ2pN3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset size: torch.Size([2000, 708])\n",
      "Number of validation set:  2000\n",
      "torch.Size([2000, 236])\n"
     ]
    }
   ],
   "source": [
    "n_test = np.size(x_test,0)\n",
    "x_test_feed = torch.from_numpy(x_test).float()\n",
    "x_test_feed = x_test_feed#.transpose(1,2)\n",
    "x_test_feed = x_test_feed.to(device)\n",
    "print('Validation dataset size:',x_test_feed.shape)\n",
    "print('Number of validation set: ',n_test)\n",
    "y_pred = net(x_test_feed)\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnNbSGYoXS3J"
   },
   "source": [
    "* Visualization\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKZQaqwJkn3P"
   },
   "source": [
    " - Visualize errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "7-JCo0KmXwm3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 236) (2000, 236)\n"
     ]
    }
   ],
   "source": [
    "y_pred1 = y_pred.cpu().detach()\n",
    "y_pred1 = torch.squeeze(y_pred1,1).numpy()#.transpose()\n",
    "print(y_test.shape,y_pred1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "NOeXUzrd9h9y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 2, 2000) (2000, 236)\n"
     ]
    }
   ],
   "source": [
    "# x=np.reshape(x,(x.shape[0]*x.shape[1],x.shape[2])) # reshape by samples not dim1\n",
    "# y=np.reshape(y,(y.shape[0]*y.shape[1],y.shape[2]))\n",
    "# print(x_pre.shape,y_pre.shape)\n",
    "\n",
    "y_pred_temp = y_pred1.copy().transpose()\n",
    "# y_pred2=np.reshape(y_pred2,(y_pre.shape[0],y_pre.shape[1],n_test))\n",
    "y_pred2=np.zeros([y.shape[0],y.shape[1],n_test])\n",
    "y_pred2[:,0,:]=y_pred_temp[:n_bus,:]\n",
    "y_pred2[:,1,:]=y_pred_temp[n_bus:,:]\n",
    "print(y_pred2.shape,y_pred1.shape)\n",
    "\n",
    "y_test_temp = y_test.copy().transpose()\n",
    "# y_pred2=np.reshape(y_pred2,(y_pre.shape[0],y_pre.shape[1],n_test))\n",
    "y_test2=np.zeros([y.shape[0],y.shape[1],n_test])\n",
    "y_test2[:,0,:]=y_test_temp[:n_bus,:]\n",
    "y_test2[:,1,:]=y_test_temp[n_bus:,:]\n",
    "y_pred1 = y_pred2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "2eVE-t3nl0Cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 2, 2000) (118, 2, 2000)\n"
     ]
    }
   ],
   "source": [
    "# recover the original p.u. scale\n",
    "# vy_deviation) * vy_scale\n",
    "y_pred1[:,1,:] = y_pred1[:,1,:] / vy_scale + vy_deviation\n",
    "y_test2[:,1,:] = y_test2[:,1,:] / vy_scale + vy_deviation\n",
    "print(y_test2.shape,y_pred1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "FUVl5LXeknC4"
   },
   "outputs": [],
   "source": [
    "n_test = np.size(y_test2,2)\n",
    "err_L2 = np.zeros(n_test)\n",
    "err_Linf = np.zeros(n_test)\n",
    "for i in range(n_test):\n",
    "  err_L2[i] = np.linalg.norm(y_test2[:,0,i] - y_pred1[:,0,i]) / np.linalg.norm(y_test2[:,0,i])\n",
    "  err_Linf[i] = np.max(np.abs(y_test2[:,0,i] - y_pred1[:,0,i])) / np.max(np.abs(y_test2[:,0,i]))\n",
    "\n",
    "err_L2_v = np.zeros(n_test)\n",
    "err_Linf_v = np.zeros(n_test)\n",
    "for i in range(n_test):\n",
    "  err_L2_v[i] = np.linalg.norm(y_test2[:,1,i] - y_pred1[:,1,i]) / np.linalg.norm(y_test2[:,1,i])\n",
    "  err_Linf_v[i] = np.max(np.abs(y_test2[:,1,i] - y_pred1[:,1,i])) / np.max(np.abs(y_test2[:,1,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "79xFCVXklkLb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price L2 mean: 0.05517208353886562 L_inf mean: 0.06398969101223595\n",
      "Voltage L2 mean: 0.005636410041250301 L_inf mean: 0.016458013605333714\n"
     ]
    }
   ],
   "source": [
    "err_L2_mean = np.mean(err_L2)\n",
    "err_Linf_mean = np.mean(err_Linf)\n",
    "print('Price L2 mean:', err_L2_mean,'L_inf mean:', err_Linf_mean )\n",
    "err_L2_mean_v = np.mean(err_L2_v)\n",
    "err_Linf_mean_v = np.mean(err_Linf_v)\n",
    "print('Voltage L2 mean:', err_L2_mean_v,'L_inf mean:', err_Linf_mean_v )\n",
    "\n",
    "# fig2 = plt.figure(figsize=(16, 16))\n",
    "# plt.subplot(2, 2, 1)\n",
    "# # plt.hist(np.abs(ga),bins = 10)\n",
    "# plt.plot(err_L2,'bo',markersize=0.5,label = 'L2 error')\n",
    "# plt.plot(err_Linf,'r^',markersize=0.5,label = 'Linf error')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('sample index')\n",
    "# plt.ylabel('error')\n",
    "# plt.title('Normalized sample error')\n",
    "# plt.grid(True)\n",
    "# # error histogram\n",
    "# plt.subplot(2, 2, 2)\n",
    "# plt.hist(err_L2, bins = 50, facecolor='b', alpha=0.75,label = 'L2 error')\n",
    "# plt.hist(err_Linf, bins = 50, facecolor='g', alpha=0.75,label = 'Linf error')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('sample error value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('Histogram of L_2 error')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)\n",
    "\n",
    "# plt.subplot(2, 2, 3)\n",
    "# # plt.hist(np.abs(ga),bins = 10)\n",
    "# plt.plot(err_L2_v,'bo',markersize=0.5,label = 'L2 error')\n",
    "# plt.plot(err_Linf_v,'r^',markersize=0.5,label = 'Linf error')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('sample index')\n",
    "# plt.ylabel('error')\n",
    "# plt.title('Normalized sample error')\n",
    "# plt.grid(True)\n",
    "# # error histogram\n",
    "# plt.subplot(2, 2, 4)\n",
    "# plt.hist(err_L2_v, bins = 50, facecolor='b', alpha=0.75,label = 'L2 error')\n",
    "# plt.hist(err_Linf_v, bins = 50, facecolor='g', alpha=0.75,label = 'Linf error')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('sample error value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('Histogram of L_2 error')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "6sUddh-uGg_p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 2000) (118, 2000)\n",
      "true range: 1.06 0.94\n",
      "predicted range 1.0854425048828125 0.9326277422904968\n"
     ]
    }
   ],
   "source": [
    "print(y_pred1[:,1,:n_test].shape,y_test2[:,1,:n_test].shape)\n",
    "print('true range:',np.max(y_test2[:,1,:n_test]),np.min(y_test2[:,1,:n_test]))\n",
    "print('predicted range',np.max(y_pred1[:,1,:n_test]),np.min(y_pred1[:,1,:n_test]))\n",
    "\n",
    "# fig3 = plt.figure(figsize=(16, 8))\n",
    "# flat_list1 = list(np.concatenate(y_test2[:,1,:n_test]).flat)\n",
    "# flat_list2 = list(np.concatenate(y_pred1[:,1,:n_test]).flat)\n",
    "# plt.hist(flat_list1,bins = 100,label = 'true')\n",
    "\n",
    "# plt.hist(flat_list2,bins = 100,label = 'pred')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "jhfImaPsjKYq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 6, 10000) 10000\n"
     ]
    }
   ],
   "source": [
    "print(x.shape,n_sample)\n",
    "\n",
    "x_new = np.zeros([x.shape[0],x.shape[1],n_sample])\n",
    "for i in range(x.shape[1]):\n",
    "  x_new[:,i,:] = x_total[n_bus*i:n_bus*(i+1),:]\n",
    "\n",
    "y_new = np.zeros([y.shape[0],y.shape[1],n_sample])\n",
    "for i in range(y.shape[1]):\n",
    "  y_new[:,i,:] = y_total[n_bus*i:n_bus*(i+1),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9x7neMNj7n3"
   },
   "source": [
    "# Predict generation using $\\pi$\n",
    "* Using predicted $\\pi$ and find the active constraints in $p_G(i)$\n",
    "* For inactive $p_G(i)$ consider other methods like power flow balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Kr29K04j2KTN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 10000)\n",
      "<class 'numpy.ndarray'> 118 [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117]\n"
     ]
    }
   ],
   "source": [
    "gen_limit0 = x_new[:,4,:].copy() # lin cost\n",
    "print(gen_limit0.shape)\n",
    "\n",
    "gen_idx = []\n",
    "gen_idx = np.arange(n_bus)\n",
    "# for i in range(n_bus):\n",
    "#   if gen_limit0[i,0] > 0:\n",
    "#     gen_idx.append(i)\n",
    "print(type(gen_idx),len(gen_idx),gen_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "gHmx9lMDXzpM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 2, 10000) (236, 10000)\n"
     ]
    }
   ],
   "source": [
    "n_sample=x_total.shape[-1]\n",
    "x_feed = torch.from_numpy(x_total.T).float()\n",
    "y_pred1=net(x_feed.to(device)).cpu().detach().numpy().T\n",
    "y_pred_temp = y_pred1.copy()\n",
    "y_pred2=np.zeros([y.shape[0],y.shape[1],n_sample])\n",
    "y_pred2[:,0,:]=y_pred_temp[:n_bus,:]\n",
    "y_pred2[:,1,:]=y_pred_temp[n_bus:,:]\n",
    "print(y_pred2.shape,y_pred1.shape)\n",
    "y_pred1 = y_pred2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "GPEHF2L91bGv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0010725173950199007\n",
      "0.784000000000006\n",
      "0.342110789983331\n",
      "0.24221453285700786\n"
     ]
    }
   ],
   "source": [
    "gen_cost0 = x_new[:,4,:].copy()\n",
    "lmp_data = y_new[:,0,:].copy()\n",
    "quadratic_a = x_new[:,5,:].copy()\n",
    "profit_pred = y_pred1[:,0,:] - gen_cost0\n",
    "print(np.min(np.abs(profit_pred)))\n",
    "profit_true = lmp_data - gen_cost0\n",
    "print(np.min(np.abs(profit_true)))\n",
    "profit_pred=(y_pred1[:,0,:]-gen_cost0)/(quadratic_a+1e-10)/2\n",
    "profit_true=(lmp_data-gen_cost0)/(quadratic_a+1e-10)/2\n",
    "print(np.min(np.abs(profit_pred)))\n",
    "print(np.min(np.abs(profit_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "uiHWtU5OLc1x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 10000) (118, 10000)\n",
      "0.04220356656642579\n"
     ]
    }
   ],
   "source": [
    "print(profit_pred.shape,profit_true.shape)\n",
    "profit_err = profit_true - profit_pred\n",
    "profit_err_l2 = np.zeros([n_sample,1])\n",
    "\n",
    "for i in range(n_sample):\n",
    "  profit_err_l2[i] = np.linalg.norm(profit_err[:,i])/np.linalg.norm(profit_true[:,i])\n",
    "print(np.mean(profit_err_l2))\n",
    "\n",
    "# fig5 = plt.figure(figsize=(16, 8))\n",
    "# # error histogram\n",
    "# plt.hist(profit_err_l2, bins = 50, facecolor='b', alpha=0.75,label = 'L2 error')\n",
    "# # plt.hist(err_linf_new, bins = 50, facecolor='g', alpha=0.75,label = 'Linf error')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('sample error value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('Histogram of L_2 error')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "ZbHchRQd_g8-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1180000,)\n",
      "-639.832612817223 -2.9391182643149665\n"
     ]
    }
   ],
   "source": [
    "p_pred_sort = np.reshape(profit_pred,n_bus*n_sample)\n",
    "p_true_sort = np.reshape(profit_true,n_bus*n_sample)\n",
    "print(p_pred_sort.shape)\n",
    "print(np.min(p_pred_sort),np.min(p_true_sort))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "TYsSdNGp-OLP"
   },
   "outputs": [],
   "source": [
    "# fig2 = plt.figure(figsize=(8, 8))\n",
    "# plt.hist(p_pred_sort, bins = 50, facecolor='b', alpha=0.75,label = 'pred. profit')\n",
    "# plt.hist(p_true_sort, bins = 50, facecolor='g', alpha=0.75,label = 'true profit')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('profit histogram')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "1J9j5tT9p_f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3016239.2425034004 2764601.835122853\n",
      "3016239.2425034004 43260060.56350001 3016239.2425034004\n"
     ]
    }
   ],
   "source": [
    "# x = [load, gen_cost, gen_lim]\n",
    "binary_thres_true = 1e-5\n",
    "binary_thres = x_new[:,0,:].copy() # upper\n",
    "binary_thres_lo = x_new[:,1,:].copy() # lower\n",
    "gen_pred_binary_full = np.zeros((n_bus,n_sample))\n",
    "gen_true_binary_full = np.zeros((n_bus,n_sample))\n",
    "\n",
    "for i in range(n_sample):\n",
    "  for j in range(len(gen_idx)):\n",
    "    # predicted generator limit\n",
    "    if profit_pred[gen_idx[j],i] > binary_thres[gen_idx[j],i]:\n",
    "      gen_pred_binary_full[gen_idx[j],i] = binary_thres[gen_idx[j],i]\n",
    "    elif profit_pred[gen_idx[j],i] < binary_thres_lo[gen_idx[j],i]:\n",
    "      gen_pred_binary_full[gen_idx[j],i] = binary_thres_lo[gen_idx[j],i]\n",
    "    else:\n",
    "      gen_pred_binary_full[gen_idx[j],i] = profit_pred[gen_idx[j],i]\n",
    "    # true generator limit\n",
    "    if profit_true[gen_idx[j],i] > binary_thres[gen_idx[j],i]:\n",
    "      gen_true_binary_full[gen_idx[j],i] = binary_thres[gen_idx[j],i]\n",
    "    elif profit_true[gen_idx[j],i] < binary_thres_lo[gen_idx[j],i]:\n",
    "      gen_true_binary_full[gen_idx[j],i] = binary_thres_lo[gen_idx[j],i]\n",
    "    else:\n",
    "      gen_true_binary_full[gen_idx[j],i] = profit_true[gen_idx[j],i]\n",
    "\n",
    "gen_inj=gen_pred_binary_full\n",
    "gen_inj_true=gen_true_binary_full\n",
    "# nodal injection\n",
    "load0 = -x_new[:,1,:].copy() # load file\n",
    "p_inj = gen_inj #- load0\n",
    "p_inj_true = gen_inj_true #- load0\n",
    "print(np.sum(p_inj),np.sum(gen_inj_true))\n",
    "print(np.sum(p_inj),np.sum(load0),np.sum(gen_inj))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WAgqdRPjAONm"
   },
   "source": [
    "## injection accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "J46pLAw2AQor"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 10000) (118, 10000)\n",
      "mean p_inj l2 err: 0.09980493942331249\n"
     ]
    }
   ],
   "source": [
    "print(p_inj_true.shape,p_inj.shape)\n",
    "p_inj_true_sort = np.reshape(p_inj_true,n_bus*n_sample)\n",
    "p_inj_sort = np.reshape(p_inj,n_bus*n_sample)\n",
    "p_err = np.zeros(n_sample)\n",
    "for i in range(n_sample):\n",
    "  p_err[i] = np.linalg.norm(p_inj_true[:,i]-p_inj[:,i]) / np.linalg.norm(p_inj_true[:,i])\n",
    "print('mean p_inj l2 err:',np.mean(p_err))\n",
    "# fig3 = plt.figure(figsize=(16, 8))\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.hist(p_inj_sort, bins = 50, facecolor='b', alpha=0.75,label = 'pred. injection')\n",
    "# plt.hist(p_inj_true_sort, bins = 50, facecolor='g', alpha=0.75,label = 'true injection')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('injection histogram')\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.hist(p_err, bins = 50, facecolor='b', alpha=0.75,label = 'injection err')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('error histogram')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)s_binary\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DaN7u4xeGIom"
   },
   "source": [
    "* Calculate flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "YT4sgn_n79MI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(186, 1) (186, 10000) (186, 10000)\n",
      "15569 13334\n",
      "0.008370430107526881 0.007168817204301075\n",
      "186 10000 (186, 10000)\n"
     ]
    }
   ],
   "source": [
    "filename=root+'118ac_fmax.txt'\n",
    "f_max1=pd.read_table(filename,sep=',',header=None).to_numpy() # flow limit\n",
    "\n",
    "n_line = np.size(S_isf,0)\n",
    "flow_est = np.zeros((n_line,n_sample))\n",
    "flow_est0 = np.zeros((n_line,n_sample))\n",
    "\n",
    "f_binary = np.zeros((n_line,n_sample))\n",
    "f_binary0 = np.zeros((n_line,n_sample))\n",
    "\n",
    "# for i in range(n_sample):\n",
    "flow_est = np.dot(S_isf,p_inj)\n",
    "flow_est0 = np.dot(S_isf,p_inj_true)\n",
    "# f_max\n",
    "# f_max_numpy = f_max.cpu().detach().numpy()\n",
    "f_max_numpy = f_max1.copy()\n",
    "f_binary = (np.abs(flow_est)-f_max_numpy > 0)\n",
    "f_binary0 = (np.abs(flow_est0)-f_max_numpy > 0)\n",
    "\n",
    "print(f_max_numpy.shape,flow_est.shape,flow_est0.shape)\n",
    "f_tot_sample = n_line * n_sample\n",
    "print(np.sum(f_binary),np.sum(f_binary0))\n",
    "print(np.sum(f_binary)/f_tot_sample,np.sum(f_binary0)/f_tot_sample)\n",
    "print(n_line,n_sample,flow_est.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "VyvDpKhyQj0M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164.9924537815395 39.64401431411352\n",
      "1.09994969187693 0.2642934287607568\n"
     ]
    }
   ],
   "source": [
    "# soft threshold\n",
    "f_err_est = np.abs(flow_est)-f_max_numpy\n",
    "f_err_true = np.abs(flow_est0)-f_max_numpy\n",
    "\n",
    "f_err_est = np.maximum(np.abs(flow_est)-f_max_numpy,0) # identify violations\n",
    "f_err_true = np.maximum(np.abs(flow_est0)-f_max_numpy,0)\n",
    "\n",
    "print(np.max(f_err_est),np.max(f_err_true))\n",
    "print(np.max(f_err_est/f_max_numpy),np.max(f_err_true/f_max_numpy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "7iuX7tN2a2Cp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11861 10451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006376881720430107 0.005618817204301075\n"
     ]
    }
   ],
   "source": [
    "f_binary_soft = (np.abs(flow_est)-f_max_numpy > 0.1*(f_max_numpy))\n",
    "f_binary0_soft = (np.abs(flow_est0)-f_max_numpy > 0.1*(f_max_numpy))\n",
    "print(np.sum(f_binary_soft),np.sum(f_binary0_soft))\n",
    "print(np.sum(f_binary_soft)/f_tot_sample,np.sum(f_binary0_soft)/f_tot_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "SYl9pxnOUUQF"
   },
   "outputs": [],
   "source": [
    "f_pred_sort = np.reshape(f_err_est/f_max_numpy,n_line*n_sample)\n",
    "f_true_sort = np.reshape(f_err_true/f_max_numpy,n_line*n_sample)\n",
    "\n",
    "# fig2 = plt.figure(figsize=(16, 8))\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.hist(f_pred_sort, bins = 10, facecolor='b', alpha=0.75,label = 'pred. f')\n",
    "# plt.hist(f_true_sort, bins = 10, facecolor='g', alpha=0.75,label = 'true f')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('percentage')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('flow violation level histogram')\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.hist(f_true_sort, bins = 10, facecolor='g', alpha=0.75,label = 'true f')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('percentage')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('flow violation level histogram')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "YglgTpWVRLri"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sample pred: 7\n",
      "max line pred: 8613\n",
      "max sample true: 3\n",
      "max line true: 10000\n"
     ]
    }
   ],
   "source": [
    "f_line = np.sum(f_binary,0)\n",
    "f_samp = np.sum(f_binary,1)\n",
    "print('max sample pred:',np.max(f_line))\n",
    "print('max line pred:',np.max(f_samp))\n",
    "\n",
    "f_line0  = np.sum(f_binary0,0)\n",
    "f_samp0 = np.sum(f_binary0,1)\n",
    "print('max sample true:',np.max(f_line0))\n",
    "print('max line true:',np.max(f_samp0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZQnarHmPl35"
   },
   "source": [
    "# Check objective optimality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "BZjhp-CgQaDz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07300548073089733\n"
     ]
    }
   ],
   "source": [
    "gen_cost_pred = np.zeros((n_bus,n_sample))\n",
    "gen_cost_true = np.zeros((n_bus,n_sample))\n",
    "objective_err = np.zeros(n_sample)\n",
    "\n",
    "gen_cost_pred = np.multiply(np.multiply(p_inj,p_inj),quadratic_a) + np.multiply(p_inj,gen_cost0)\n",
    "gen_cost_true = np.multiply(np.multiply(p_inj_true,p_inj_true),quadratic_a) + np.multiply(p_inj_true,gen_cost0)\n",
    "\n",
    "objective_err = np.sum(np.abs(gen_cost_true-gen_cost_pred),axis=0) / np.sum(gen_cost_true,axis=0)\n",
    "print(np.mean(objective_err))\n",
    "\n",
    "# fig6 = plt.figure(figsize=(16, 8))\n",
    "# # error histogram\n",
    "# plt.hist(objective_err, bins = 50, facecolor='b', alpha=0.75,label = 'L2 error')\n",
    "# # plt.hist(err_linf_new, bins = 50, facecolor='g', alpha=0.75,label = 'Linf error')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('sample error value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('Histogram of L_2 error')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdrsAWpYo0-w"
   },
   "source": [
    "## injection accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "mArjSN-So0-x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 10000) (118, 10000)\n",
      "mean p_inj l2 err: 0.09980493942331249\n"
     ]
    }
   ],
   "source": [
    "print(p_inj_true.shape,p_inj.shape)\n",
    "\n",
    "p_inj_true_sort = np.reshape(p_inj_true,n_bus*n_sample)\n",
    "p_inj_sort = np.reshape(p_inj,n_bus*n_sample)\n",
    "\n",
    "p_err = np.zeros(n_sample)\n",
    "for i in range(n_sample):\n",
    "  p_err[i] = np.linalg.norm(p_inj_true[:,i]-p_inj[:,i]) / np.linalg.norm(p_inj_true[:,i])\n",
    "\n",
    "print('mean p_inj l2 err:',np.mean(p_err))\n",
    "# fig3 = plt.figure(figsize=(16, 8))\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.hist(p_inj_sort, bins = 50, facecolor='b', alpha=0.75,label = 'pred. injection')\n",
    "# plt.hist(p_inj_true_sort, bins = 50, facecolor='g', alpha=0.75,label = 'true injection')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('injection histogram')s_binary\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.hist(p_err, bins = 50, facecolor='b', alpha=0.75,label = 'injection err')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('error histogram')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujU84tOSpqsy"
   },
   "source": [
    "# Test AC feasibility\n",
    "* P in actual value, V in p.u.\n",
    "* Use P to recover $\\theta$, or solve $\\theta$ and Q for PF\n",
    "$$ Q_m = V_m \\sum_{n=1}^N V_n \\left(G_{mn}\\sin\\theta_{mn} - B_{mn}\\cos\\theta_{mn} \\right) $$\n",
    "calculate $Q_{mn}$ directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "Of6mEXF4puDN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 118) (118, 118)\n",
      "(117, 117) (118, 10000) (118, 10000)\n",
      "(118, 10000) (118, 10000)\n"
     ]
    }
   ],
   "source": [
    "# Bbus and B_r inverse\n",
    "filename1 = root+'ieee118_Bbus.txt'\n",
    "Bbus=pd.read_table(filename1,sep=',',header=None).to_numpy()\n",
    "B_r = np.delete(Bbus,68,axis=0)\n",
    "B_r = np.delete(B_r,68,axis=1)\n",
    "Br_inv = np.linalg.inv(B_r)\n",
    "\n",
    "# Y = G + jB\n",
    "filename1 = root+'ieee118_Gmat.txt'\n",
    "G_mat=pd.read_table(filename1,sep=',',header=None).to_numpy()\n",
    "filename1 = root+'ieee118_Bmat.txt'\n",
    "B_mat=pd.read_table(filename1,sep=',',header=None).to_numpy()\n",
    "print(G_mat.shape,B_mat.shape)\n",
    "\n",
    "# line parameters\n",
    "filename1 = root+'ieee118_lineloc.txt'\n",
    "line_loc = pd.read_table(filename1,sep=',',header=None).to_numpy()\n",
    "\n",
    "# load line params\n",
    "filename1 = root+'ieee118_lineparams.txt'\n",
    "line_params = pd.read_table(filename1,sep=',',header=None).to_numpy()\n",
    "R_line = line_params[:,0].copy()\n",
    "X_line = line_params[:,1].copy()\n",
    "B_shunt = line_params[:,2].copy()\n",
    "Z_line = R_line + 1j * X_line \n",
    "Y_line = 1 / Z_line\n",
    "G_line = np.real(Y_line)\n",
    "B_line = np.imag(Y_line)\n",
    "# P_inj w/out reference bus in p.u.\n",
    "p_inj_r = np.delete(p_inj,68,axis=0) / 100\n",
    "p_inj_true_r = np.delete(p_inj_true,68,axis=0) / 100\n",
    "p_inj_pu = p_inj / 100\n",
    "p_inj_true_pu = p_inj_true / 100\n",
    "print(Br_inv.shape,p_inj.shape,p_inj_true.shape)#p_inj_true\n",
    "\n",
    "theta0 = np.matmul(Br_inv,p_inj_r)\n",
    "theta_true0 = np.matmul(Br_inv,p_inj_true_r)\n",
    "theta = np.insert(theta0,68,0,axis = 0)\n",
    "theta_true = np.insert(theta_true0,68,0,axis = 0)\n",
    "print(theta.shape,theta_true.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46242211132571 -0.9739248707802468\n",
      "2.7803011534120627 -9.166735486002148\n"
     ]
    }
   ],
   "source": [
    "print(np.max(theta),np.min(theta))\n",
    "math.sin(math.pi/6)\n",
    "print(G_line[0],B_line[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(186, 10000)\n",
      "1.0932212638854981 0.9323446202278137 (118, 10000)\n"
     ]
    }
   ],
   "source": [
    "# Calculate real and reactive flow\n",
    "f_p = np.zeros((n_line,n_sample))\n",
    "f_q = np.zeros((n_line,n_sample))\n",
    "fji_p = np.zeros((n_line,n_sample))\n",
    "fji_q = np.zeros((n_line,n_sample))\n",
    "print(f_q.shape)\n",
    "\n",
    "v_pred = y_pred1[:,1,:].copy()\n",
    "v_pred = v_pred / vy_scale + vy_deviation\n",
    "print(np.max(v_pred),np.min(v_pred),v_pred.shape)\n",
    "\n",
    "theta1 = theta[line_loc[:,0]-1,:]\n",
    "theta2 = theta[line_loc[:,1]-1,:]\n",
    "V1 = v_pred[line_loc[:,0]-1,:]\n",
    "V2 = v_pred[line_loc[:,1]-1,:] \n",
    "f_p=(a*G_line*(V1*V1).T)-a*((V1*V2).T)*(G_line*np.cos(theta1-theta2).T+B_line*np.sin(theta1-theta2).T)\n",
    "f_p=f_p.T\n",
    "f_q=-a*(V1.T)*(a*V1.T)*(B_line+B_shunt/2)+a*((V1*V2).T)*(B_line*np.cos(theta1-theta2).T-G_line*np.sin(theta1-theta2).T)\n",
    "f_q=f_q.T\n",
    "\n",
    "theta1 = theta[line_loc[:,1]-1,:]\n",
    "theta2 = theta[line_loc[:,0]-1,:]\n",
    "V1 = v_pred[line_loc[:,1]-1,:]\n",
    "V2 = v_pred[line_loc[:,0]-1,:]\n",
    "fji_p=(a*G_line*(V1*V1).T)-a*((V1*V2).T)*(G_line*np.cos(theta1-theta2).T+B_line*np.sin(theta1-theta2).T)\n",
    "fji_p=fji_p.T\n",
    "fji_q=-a*(V1.T)*(a*V1.T)*(B_line+B_shunt/2)+a*((V1*V2).T)*(B_line*np.cos(theta1-theta2).T-G_line*np.sin(theta1-theta2).T)\n",
    "fji_q=fji_q.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "gKfcrbTSVMeO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0224688083071154 -1.4823906881944708\n",
      "16.19517613313663 151.0\n"
     ]
    }
   ],
   "source": [
    "s_pred = np.sqrt(f_p*f_p+f_q*f_q)*100\n",
    "sji_pred = np.sqrt(fji_p*fji_p+fji_q*fji_q)*100\n",
    "print(np.max(f_q),np.min(f_q))\n",
    "flow_est.shape\n",
    "print(np.mean(s_pred[0,:]),np.mean(f_max_numpy[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "YO4__LJ2brLu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20443\n",
      "hard violation rate: 0.010990860215053763\n",
      "13655\n",
      "0.007341397849462365\n"
     ]
    }
   ],
   "source": [
    "sij_binary = (np.abs(s_pred)-f_max_numpy[:n_line] > 0)\n",
    "sji_binary = (np.abs(sji_pred)-f_max_numpy[:n_line] > 0)\n",
    "s_binary = np.maximum(sij_binary,sji_binary)\n",
    "print(np.sum(s_binary))#,np.sum(f_binary0))\n",
    "print('hard violation rate:',np.sum(s_binary)/n_sample/n_line)#,np.sum(f_binary0)/f_tot_sample)\n",
    "s_binary_soft = (np.abs(s_pred)-f_max_numpy[:n_line] > 0.1*(f_max_numpy[:n_line]))\n",
    "print(np.sum(s_binary_soft))#,np.sum(f_binary0_soft))\n",
    "print(np.sum(s_binary_soft)/n_sample/n_line)#,np.sum(f_binary0_soft)/f_tot_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "Ty0WpBdfJwMR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S violation level:\n",
      "hard: 0.010990860215053763\n",
      "mean: 0.0028178086499261153\n",
      "median: 0.0\n",
      "max: 1.1684838578814845\n",
      "std: 0.03493042505529336\n",
      "p99: 0.023918351943738647\n",
      "f violation level:\n",
      "hard: 0.008370430107526881 0.007168817204301075\n",
      "mean: 0.002580616441186647\n",
      "median: 0.0\n",
      "max: 1.09994969187693\n",
      "std: 0.0356673549852847\n",
      "p99: 0.0\n"
     ]
    }
   ],
   "source": [
    "# violation level\n",
    "sij_violation = np.abs(s_pred)-f_max_numpy[:n_line] #/ f_max_numpy\n",
    "sij_violation_level = np.maximum(sij_violation,0)\n",
    "sji_violation = np.abs(sji_pred)-f_max_numpy[:n_line] #/ f_max_numpy\n",
    "sji_violation_level = np.maximum(sji_violation,0)\n",
    "s_violation_level = np.maximum(sij_violation_level,sji_violation_level)\n",
    "s_violation_level = np.divide(s_violation_level,f_max_numpy[:n_line])\n",
    "s_vio_lvl = np.reshape(s_violation_level,n_line*n_sample)\n",
    "\n",
    "print('S violation level:')\n",
    "print('hard:',np.sum(s_binary)/f_tot_sample)\n",
    "print('mean:',np.mean(s_vio_lvl))\n",
    "print('median:',np.median(s_vio_lvl))\n",
    "print('max:',np.max(s_vio_lvl))\n",
    "print('std:',np.std(s_vio_lvl))\n",
    "print('p99:',np.percentile(s_vio_lvl,99))\n",
    "\n",
    "f_violation = np.abs(flow_est)-f_max_numpy #/ f_max_numpy\n",
    "f_violation_level = np.maximum(f_violation,0)\n",
    "f_violation_level = np.divide(f_violation_level,f_max_numpy)\n",
    "f_vio_lvl = np.reshape(f_violation_level,n_line*n_sample)\n",
    "\n",
    "print('f violation level:')\n",
    "print('hard:',np.sum(f_binary)/f_tot_sample,np.sum(f_binary0)/f_tot_sample)\n",
    "print('mean:',np.mean(f_vio_lvl))\n",
    "print('median:',np.median(f_vio_lvl))\n",
    "print('max:',np.max(f_vio_lvl))\n",
    "print('std:',np.std(f_vio_lvl))\n",
    "print('p99:',np.percentile(f_vio_lvl,99))\n",
    "\n",
    "# fig4 = plt.figure(figsize=(6,4))\n",
    "# plt.hist(s_vio_lvl, bins = 50, facecolor='b', alpha=0.75,label = 's violation')\n",
    "# plt.hist(f_vio_lvl, bins = 50, facecolor='r', alpha=0.75,label = 'f violation')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('violation level')\n",
    "# plt.ylabel('frequency')\n",
    "# # plt.title('injection histogram')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "kWXEpj-ryjbJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price L2 mean: 0.05517208353886562 L_inf mean: 0.06398969101223595\n",
      "std: 0.0358020871307537\n",
      "Voltage L2 mean: 0.005636410041250301 L_inf mean: 0.016458013605333714\n",
      "std: 0.0031996576590346327\n"
     ]
    }
   ],
   "source": [
    "# err_L2_mean = np.mean(err_L2)\n",
    "# err_Linf_mean = np.mean(err_Linf)\n",
    "print('Price L2 mean:', err_L2_mean,'L_inf mean:', err_Linf_mean )\n",
    "print('std:',np.std(err_L2))\n",
    "# err_L2_mean_v = np.mean(err_L2_v)\n",
    "# err_Linf_mean_v = np.mean(err_Linf_v)\n",
    "print('Voltage L2 mean:', err_L2_mean_v,'L_inf mean:', err_Linf_mean_v )\n",
    "print('std:',np.std(err_L2_v))\n",
    "\n",
    "params = (sum(temp.numel() for temp in net.parameters() if temp.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "輸出內容已儲存到 C:\\Users\\USER\\Desktop\\GNN_OPF_electricity_market-main\\output_loss_fig/118ac_output_02181727.txt\n"
     ]
    }
   ],
   "source": [
    "import os   \n",
    "# 建立輸出內容字串\n",
    "output_content = f\"\"\"\n",
    "date: {timestamp}\n",
    "final_epoch: {final_epoch}\n",
    "training time: {t1 - t0:.4e}s\n",
    "\n",
    "factor: {factor}\n",
    "patience: {patience}\n",
    "\n",
    "start learning rate: {lr:.4e}\n",
    "batch size: {batch_size}\n",
    "params number: {params:.4e}\n",
    "\n",
    "Price L2 mean: {err_L2_mean:.4e} \n",
    "Price std: {np.std(err_L2):.4e}\n",
    "\n",
    "Voltage L2 mean: {err_L2_mean_v:.4e} \n",
    "Voltage std: {np.std(err_L2_v):.4e}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# 設定儲存路徑和檔名\n",
    "output_dir = r'C:\\Users\\USER\\Desktop\\GNN_OPF_electricity_market-main\\output_loss_fig'\n",
    "timestamp = datetime.now().strftime('%m%d%H%M')\n",
    "output_file = f'{output_dir}/118ac_output_{timestamp}.txt'\n",
    "\n",
    "# 確保目錄存在\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 將內容寫入 txt 檔案\n",
    "with open(output_file, 'w') as file:\n",
    "    file.write(output_content)\n",
    "\n",
    "print(f\"輸出內容已儲存到 {output_file}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "118ac_feasdnn0417.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "AC_OPF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
