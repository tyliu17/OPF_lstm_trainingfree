{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JKLovDeXoCCP"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import math\n",
    "from datetime import datetime\n",
    "root=''\n",
    "# try:\n",
    "#   from google.colab import drive\n",
    "#   drive.mount('/content/drive')\n",
    "#   root='./drive/MyDrive/gnn/data/'\n",
    "# except:\n",
    "#   pass\n",
    "# device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# print(torch.cuda.get_device_name(0))\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# 檢查是否有可用的 GPU，否則使用 CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([186, 118]) torch.Size([186, 1]) tensor(72)\n",
      "1654.8 -332.4 949.67 0.94\n",
      "voltage range(scaled): 3.9999999999999925 16.000000000000004\n",
      "price range old: 11.547 949.67\n",
      "voltage range old: 3.9999999999999925 16.000000000000004\n",
      "price range new: 11.547 949.67\n",
      "voltage range new: 3.9999999999999925 16.000000000000004\n",
      "(118, 6, 10000) (118, 2, 10000)\n"
     ]
    }
   ],
   "source": [
    "root = 'C:/Users/USER/Desktop/GNN_OPF_electricity_market-main/GNN_OPF_electricity_market-main/GNN_OPF_electricity_market-main/data_gen/118_data/'\n",
    "# C:\\Users\\USER\\Desktop\\GNN_OPF_electricity_market-main\\GNN_OPF_electricity_market-main\\GNN_OPF_electricity_market-main\\data_gen\\118_data\\118dc_quad_ISF.txt\n",
    "filename = root + '118dc_quad_ISF.txt'\n",
    "\n",
    "S_isf=pd.read_table(filename,sep=',',header=None).to_numpy() # ISF matrix\n",
    "filename=root+'118ac_fmax.txt'\n",
    "f_max=pd.read_table(filename,sep=',',header=None).to_numpy() # flow limit\n",
    "n_line = np.size(S_isf,0)\n",
    "S = torch.from_numpy(S_isf).to(device) # ISF\n",
    "f_max = torch.from_numpy(f_max).to(device) # flow limit\n",
    "print(S.shape,f_max.shape,torch.min(f_max))\n",
    "\n",
    "x=np.load(root+'ac118_p10_x_v.npy')\n",
    "y=np.load(root+'ac118_p10_y_v.npy')\n",
    "W=np.load(root+'ac118_p10_w.npy')\n",
    "print(np.max(x),np.min(x),np.max(y),np.min(y))\n",
    "\n",
    "# scaling on voltage\n",
    "vy_deviation = 0.9\n",
    "vy_scale = 100\n",
    "y[:,1,:] = (y[:,1,:] - vy_deviation) * vy_scale\n",
    "print('voltage range(scaled):',np.min(y[:,1,:]),np.max(y[:,1,:]))\n",
    "\n",
    "# scaling on price\n",
    "pi_deviation = 0\n",
    "y[:,0,:] = y[:,0,:] + pi_deviation\n",
    "# filter out extreme points in price\n",
    "y_sort_arg = np.argsort(np.amax(np.abs(y[:,0,:]),axis=0)) # max extreme\n",
    "y_sort_arg1 = np.argsort(np.amin(y[:,0,:],axis=0),axis=0) # min extreme\n",
    "\n",
    "del_idx0 = []\n",
    "del_num = 0\n",
    "for i in range(del_num):\n",
    "  del_idx0.append(y_sort_arg[-i])\n",
    "  del_idx0.append(y_sort_arg1[i])\n",
    "# print(del_idx0)\n",
    "del_idx = [] # keep only non-repeated\n",
    "[del_idx.append(x) for x in del_idx0 if x not in del_idx]\n",
    "del_idx = np.sort(del_idx)\n",
    "# delete extreme points\n",
    "print('price range old:',np.min(y[:,0,:]),np.max(y[:,0,:]))\n",
    "print('voltage range old:',np.min(y[:,1,:]),np.max(y[:,1,:]))\n",
    "# x = np.delete(x, del_idx, axis=2)\n",
    "# y = np.delete(y, del_idx, axis=2)\n",
    "print('price range new:',np.min(y[:,0,:]),np.max(y[:,0,:]))\n",
    "print('voltage range new:',np.min(y[:,1,:]),np.max(y[:,1,:]))\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Hj9_eLfoWQY3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([186, 118]) torch.Size([186, 1]) tensor(72)\n",
      "1654.8 -332.4 949.67 0.94\n",
      "voltage range(scaled): 3.9999999999999925 16.000000000000004\n",
      "price range old: 11.547 949.67\n",
      "voltage range old: 3.9999999999999925 16.000000000000004\n",
      "price range new: 11.547 949.67\n",
      "voltage range new: 3.9999999999999925 16.000000000000004\n",
      "(118, 6, 10000) (118, 2, 10000)\n"
     ]
    }
   ],
   "source": [
    "root = 'C:/Users/USER/Desktop/GNN_OPF_electricity_market-main/GNN_OPF_electricity_market-main/GNN_OPF_electricity_market-main/data_gen/118_data/'\n",
    "# C:\\Users\\USER\\Desktop\\GNN_OPF_electricity_market-main\\GNN_OPF_electricity_market-main\\GNN_OPF_electricity_market-main\\data_gen\\118_data\\118dc_quad_ISF.txt\n",
    "filename = root + '118dc_quad_ISF.txt'\n",
    "\n",
    "S_isf=pd.read_table(filename,sep=',',header=None).to_numpy() # ISF matrix\n",
    "filename=root+'118ac_fmax.txt'\n",
    "f_max=pd.read_table(filename,sep=',',header=None).to_numpy() # flow limit\n",
    "n_line = np.size(S_isf,0)\n",
    "S = torch.from_numpy(S_isf).to(device) # ISF\n",
    "f_max = torch.from_numpy(f_max).to(device) # flow limit\n",
    "print(S.shape,f_max.shape,torch.min(f_max))\n",
    "\n",
    "x=np.load(root+'ac118_p10_x_v.npy')\n",
    "y=np.load(root+'ac118_p10_y_v.npy')\n",
    "W=np.load(root+'ac118_p10_w.npy')\n",
    "print(np.max(x),np.min(x),np.max(y),np.min(y))\n",
    "\n",
    "# scaling on voltage\n",
    "vy_deviation = 0.9\n",
    "vy_scale = 100\n",
    "y[:,1,:] = (y[:,1,:] - vy_deviation) * vy_scale\n",
    "print('voltage range(scaled):',np.min(y[:,1,:]),np.max(y[:,1,:]))\n",
    "\n",
    "# scaling on price\n",
    "pi_deviation = 0\n",
    "y[:,0,:] = y[:,0,:] + pi_deviation\n",
    "# filter out extreme points in price\n",
    "y_sort_arg = np.argsort(np.amax(np.abs(y[:,0,:]),axis=0)) # max extreme\n",
    "y_sort_arg1 = np.argsort(np.amin(y[:,0,:],axis=0),axis=0) # min extreme\n",
    "\n",
    "del_idx0 = []\n",
    "del_num = 0\n",
    "for i in range(del_num):\n",
    "  del_idx0.append(y_sort_arg[-i])\n",
    "  del_idx0.append(y_sort_arg1[i])\n",
    "# print(del_idx0)\n",
    "del_idx = [] # keep only non-repeated\n",
    "[del_idx.append(x) for x in del_idx0 if x not in del_idx]\n",
    "del_idx = np.sort(del_idx)\n",
    "# delete extreme points\n",
    "print('price range old:',np.min(y[:,0,:]),np.max(y[:,0,:]))\n",
    "print('voltage range old:',np.min(y[:,1,:]),np.max(y[:,1,:]))\n",
    "# x = np.delete(x, del_idx, axis=2)\n",
    "# y = np.delete(y, del_idx, axis=2)\n",
    "print('price range new:',np.min(y[:,0,:]),np.max(y[:,0,:]))\n",
    "print('voltage range new:',np.min(y[:,1,:]),np.max(y[:,1,:]))\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "YcR42RBbdZqZ"
   },
   "outputs": [],
   "source": [
    "n_sample=y.shape[-1]\n",
    "n_bus=y.shape[0]\n",
    "x_total=x.transpose((1,0,2)).reshape(-1,x.shape[-1])\n",
    "y_total=y.transpose((1,0,2)).reshape(-1,y.shape[-1])\n",
    "x_train,x_test,y_train,y_test=train_test_split(x_total.T,y_total.T,test_size=0.2)\n",
    "\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        self.x=torch.from_numpy(x).float()\n",
    "        self.y=torch.from_numpy(y).float()\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "          idx=idx.tolist()\n",
    "        return self.x[idx],self.y[idx]\n",
    "    \n",
    "batch_size=512\n",
    "#512\n",
    "params={'batch_size': batch_size,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 0}\n",
    "train=Dataset(x_train,y_train)\n",
    "train_set=torch.utils.data.DataLoader(train,**params)\n",
    "val=Dataset(x_test,y_test)\n",
    "val_set=torch.utils.data.DataLoader(val,**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 708])\n",
      "Train Mean: 6.476999282836914\n",
      "Train Std: 3.7040112018585205\n",
      "Validation Mean: 6.432124614715576\n",
      "Validation Std: 3.668586015701294\n"
     ]
    }
   ],
   "source": [
    "# 資料分析\n",
    "# 從資料集中建立 DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 取第一個 batch 進行分析\n",
    "train_batch_x, _ = next(iter(train_loader))\n",
    "val_batch_x, _ = next(iter(val_loader))\n",
    "\n",
    "# 計算均值和標準差\n",
    "print(train_batch_x.shape)\n",
    "train_mean, train_std = train_batch_x.mean(dim=0), train_batch_x.std(dim=0)\n",
    "val_mean, val_std = val_batch_x.mean(dim=0), val_batch_x.std(dim=0)\n",
    "\n",
    "mean_value_train = train_mean.mean().item()\n",
    "print('Train Mean:', mean_value_train)\n",
    "std_value_train = train_std.mean().item()\n",
    "print('Train Std:', std_value_train)\n",
    "mean_value_val = val_mean.mean().item()\n",
    "print('Validation Mean:', mean_value_val)\n",
    "std_value_val = val_std.mean().item()\n",
    "print('Validation Std:', std_value_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "JN9gN9BnCNim"
   },
   "outputs": [],
   "source": [
    "# fig2 = plt.figure(figsize=(8,4))\n",
    "# flat_list = list(np.concatenate(y[:,n_sample:]).flat)\n",
    "# flat_list3 = list(np.concatenate(y[:,:n_sample]).flat)\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.hist(flat_list,bins = 100,label = 'voltage')\n",
    "# plt.subplot(1,2,2)\n",
    "# # plt.hist(flat_list3,range=[-2000, 2000],bins = 100,label = 'price')\n",
    "# plt.hist(flat_list3,bins = 100,label = 'price')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "iOyWpG_4yCtz"
   },
   "outputs": [],
   "source": [
    "\n",
    "class dnn(torch.nn.Module):\n",
    "  def __init__(self,shape):\n",
    "    super(dnn,self).__init__()\n",
    "    layers=[]\n",
    "    for idx in range(len(shape)-2):\n",
    "      layers.extend([\n",
    "        nn.Linear(shape[idx],shape[idx+1]),\n",
    "        nn.BatchNorm1d(shape[idx+1]),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "      ])\n",
    "    layers+=[nn.Linear(shape[-2],shape[-1])]\n",
    "    self.features=nn.Sequential(*layers)\n",
    "    for temp in self.features:\n",
    "      if type(temp)==nn.Linear:\n",
    "        torch.nn.init.normal_(temp.weight,mean=0,std=1)\n",
    "  def forward(self,x): return self.features(x)\n",
    "#net=dnn([n_bus*6,n_bus*10,n_bus*10,n_bus*10,n_bus*2]).to(device)\n",
    "#print('number of params: %d'%(sum(temp.numel() for temp in net.parameters() if temp.requires_grad)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_bus: 118\n",
      "Total Parameters: 5,935,636\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 590]         418,310\n",
      "       BatchNorm1d-2                  [-1, 590]           1,180\n",
      "              ReLU-3                  [-1, 590]               0\n",
      "           Dropout-4                  [-1, 590]               0\n",
      "            Linear-5                  [-1, 590]         348,690\n",
      "       BatchNorm1d-6                  [-1, 590]           1,180\n",
      "              ReLU-7                  [-1, 590]               0\n",
      "           Dropout-8                  [-1, 590]               0\n",
      "            Linear-9                 [-1, 1180]         697,380\n",
      "      BatchNorm1d-10                 [-1, 1180]           2,360\n",
      "             ReLU-11                 [-1, 1180]               0\n",
      "          Dropout-12                 [-1, 1180]               0\n",
      "           Linear-13                 [-1, 1180]       1,393,580\n",
      "      BatchNorm1d-14                 [-1, 1180]           2,360\n",
      "             ReLU-15                 [-1, 1180]               0\n",
      "          Dropout-16                 [-1, 1180]               0\n",
      "           Linear-17                 [-1, 1180]       1,393,580\n",
      "      BatchNorm1d-18                 [-1, 1180]           2,360\n",
      "             ReLU-19                 [-1, 1180]               0\n",
      "          Dropout-20                 [-1, 1180]               0\n",
      "           Linear-21                 [-1, 1180]       1,393,580\n",
      "      BatchNorm1d-22                 [-1, 1180]           2,360\n",
      "             ReLU-23                 [-1, 1180]               0\n",
      "          Dropout-24                 [-1, 1180]               0\n",
      "           Linear-25                  [-1, 236]         278,716\n",
      "================================================================\n",
      "Total params: 5,935,636\n",
      "Trainable params: 5,935,636\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.18\n",
      "Params size (MB): 22.64\n",
      "Estimated Total Size (MB): 22.83\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# class dnn(torch.nn.Module):\n",
    "#   def __init__(self,shape,dropout=0):\n",
    "#     super(dnn,self).__init__()\n",
    "#     layers=[]\n",
    "#     for idx in range(len(shape)-2):\n",
    "#       layers.extend([\n",
    "#         nn.Linear(shape[idx],shape[idx+1]),\n",
    "#         nn.ReLU(),\n",
    "#         nn.BatchNorm1d(shape[idx+1]),\n",
    "#         nn.Dropout(dropout)\n",
    "#       ])\n",
    "#     layers.append(nn.Linear(shape[-2],shape[-1]))\n",
    "#     self.features=nn.Sequential(*layers)\n",
    "#     # initialize\n",
    "#     for temp in self.features:\n",
    "#       if type(temp)==nn.Linear:\n",
    "#         torch.nn.init.normal_(temp.weight,mean=0,std=1)\n",
    "#   def forward(self,x):\n",
    "#     return self.features(x)\n",
    "# net=dnn([118*6,118*5,118*5,118*10,118*10,118*10,118*10,118]).to(device)\n",
    "# print('number of params: %d'%(sum(temp.numel() for temp in net.parameters() if temp.requires_grad)))\n",
    "from torchsummary import summary\n",
    "class dnn(torch.nn.Module):\n",
    "    def __init__(self, shape):\n",
    "        super(dnn, self).__init__()\n",
    "        layers = []\n",
    "        for idx in range(len(shape) - 2):\n",
    "            layers.extend([\n",
    "                nn.Linear(shape[idx], shape[idx+1]),\n",
    "                nn.BatchNorm1d(num_features=shape[idx+1]),  # 確保 num_features 正確\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5),\n",
    "            ])\n",
    "        layers.append(nn.Linear(shape[-2], shape[-1]))  # 最後一層\n",
    "        self.features = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # 保證 batch_size 在第一維\n",
    "        return self.features(x)\n",
    "\n",
    "# 測試模型\n",
    "print('n_bus:',n_bus)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#net=dnn([n_bus*6,n_bus*10,n_bus*10,n_bus*10,n_bus*2]).to(device) #source code\n",
    "net = dnn([n_bus*6, n_bus*5, n_bus*5, n_bus*10, n_bus*10, n_bus*10, n_bus*10, n_bus*2]).to(device)#my code\n",
    "\n",
    "\n",
    "# 打印總參數數量\n",
    "total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "\n",
    "summary(net, (1, n_bus*6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "GEQ-MDKOTqW1"
   },
   "outputs": [],
   "source": [
    "# threshold function for p_g\n",
    "class my_gen_pred_binary(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(my_gen_pred_binary,self).__init__()\n",
    "  def forward(self,x,thresh):\n",
    "    right_thresh=thresh.clone().detach().requires_grad_(True).double()\n",
    "    left_thresh=torch.tensor(0).double()\n",
    "    x=x.double()\n",
    "    output = torch.sigmoid(left_thresh - x)\n",
    "    output = torch.mul(output,left_thresh - x) + x\n",
    "    output = torch.sigmoid(output - right_thresh)\n",
    "    output = torch.mul(output,output - right_thresh) + right_thresh\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9mYCMszHaosA"
   },
   "outputs": [],
   "source": [
    "## params needed for S calculation\n",
    "# line parameters\n",
    "filename1 = root+'ieee118_lineloc.txt'\n",
    "filename2 = root+'ieee118_lineparams.txt'\n",
    "filename3 = root+'ieee118_Bmat.txt'\n",
    "# incidence info\n",
    "line_loc = pd.read_table(filename1,sep=',',header=None).to_numpy()\n",
    "# r, x, shunt, S_max\n",
    "line_params = pd.read_table(filename2,sep=',',header=None).to_numpy()\n",
    "B_mat=pd.read_table(filename3,sep=',',header=None).to_numpy()\n",
    "B_r = np.delete(B_mat,68,axis=0)\n",
    "B_r = np.delete(B_r,68,axis=1)\n",
    "Br_inv = np.linalg.inv(B_r)\n",
    "\n",
    "R_line = line_params[:,0].copy()\n",
    "X_line = line_params[:,1].copy()\n",
    "\n",
    "B_shunt = line_params[:,2].copy()\n",
    "\n",
    "Z_line = R_line + 1j * X_line \n",
    "Y_line = 1 / Z_line\n",
    "\n",
    "G_line = np.real(Y_line)\n",
    "B_line = np.imag(Y_line)\n",
    "\n",
    "# transformer indicator\n",
    "a = (R_line > 0).astype(int)\n",
    "\n",
    "# params to tensor and GPU\n",
    "G_line_tensor = torch.from_numpy(G_line).to(device) # conductance\n",
    "B_line_tensor = torch.from_numpy(B_line).to(device) # susceptance\n",
    "B_shunt_tensor = torch.from_numpy(B_shunt/2).to(device) # conductance\n",
    "Br_inv_tensor = torch.from_numpy(Br_inv).to(device) # reduced Bbus matrix\n",
    "a_tensor = torch.from_numpy(a).double().to(device) # line/transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Go4bwoEmoi0D"
   },
   "outputs": [],
   "source": [
    "class loss_func:\n",
    "    def __init__(self,s_max,G_line,B_line,B_shunt,Br_inv,a,line_loc):\n",
    "      self.s=s_max\n",
    "      self.g=G_line\n",
    "      self.b=B_line\n",
    "      self.c=B_shunt\n",
    "      self.r=Br_inv\n",
    "      self.a=a\n",
    "      self.mse=nn.MSELoss() # MSE loss\n",
    "      self.lmda1=torch.tensor(10).to(device) # V MSE \n",
    "      self.lmda2=torch.tensor(1).to(device) # pi MSE \n",
    "      self.lmda3=torch.tensor(0.1).to(device) # v l_inf\n",
    "      self.lmda4=torch.tensor(0.1).to(device) # s feasibility\n",
    "      self.lmda5=torch.tensor(0.01).to(device) # pi l_inf\n",
    "      self.line_loc=line_loc\n",
    "      self.binary_cell=my_gen_pred_binary()\n",
    "    def calc(self,pred,label,x,feas):\n",
    "      mse_p=self.mse(pred[:,:n_bus],label[:,:n_bus])\n",
    "      mse_v=self.mse(pred[:,n_bus:],label[:,n_bus:])\n",
    "      linf_p=(pred[:,:n_bus]-label[:,:n_bus]).norm(p=float('inf'))\n",
    "      linf_v=(pred[:,n_bus:]-label[:,n_bus:]).norm(p=float('inf'))\n",
    "      if feas==False:\n",
    "        return self.lmda1*mse_v+self.lmda2*mse_p+self.lmda3*linf_v+self.lmda5*linf_p\n",
    "      label_pred=pred[:,:n_bus]\n",
    "      p_max=x[:,:n_bus*1]-x[:,n_bus*1:n_bus*2]\n",
    "      quadratic_b=x[:,n_bus*4:n_bus*5]\n",
    "      quadratic_a=x[:,n_bus*5:n_bus*6]\n",
    "      quadratic_center=(label_pred-quadratic_b)/(quadratic_a+1e-5)/2\n",
    "      p_inj=self.binary_cell(quadratic_center,p_max)\n",
    "      bus_inj=p_inj+x[:,n_bus:n_bus*2]\n",
    "      p_inj_r=torch.cat((bus_inj[:,:68],bus_inj[:,69:]),1)/100\n",
    "      theta0=torch.matmul(self.r,p_inj_r.T)\n",
    "      ref_ang=torch.zeros(1,theta0.shape[1]).to(device)\n",
    "      theta=torch.cat([theta0[:68,:],ref_ang,theta0[68:,:]],0)\n",
    "      v_pred=(pred[:,n_bus:].transpose(0,1))*0.01+0.9\n",
    "      \n",
    "      # s penalty\n",
    "      theta1=theta[self.line_loc[:,0]-1,:]\n",
    "      theta2=theta[self.line_loc[:,1]-1,:]\n",
    "      V1=(v_pred[self.line_loc[:,0]-1,:]).double()\n",
    "      V2=(v_pred[self.line_loc[:,1]-1,:]).double()\n",
    "      f_p=(self.a*self.g*(V1*V1).T)-self.a*((V1*V2).T)*(self.g*torch.cos(theta1-theta2).T+self.b*torch.sin(theta1-theta2).T)\n",
    "      f_p=f_p.T\n",
    "      f_q=-self.a*(V1.T)*(self.a*V1.T)*(self.b+self.c/2)+self.a*((V1*V2).T)*(self.b*torch.cos(theta1-theta2).T-self.g*torch.sin(theta1-theta2).T)\n",
    "      f_q=f_q.T\n",
    "      s_pred=torch.sqrt(f_p*f_p+f_q*f_q+1e-5)*100\n",
    "      s_penalty=torch.sigmoid(s_pred-self.s)+torch.sigmoid(-s_pred-self.s)\n",
    "      s_total=torch.sum(s_penalty)\n",
    "\n",
    "      # sji penalty\n",
    "      theta1=theta[self.line_loc[:,1]-1,:]\n",
    "      theta2=theta[self.line_loc[:,0]-1,:]\n",
    "      V1=(v_pred[self.line_loc[:,1]-1,:]).double()\n",
    "      V2=(v_pred[self.line_loc[:,0]-1,:]).double() \n",
    "      fji_p=(self.a*self.g*(V1*V1).T)-self.a*((V1*V2).T)*(self.g*torch.cos(theta1-theta2).T+self.b*torch.sin(theta1-theta2).T)\n",
    "      fji_p=fji_p.T\n",
    "      fji_q=-self.a*(V1.T)*(self.a*V1.T)*(self.b+self.c/2)+self.a*((V1*V2).T)*(self.b*torch.cos(theta1-theta2).T-self.g*torch.sin(theta1-theta2).T)\n",
    "      fji_q=fji_q.T\n",
    "      sji_pred=torch.sqrt(fji_p*fji_p+fji_q*fji_q+1e-5)*100\n",
    "      sji_penalty=torch.sigmoid(sji_pred-self.s)+torch.sigmoid(-sji_pred-self.s)\n",
    "      sji_total=torch.sum(sji_penalty)\n",
    "\n",
    "      return self.lmda1*mse_v+self.lmda2*mse_p+self.lmda3*linf_v+self.lmda5*linf_p+self.lmda4*s_total+self.lmda4*sji_total\n",
    "my_loss=loss_func(f_max,G_line_tensor,B_line_tensor,B_shunt_tensor,Br_inv_tensor,a_tensor,line_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cold start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\AC_OPF\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Training loss: 402.8470 | Learning Rate: 0.001000\n",
      "Epoch 0 | Validation loss: 407.9184 | Learning Rate: 0.001000\n",
      "Epoch 1 | Training loss: 387.7569 | Learning Rate: 0.001000\n",
      "Epoch 2 | Training loss: 373.0236 | Learning Rate: 0.001000\n",
      "Epoch 3 | Training loss: 357.0128 | Learning Rate: 0.001000\n",
      "Epoch 4 | Training loss: 340.0839 | Learning Rate: 0.001000\n",
      "Epoch 5 | Training loss: 320.8031 | Learning Rate: 0.001000\n",
      "Epoch 5 | Validation loss: 272.8796 | Learning Rate: 0.001000\n",
      "Epoch 6 | Training loss: 293.7709 | Learning Rate: 0.001000\n",
      "Epoch 7 | Training loss: 263.6003 | Learning Rate: 0.001000\n",
      "Epoch 8 | Training loss: 238.2100 | Learning Rate: 0.001000\n",
      "Epoch 9 | Training loss: 214.2452 | Learning Rate: 0.001000\n",
      "Epoch 10 | Training loss: 191.0469 | Learning Rate: 0.001000\n",
      "Epoch 10 | Validation loss: 176.6790 | Learning Rate: 0.001000\n",
      "Epoch 11 | Training loss: 168.8224 | Learning Rate: 0.001000\n",
      "Epoch 12 | Training loss: 147.7256 | Learning Rate: 0.001000\n",
      "Epoch 13 | Training loss: 128.0412 | Learning Rate: 0.001000\n",
      "Epoch 14 | Training loss: 109.5717 | Learning Rate: 0.001000\n",
      "Epoch 15 | Training loss: 92.8887 | Learning Rate: 0.001000\n",
      "Epoch 15 | Validation loss: 77.0207 | Learning Rate: 0.001000\n",
      "Epoch 16 | Training loss: 77.8186 | Learning Rate: 0.001000\n",
      "Epoch 17 | Training loss: 64.6278 | Learning Rate: 0.001000\n",
      "Epoch 18 | Training loss: 53.2038 | Learning Rate: 0.001000\n",
      "Epoch 19 | Training loss: 42.7808 | Learning Rate: 0.001000\n",
      "Epoch 20 | Training loss: 34.1224 | Learning Rate: 0.001000\n",
      "Epoch 20 | Validation loss: 35.1677 | Learning Rate: 0.001000\n",
      "Epoch 21 | Training loss: 26.9489 | Learning Rate: 0.001000\n",
      "Epoch 22 | Training loss: 21.3633 | Learning Rate: 0.001000\n",
      "Epoch 23 | Training loss: 16.8995 | Learning Rate: 0.001000\n",
      "Epoch 24 | Training loss: 13.3639 | Learning Rate: 0.001000\n",
      "Epoch 25 | Training loss: 10.5078 | Learning Rate: 0.001000\n",
      "Epoch 25 | Validation loss: 10.0678 | Learning Rate: 0.001000\n",
      "Epoch 26 | Training loss: 8.6717 | Learning Rate: 0.001000\n",
      "Epoch 27 | Training loss: 6.9244 | Learning Rate: 0.001000\n",
      "Epoch 28 | Training loss: 5.8806 | Learning Rate: 0.001000\n",
      "Epoch 29 | Training loss: 5.0221 | Learning Rate: 0.001000\n",
      "Epoch 30 | Training loss: 4.3656 | Learning Rate: 0.001000\n",
      "Epoch 30 | Validation loss: 2.2645 | Learning Rate: 0.001000\n",
      "Epoch 31 | Training loss: 3.8439 | Learning Rate: 0.001000\n",
      "Epoch 32 | Training loss: 3.5592 | Learning Rate: 0.001000\n",
      "Epoch 33 | Training loss: 3.2120 | Learning Rate: 0.001000\n",
      "Epoch 34 | Training loss: 3.0790 | Learning Rate: 0.001000\n",
      "Epoch 35 | Training loss: 2.9453 | Learning Rate: 0.001000\n",
      "Epoch 35 | Validation loss: 1.6978 | Learning Rate: 0.001000\n",
      "Epoch 36 | Training loss: 2.8968 | Learning Rate: 0.001000\n",
      "Epoch 37 | Training loss: 2.6796 | Learning Rate: 0.001000\n",
      "Epoch 38 | Training loss: 2.5894 | Learning Rate: 0.001000\n",
      "Epoch 39 | Training loss: 2.5064 | Learning Rate: 0.001000\n",
      "Epoch 40 | Training loss: 2.6085 | Learning Rate: 0.001000\n",
      "Epoch 40 | Validation loss: 1.4877 | Learning Rate: 0.001000\n",
      "Epoch 41 | Training loss: 2.3720 | Learning Rate: 0.001000\n",
      "Epoch 42 | Training loss: 2.4166 | Learning Rate: 0.001000\n",
      "Epoch 43 | Training loss: 2.3241 | Learning Rate: 0.001000\n",
      "Epoch 44 | Training loss: 2.2888 | Learning Rate: 0.001000\n",
      "Epoch 45 | Training loss: 2.2773 | Learning Rate: 0.001000\n",
      "Epoch 45 | Validation loss: 1.3921 | Learning Rate: 0.001000\n",
      "Epoch 46 | Training loss: 2.2027 | Learning Rate: 0.001000\n",
      "Epoch 47 | Training loss: 2.2213 | Learning Rate: 0.001000\n",
      "Epoch 48 | Training loss: 2.1991 | Learning Rate: 0.001000\n",
      "Epoch 49 | Training loss: 2.1915 | Learning Rate: 0.001000\n",
      "Epoch 50 | Training loss: 2.1829 | Learning Rate: 0.001000\n",
      "Epoch 50 | Validation loss: 1.3220 | Learning Rate: 0.001000\n",
      "Epoch 51 | Training loss: 2.1385 | Learning Rate: 0.001000\n",
      "Epoch 52 | Training loss: 2.1233 | Learning Rate: 0.001000\n",
      "Epoch 53 | Training loss: 2.1522 | Learning Rate: 0.001000\n",
      "Epoch 54 | Training loss: 2.1331 | Learning Rate: 0.001000\n",
      "Epoch 55 | Training loss: 2.1641 | Learning Rate: 0.001000\n",
      "Epoch 55 | Validation loss: 1.3194 | Learning Rate: 0.001000\n",
      "Epoch 56 | Training loss: 2.1480 | Learning Rate: 0.001000\n",
      "Epoch 57 | Training loss: 2.0891 | Learning Rate: 0.001000\n",
      "Epoch 58 | Training loss: 2.1067 | Learning Rate: 0.001000\n",
      "Epoch 59 | Training loss: 2.0738 | Learning Rate: 0.001000\n",
      "Epoch 60 | Training loss: 2.0804 | Learning Rate: 0.001000\n",
      "Epoch 60 | Validation loss: 1.2730 | Learning Rate: 0.001000\n",
      "Epoch 61 | Training loss: 2.0417 | Learning Rate: 0.001000\n",
      "Epoch 62 | Training loss: 2.0397 | Learning Rate: 0.001000\n",
      "Epoch 63 | Training loss: 2.0372 | Learning Rate: 0.001000\n",
      "Epoch 64 | Training loss: 2.0997 | Learning Rate: 0.001000\n",
      "Epoch 65 | Training loss: 2.0863 | Learning Rate: 0.001000\n",
      "Epoch 65 | Validation loss: 1.2737 | Learning Rate: 0.001000\n",
      "Epoch 66 | Training loss: 2.0032 | Learning Rate: 0.001000\n",
      "Epoch 67 | Training loss: 1.9699 | Learning Rate: 0.001000\n",
      "Epoch 68 | Training loss: 2.0003 | Learning Rate: 0.001000\n",
      "Epoch 69 | Training loss: 2.0100 | Learning Rate: 0.001000\n",
      "Epoch 70 | Training loss: 1.9875 | Learning Rate: 0.001000\n",
      "Epoch 70 | Validation loss: 1.3242 | Learning Rate: 0.001000\n",
      "Epoch 71 | Training loss: 1.9906 | Learning Rate: 0.001000\n",
      "Epoch 72 | Training loss: 2.0460 | Learning Rate: 0.001000\n",
      "Epoch 73 | Training loss: 1.9753 | Learning Rate: 0.001000\n",
      "Epoch 74 | Training loss: 2.0380 | Learning Rate: 0.001000\n",
      "Epoch 75 | Training loss: 1.9986 | Learning Rate: 0.001000\n",
      "Epoch 75 | Validation loss: 1.3121 | Learning Rate: 0.001000\n",
      "Epoch 76 | Training loss: 1.9709 | Learning Rate: 0.001000\n",
      "Epoch 77 | Training loss: 1.9364 | Learning Rate: 0.001000\n",
      "Epoch 78 | Training loss: 1.9533 | Learning Rate: 0.001000\n",
      "Epoch 79 | Training loss: 1.9627 | Learning Rate: 0.001000\n",
      "Epoch 80 | Training loss: 1.9702 | Learning Rate: 0.001000\n",
      "Epoch 80 | Validation loss: 1.2656 | Learning Rate: 0.001000\n",
      "Epoch 81 | Training loss: 1.9890 | Learning Rate: 0.001000\n",
      "Epoch 82 | Training loss: 1.9861 | Learning Rate: 0.001000\n",
      "Epoch 83 | Training loss: 1.9373 | Learning Rate: 0.001000\n",
      "Epoch 84 | Training loss: 1.8904 | Learning Rate: 0.001000\n",
      "Epoch 85 | Training loss: 1.9541 | Learning Rate: 0.001000\n",
      "Epoch 85 | Validation loss: 1.3011 | Learning Rate: 0.001000\n",
      "Epoch 86 | Training loss: 1.9894 | Learning Rate: 0.001000\n",
      "Epoch 87 | Training loss: 1.9643 | Learning Rate: 0.001000\n",
      "Epoch 88 | Training loss: 1.9218 | Learning Rate: 0.001000\n",
      "Epoch 89 | Training loss: 1.8924 | Learning Rate: 0.001000\n",
      "Epoch 90 | Training loss: 1.9044 | Learning Rate: 0.001000\n",
      "Epoch 90 | Validation loss: 1.5891 | Learning Rate: 0.001000\n",
      "Epoch 91 | Training loss: 1.9017 | Learning Rate: 0.001000\n",
      "Epoch 92 | Training loss: 1.8611 | Learning Rate: 0.001000\n",
      "Epoch 93 | Training loss: 1.8728 | Learning Rate: 0.001000\n",
      "Epoch 94 | Training loss: 1.8705 | Learning Rate: 0.001000\n",
      "Epoch 95 | Training loss: 1.8655 | Learning Rate: 0.001000\n",
      "Epoch 95 | Validation loss: 1.3456 | Learning Rate: 0.001000\n",
      "Epoch 96 | Training loss: 1.9010 | Learning Rate: 0.001000\n",
      "Epoch 97 | Training loss: 1.8683 | Learning Rate: 0.001000\n",
      "Epoch 98 | Training loss: 1.8776 | Learning Rate: 0.001000\n",
      "Epoch 99 | Training loss: 1.8756 | Learning Rate: 0.001000\n",
      "Epoch 100 | Training loss: 1.8463 | Learning Rate: 0.001000\n",
      "Epoch 100 | Validation loss: 1.3005 | Learning Rate: 0.001000\n",
      "Epoch 101 | Training loss: 1.8835 | Learning Rate: 0.001000\n",
      "Epoch 102 | Training loss: 1.9199 | Learning Rate: 0.001000\n",
      "Epoch 103 | Training loss: 1.8450 | Learning Rate: 0.001000\n",
      "Epoch 104 | Training loss: 1.8600 | Learning Rate: 0.001000\n",
      "Epoch 105 | Training loss: 1.8023 | Learning Rate: 0.001000\n",
      "Epoch 105 | Validation loss: 1.2086 | Learning Rate: 0.001000\n",
      "Epoch 106 | Training loss: 1.8219 | Learning Rate: 0.001000\n",
      "Epoch 107 | Training loss: 1.8248 | Learning Rate: 0.001000\n",
      "Epoch 108 | Training loss: 1.7811 | Learning Rate: 0.001000\n",
      "Epoch 109 | Training loss: 1.8059 | Learning Rate: 0.001000\n",
      "Epoch 110 | Training loss: 1.7388 | Learning Rate: 0.001000\n",
      "Epoch 110 | Validation loss: 1.5051 | Learning Rate: 0.001000\n",
      "Epoch 111 | Training loss: 1.8108 | Learning Rate: 0.001000\n",
      "Epoch 112 | Training loss: 1.7919 | Learning Rate: 0.001000\n",
      "Epoch 113 | Training loss: 1.7724 | Learning Rate: 0.001000\n",
      "Epoch 114 | Training loss: 1.7099 | Learning Rate: 0.001000\n",
      "Epoch 115 | Training loss: 1.7385 | Learning Rate: 0.001000\n",
      "Epoch 115 | Validation loss: 2.3973 | Learning Rate: 0.001000\n",
      "Epoch 116 | Training loss: 1.7196 | Learning Rate: 0.001000\n",
      "Epoch 117 | Training loss: 1.8173 | Learning Rate: 0.001000\n",
      "Epoch 118 | Training loss: 1.7878 | Learning Rate: 0.001000\n",
      "Epoch 119 | Training loss: 1.7427 | Learning Rate: 0.001000\n",
      "Epoch 120 | Training loss: 1.7472 | Learning Rate: 0.001000\n",
      "Epoch 120 | Validation loss: 1.9740 | Learning Rate: 0.001000\n",
      "Epoch 121 | Training loss: 1.7723 | Learning Rate: 0.001000\n",
      "Epoch 122 | Training loss: 1.7351 | Learning Rate: 0.001000\n",
      "Epoch 123 | Training loss: 1.6703 | Learning Rate: 0.001000\n",
      "Epoch 124 | Training loss: 1.6954 | Learning Rate: 0.001000\n",
      "Epoch 125 | Training loss: 1.7083 | Learning Rate: 0.001000\n",
      "Epoch 125 | Validation loss: 2.1764 | Learning Rate: 0.001000\n",
      "Epoch 126 | Training loss: 1.7015 | Learning Rate: 0.001000\n",
      "Epoch 127 | Training loss: 1.6618 | Learning Rate: 0.001000\n",
      "Epoch 128 | Training loss: 1.6441 | Learning Rate: 0.001000\n",
      "Epoch 129 | Training loss: 1.6678 | Learning Rate: 0.001000\n",
      "Epoch 130 | Training loss: 1.6766 | Learning Rate: 0.001000\n",
      "Epoch 130 | Validation loss: 2.4438 | Learning Rate: 0.001000\n",
      "Epoch 131 | Training loss: 1.7309 | Learning Rate: 0.001000\n",
      "Epoch 132 | Training loss: 1.6615 | Learning Rate: 0.001000\n",
      "Epoch 133 | Training loss: 1.6833 | Learning Rate: 0.001000\n",
      "Epoch 134 | Training loss: 1.6600 | Learning Rate: 0.001000\n",
      "Epoch 135 | Training loss: 1.6560 | Learning Rate: 0.001000\n",
      "Epoch 135 | Validation loss: 1.2692 | Learning Rate: 0.001000\n",
      "Epoch 136 | Training loss: 1.5649 | Learning Rate: 0.000900\n",
      "Epoch 137 | Training loss: 1.5964 | Learning Rate: 0.000900\n",
      "Epoch 138 | Training loss: 1.5965 | Learning Rate: 0.000900\n",
      "Epoch 139 | Training loss: 1.6062 | Learning Rate: 0.000900\n",
      "Epoch 140 | Training loss: 1.5932 | Learning Rate: 0.000900\n",
      "Epoch 140 | Validation loss: 1.2179 | Learning Rate: 0.000900\n",
      "Epoch 141 | Training loss: 1.5803 | Learning Rate: 0.000900\n",
      "Epoch 142 | Training loss: 1.5609 | Learning Rate: 0.000900\n",
      "Epoch 143 | Training loss: 1.5877 | Learning Rate: 0.000900\n",
      "Epoch 144 | Training loss: 1.6403 | Learning Rate: 0.000900\n",
      "Epoch 145 | Training loss: 1.5336 | Learning Rate: 0.000900\n",
      "Epoch 145 | Validation loss: 3.1855 | Learning Rate: 0.000900\n",
      "Epoch 146 | Training loss: 1.5389 | Learning Rate: 0.000900\n",
      "Epoch 147 | Training loss: 1.5293 | Learning Rate: 0.000900\n",
      "Epoch 148 | Training loss: 1.5568 | Learning Rate: 0.000900\n",
      "Epoch 149 | Training loss: 1.5651 | Learning Rate: 0.000900\n",
      "Epoch 150 | Training loss: 1.5608 | Learning Rate: 0.000900\n",
      "Epoch 150 | Validation loss: 3.0783 | Learning Rate: 0.000900\n",
      "Epoch 151 | Training loss: 1.5578 | Learning Rate: 0.000900\n",
      "Epoch 152 | Training loss: 1.5671 | Learning Rate: 0.000900\n",
      "Epoch 153 | Training loss: 1.5327 | Learning Rate: 0.000900\n",
      "Epoch 154 | Training loss: 1.5650 | Learning Rate: 0.000900\n",
      "Epoch 155 | Training loss: 1.4861 | Learning Rate: 0.000900\n",
      "Epoch 155 | Validation loss: 1.4237 | Learning Rate: 0.000900\n",
      "Epoch 156 | Training loss: 1.4998 | Learning Rate: 0.000900\n",
      "Epoch 157 | Training loss: 1.5627 | Learning Rate: 0.000900\n",
      "Epoch 158 | Training loss: 1.5221 | Learning Rate: 0.000900\n",
      "Epoch 159 | Training loss: 1.5304 | Learning Rate: 0.000900\n",
      "Epoch 160 | Training loss: 1.5260 | Learning Rate: 0.000900\n",
      "Epoch 160 | Validation loss: 2.0226 | Learning Rate: 0.000900\n",
      "Epoch 161 | Training loss: 1.5028 | Learning Rate: 0.000900\n",
      "Epoch 162 | Training loss: 1.5198 | Learning Rate: 0.000900\n",
      "Epoch 163 | Training loss: 1.5145 | Learning Rate: 0.000900\n",
      "Epoch 164 | Training loss: 1.4632 | Learning Rate: 0.000900\n",
      "Epoch 165 | Training loss: 1.4620 | Learning Rate: 0.000900\n",
      "Epoch 165 | Validation loss: 1.3148 | Learning Rate: 0.000900\n",
      "Epoch 166 | Training loss: 1.4409 | Learning Rate: 0.000810\n",
      "Epoch 167 | Training loss: 1.4339 | Learning Rate: 0.000810\n",
      "Epoch 168 | Training loss: 1.4354 | Learning Rate: 0.000810\n",
      "Epoch 169 | Training loss: 1.4482 | Learning Rate: 0.000810\n",
      "Epoch 170 | Training loss: 1.5085 | Learning Rate: 0.000810\n",
      "Epoch 170 | Validation loss: 2.2092 | Learning Rate: 0.000810\n",
      "Epoch 171 | Training loss: 1.4147 | Learning Rate: 0.000810\n",
      "Epoch 172 | Training loss: 1.4271 | Learning Rate: 0.000810\n",
      "Epoch 173 | Training loss: 1.4326 | Learning Rate: 0.000810\n",
      "Epoch 174 | Training loss: 1.4224 | Learning Rate: 0.000810\n",
      "Epoch 175 | Training loss: 1.4058 | Learning Rate: 0.000810\n",
      "Epoch 175 | Validation loss: 1.1265 | Learning Rate: 0.000810\n",
      "Epoch 176 | Training loss: 1.4371 | Learning Rate: 0.000810\n",
      "Epoch 177 | Training loss: 1.4067 | Learning Rate: 0.000810\n",
      "Epoch 178 | Training loss: 1.4277 | Learning Rate: 0.000810\n",
      "Epoch 179 | Training loss: 1.4060 | Learning Rate: 0.000810\n",
      "Epoch 180 | Training loss: 1.4028 | Learning Rate: 0.000810\n",
      "Epoch 180 | Validation loss: 1.1467 | Learning Rate: 0.000810\n",
      "Epoch 181 | Training loss: 1.4471 | Learning Rate: 0.000810\n",
      "Epoch 182 | Training loss: 1.4256 | Learning Rate: 0.000810\n",
      "Epoch 183 | Training loss: 1.4072 | Learning Rate: 0.000810\n",
      "Epoch 184 | Training loss: 1.4093 | Learning Rate: 0.000810\n",
      "Epoch 185 | Training loss: 1.3720 | Learning Rate: 0.000810\n",
      "Epoch 185 | Validation loss: 1.5020 | Learning Rate: 0.000810\n",
      "Epoch 186 | Training loss: 1.4184 | Learning Rate: 0.000810\n",
      "Epoch 187 | Training loss: 1.3956 | Learning Rate: 0.000810\n",
      "Epoch 188 | Training loss: 1.3917 | Learning Rate: 0.000810\n",
      "Epoch 189 | Training loss: 1.3911 | Learning Rate: 0.000810\n",
      "Epoch 190 | Training loss: 1.3778 | Learning Rate: 0.000810\n",
      "Epoch 190 | Validation loss: 1.3057 | Learning Rate: 0.000810\n",
      "Epoch 191 | Training loss: 1.3873 | Learning Rate: 0.000810\n",
      "Epoch 192 | Training loss: 1.3700 | Learning Rate: 0.000810\n",
      "Epoch 193 | Training loss: 1.3405 | Learning Rate: 0.000810\n",
      "Epoch 194 | Training loss: 1.3624 | Learning Rate: 0.000810\n",
      "Epoch 195 | Training loss: 1.4004 | Learning Rate: 0.000810\n",
      "Epoch 195 | Validation loss: 3.4345 | Learning Rate: 0.000810\n",
      "Epoch 196 | Training loss: 1.4479 | Learning Rate: 0.000810\n",
      "Epoch 197 | Training loss: 1.3876 | Learning Rate: 0.000810\n",
      "Epoch 198 | Training loss: 1.3694 | Learning Rate: 0.000810\n",
      "Epoch 199 | Training loss: 1.3092 | Learning Rate: 0.000810\n",
      "Epoch 200 | Training loss: 1.3857 | Learning Rate: 0.000810\n",
      "Epoch 200 | Validation loss: 2.1463 | Learning Rate: 0.000810\n",
      "Epoch 201 | Training loss: 1.3319 | Learning Rate: 0.000810\n",
      "Epoch 202 | Training loss: 1.3177 | Learning Rate: 0.000810\n",
      "Epoch 203 | Training loss: 1.3580 | Learning Rate: 0.000810\n",
      "Epoch 204 | Training loss: 1.3114 | Learning Rate: 0.000810\n",
      "Epoch 205 | Training loss: 1.3394 | Learning Rate: 0.000810\n",
      "Epoch 205 | Validation loss: 1.3995 | Learning Rate: 0.000810\n",
      "Epoch 206 | Training loss: 1.3160 | Learning Rate: 0.000729\n",
      "Epoch 207 | Training loss: 1.3329 | Learning Rate: 0.000729\n",
      "Epoch 208 | Training loss: 1.3122 | Learning Rate: 0.000729\n",
      "Epoch 209 | Training loss: 1.3519 | Learning Rate: 0.000729\n",
      "Epoch 210 | Training loss: 1.3411 | Learning Rate: 0.000729\n",
      "Epoch 210 | Validation loss: 1.9100 | Learning Rate: 0.000729\n",
      "Epoch 211 | Training loss: 1.3109 | Learning Rate: 0.000729\n",
      "Epoch 212 | Training loss: 1.3315 | Learning Rate: 0.000729\n",
      "Epoch 213 | Training loss: 1.2934 | Learning Rate: 0.000729\n",
      "Epoch 214 | Training loss: 1.2725 | Learning Rate: 0.000729\n",
      "Epoch 215 | Training loss: 1.3039 | Learning Rate: 0.000729\n",
      "Epoch 215 | Validation loss: 1.1439 | Learning Rate: 0.000729\n",
      "Epoch 216 | Training loss: 1.2898 | Learning Rate: 0.000729\n",
      "Epoch 217 | Training loss: 1.3076 | Learning Rate: 0.000729\n",
      "Epoch 218 | Training loss: 1.2834 | Learning Rate: 0.000729\n",
      "Epoch 219 | Training loss: 1.2786 | Learning Rate: 0.000729\n",
      "Epoch 220 | Training loss: 1.2929 | Learning Rate: 0.000729\n",
      "Epoch 220 | Validation loss: 1.0948 | Learning Rate: 0.000729\n",
      "Epoch 221 | Training loss: 1.2621 | Learning Rate: 0.000729\n",
      "Epoch 222 | Training loss: 1.2856 | Learning Rate: 0.000729\n",
      "Epoch 223 | Training loss: 1.2466 | Learning Rate: 0.000729\n",
      "Epoch 224 | Training loss: 1.2276 | Learning Rate: 0.000729\n",
      "Epoch 225 | Training loss: 1.2579 | Learning Rate: 0.000729\n",
      "Epoch 225 | Validation loss: 1.3602 | Learning Rate: 0.000729\n",
      "Epoch 226 | Training loss: 1.2488 | Learning Rate: 0.000729\n",
      "Epoch 227 | Training loss: 1.2634 | Learning Rate: 0.000729\n",
      "Epoch 228 | Training loss: 1.2898 | Learning Rate: 0.000729\n",
      "Epoch 229 | Training loss: 1.2606 | Learning Rate: 0.000729\n",
      "Epoch 230 | Training loss: 1.2347 | Learning Rate: 0.000729\n",
      "Epoch 230 | Validation loss: 1.2240 | Learning Rate: 0.000729\n",
      "Epoch 231 | Training loss: 1.2637 | Learning Rate: 0.000729\n",
      "Epoch 232 | Training loss: 1.2116 | Learning Rate: 0.000729\n",
      "Epoch 233 | Training loss: 1.2397 | Learning Rate: 0.000729\n",
      "Epoch 234 | Training loss: 1.2796 | Learning Rate: 0.000729\n",
      "Epoch 235 | Training loss: 1.2823 | Learning Rate: 0.000729\n",
      "Epoch 235 | Validation loss: 1.1274 | Learning Rate: 0.000729\n",
      "Epoch 236 | Training loss: 1.2143 | Learning Rate: 0.000729\n",
      "Epoch 237 | Training loss: 1.2381 | Learning Rate: 0.000729\n",
      "Epoch 238 | Training loss: 1.2458 | Learning Rate: 0.000729\n",
      "Epoch 239 | Training loss: 1.2059 | Learning Rate: 0.000729\n",
      "Epoch 240 | Training loss: 1.2175 | Learning Rate: 0.000729\n",
      "Epoch 240 | Validation loss: 1.0402 | Learning Rate: 0.000729\n",
      "Epoch 241 | Training loss: 1.2441 | Learning Rate: 0.000729\n",
      "Epoch 242 | Training loss: 1.2241 | Learning Rate: 0.000729\n",
      "Epoch 243 | Training loss: 1.2013 | Learning Rate: 0.000729\n",
      "Epoch 244 | Training loss: 1.2165 | Learning Rate: 0.000729\n",
      "Epoch 245 | Training loss: 1.1992 | Learning Rate: 0.000729\n",
      "Epoch 245 | Validation loss: 1.2230 | Learning Rate: 0.000729\n",
      "Epoch 246 | Training loss: 1.2239 | Learning Rate: 0.000729\n",
      "Epoch 247 | Training loss: 1.1907 | Learning Rate: 0.000729\n",
      "Epoch 248 | Training loss: 1.1939 | Learning Rate: 0.000729\n",
      "Epoch 249 | Training loss: 1.1943 | Learning Rate: 0.000729\n",
      "Epoch 250 | Training loss: 1.2606 | Learning Rate: 0.000729\n",
      "Epoch 250 | Validation loss: 1.0559 | Learning Rate: 0.000729\n",
      "Epoch 251 | Training loss: 1.2313 | Learning Rate: 0.000729\n",
      "Epoch 252 | Training loss: 1.2444 | Learning Rate: 0.000729\n",
      "Epoch 253 | Training loss: 1.2571 | Learning Rate: 0.000729\n",
      "Epoch 254 | Training loss: 1.1997 | Learning Rate: 0.000729\n",
      "Epoch 255 | Training loss: 1.2518 | Learning Rate: 0.000729\n",
      "Epoch 255 | Validation loss: 1.5715 | Learning Rate: 0.000729\n",
      "Epoch 256 | Training loss: 1.2044 | Learning Rate: 0.000729\n",
      "Epoch 257 | Training loss: 1.1767 | Learning Rate: 0.000729\n",
      "Epoch 258 | Training loss: 1.1920 | Learning Rate: 0.000729\n",
      "Epoch 259 | Training loss: 1.1956 | Learning Rate: 0.000729\n",
      "Epoch 260 | Training loss: 1.2002 | Learning Rate: 0.000729\n",
      "Epoch 260 | Validation loss: 1.7696 | Learning Rate: 0.000729\n",
      "Epoch 261 | Training loss: 1.2178 | Learning Rate: 0.000729\n",
      "Epoch 262 | Training loss: 1.2015 | Learning Rate: 0.000729\n",
      "Epoch 263 | Training loss: 1.2008 | Learning Rate: 0.000729\n",
      "Epoch 264 | Training loss: 1.2130 | Learning Rate: 0.000729\n",
      "Epoch 265 | Training loss: 1.1823 | Learning Rate: 0.000729\n",
      "Epoch 265 | Validation loss: 2.1535 | Learning Rate: 0.000729\n",
      "Epoch 266 | Training loss: 1.1721 | Learning Rate: 0.000729\n",
      "Epoch 267 | Training loss: 1.1910 | Learning Rate: 0.000729\n",
      "Epoch 268 | Training loss: 1.1925 | Learning Rate: 0.000729\n",
      "Epoch 269 | Training loss: 1.2306 | Learning Rate: 0.000729\n",
      "Epoch 270 | Training loss: 1.1852 | Learning Rate: 0.000729\n",
      "Epoch 270 | Validation loss: 1.0568 | Learning Rate: 0.000729\n",
      "Epoch 271 | Training loss: 1.1480 | Learning Rate: 0.000656\n",
      "Epoch 272 | Training loss: 1.1741 | Learning Rate: 0.000656\n",
      "Epoch 273 | Training loss: 1.1505 | Learning Rate: 0.000656\n",
      "Epoch 274 | Training loss: 1.1429 | Learning Rate: 0.000656\n",
      "Epoch 275 | Training loss: 1.1580 | Learning Rate: 0.000656\n",
      "Epoch 275 | Validation loss: 1.0255 | Learning Rate: 0.000656\n",
      "Epoch 276 | Training loss: 1.1543 | Learning Rate: 0.000656\n",
      "Epoch 277 | Training loss: 1.1611 | Learning Rate: 0.000656\n",
      "Epoch 278 | Training loss: 1.1531 | Learning Rate: 0.000656\n",
      "Epoch 279 | Training loss: 1.1563 | Learning Rate: 0.000656\n",
      "Epoch 280 | Training loss: 1.1460 | Learning Rate: 0.000656\n",
      "Epoch 280 | Validation loss: 1.9737 | Learning Rate: 0.000656\n",
      "Epoch 281 | Training loss: 1.1595 | Learning Rate: 0.000656\n",
      "Epoch 282 | Training loss: 1.1482 | Learning Rate: 0.000656\n",
      "Epoch 283 | Training loss: 1.1694 | Learning Rate: 0.000656\n",
      "Epoch 284 | Training loss: 1.1304 | Learning Rate: 0.000656\n",
      "Epoch 285 | Training loss: 1.1672 | Learning Rate: 0.000656\n",
      "Epoch 285 | Validation loss: 2.8295 | Learning Rate: 0.000656\n",
      "Epoch 286 | Training loss: 1.1397 | Learning Rate: 0.000656\n",
      "Epoch 287 | Training loss: 1.1294 | Learning Rate: 0.000656\n",
      "Epoch 288 | Training loss: 1.1501 | Learning Rate: 0.000656\n",
      "Epoch 289 | Training loss: 1.1481 | Learning Rate: 0.000656\n",
      "Epoch 290 | Training loss: 1.1091 | Learning Rate: 0.000656\n",
      "Epoch 290 | Validation loss: 1.0768 | Learning Rate: 0.000656\n",
      "Epoch 291 | Training loss: 1.0990 | Learning Rate: 0.000656\n",
      "Epoch 292 | Training loss: 1.1561 | Learning Rate: 0.000656\n",
      "Epoch 293 | Training loss: 1.1307 | Learning Rate: 0.000656\n",
      "Epoch 294 | Training loss: 1.1270 | Learning Rate: 0.000656\n",
      "Epoch 295 | Training loss: 1.1242 | Learning Rate: 0.000656\n",
      "Epoch 295 | Validation loss: 1.3949 | Learning Rate: 0.000656\n",
      "Epoch 296 | Training loss: 1.1474 | Learning Rate: 0.000656\n",
      "Epoch 297 | Training loss: 1.0727 | Learning Rate: 0.000656\n",
      "Epoch 298 | Training loss: 1.1170 | Learning Rate: 0.000656\n",
      "Epoch 299 | Training loss: 1.1078 | Learning Rate: 0.000656\n",
      "Training time: 196.5896s\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "path = root + 'data_118_quad/gnn_trained_ac118.pickle'\n",
    "try:\n",
    "    net.load_state_dict(torch.load(path))\n",
    "    print('params loaded')\n",
    "except:\n",
    "    print('cold start')\n",
    "\n",
    "# 初始學習率\n",
    "lr = 0.001\n",
    "factor = 0.9\n",
    "patience = 5\n",
    "optimizer = optim.AdamW(net.parameters(), lr)\n",
    "\n",
    "# Learning Rate Scheduler: ReduceLROnPlateau\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#     optimizer, \n",
    "#     mode='min',       # 監測 loss，越小越好\n",
    "#     factor=0.9,       # 學習率變成原來的 90%\n",
    "#     patience=5,       # 5 個 epoch 沒有改善才降低學習率\n",
    "#     verbose=True,     # 會輸出學習率變更訊息\n",
    "#     min_lr=1e-6       # 學習率最低不低於 1e-6\n",
    "# )\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min',       # 監測 loss\n",
    "    factor=factor,       # 學習率變成 50% (下降更多)\n",
    "    patience=patience,       # 3 個 epoch 沒有改善就降低學習率\n",
    "    verbose=True,     \n",
    "    min_lr=1e-6       \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "lr_list = []\n",
    "\n",
    "t0 = time.time()\n",
    "max_epochs = 300\n",
    "eval_epoch = 5\n",
    "\n",
    "tolerance = 5  # 早停耐心值\n",
    "min_delta = 1e-3\n",
    "previous = float('inf')\n",
    "\n",
    "feas = False  # 加入可行性標記\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    total_loss = 0.0\n",
    "    for local_batch, local_label in train_set:\n",
    "        optimizer.zero_grad()\n",
    "        local_batch, local_label = local_batch.to(device), local_label.to(device)\n",
    "        logits = net(local_batch)\n",
    "        loss = my_loss.calc(logits, local_label, local_batch, feas)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_set.dataset)\n",
    "    train_loss.append(avg_loss)\n",
    "    lr_list.append(scheduler.optimizer.param_groups[0]['lr'])\n",
    "    print(f\"Epoch {epoch} | Training loss: {avg_loss:.4f} | Learning Rate: {scheduler.optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    if epoch % eval_epoch == 0:\n",
    "        net.eval()\n",
    "        total_loss = 0.0\n",
    "        for local_batch, local_label in val_set:\n",
    "            local_batch, local_label = local_batch.to(device), local_label.to(device)\n",
    "            logits = net(local_batch)\n",
    "            loss = my_loss.calc(logits, local_label, local_batch, feas)\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(val_set.dataset)\n",
    "        val_loss.append([epoch, avg_loss])\n",
    "        print(f\"Epoch {epoch} | Validation loss: {avg_loss:.4f} | Learning Rate: {scheduler.optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        # 更新 learning rate\n",
    "        scheduler.step(avg_loss)\n",
    "        \n",
    "        # 早停機制\n",
    "        # if epoch:\n",
    "        #     if previous - avg_loss < min_delta:\n",
    "        #         tolerance -= 1\n",
    "        #     if tolerance == 0:\n",
    "        #         print(\"Early stopping triggered\")\n",
    "        #         break\n",
    "        previous = avg_loss\n",
    "        net.train()\n",
    "\n",
    "        final_epoch = epoch\n",
    "\n",
    "t1 = time.time()\n",
    "print(f\"Training time: {t1 - t0:.4f}s\")\n",
    "#印出 lr validation loss training loss 圖\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 60 300\n",
      "✅ 輸出內容已儲存到 C:\\Users\\USER\\Desktop\\GNN_OPF_electricity_market-main\\output_loss_fig\\trainingloss_gunplot\\02181120.txt\n",
      "✅ Plots generated successfully.\n",
      "📂 Loss plot: C:/Users/USER/Desktop/GNN_OPF_electricity_market-main/output_loss_fig/trainingloss_gunplot/loss_plot_02181120.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# **設定輸出目錄**\n",
    "output_dir = r'C:\\Users\\USER\\Desktop\\GNN_OPF_electricity_market-main\\output_loss_fig\\trainingloss_gunplot'\n",
    "timestamp = datetime.now().strftime('%m%d%H%M')\n",
    "output_file = os.path.join(output_dir, f'{timestamp}.txt')\n",
    "\n",
    "# **確保目錄存在**\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(len(train_loss), len(val_loss), len(lr_list))  # **確認長度**\n",
    "\n",
    "# **儲存數據到 txt 檔案**\n",
    "with open(output_file, 'w') as file:\n",
    "    for i in range(len(train_loss)):\n",
    "        if i % 5 == 0 and (i // 5) < len(val_loss):  # **確保 val_loss 沒有超出索引**\n",
    "            val = val_loss[i // 5]  # **正確存入數值**\n",
    "        else:\n",
    "            val = \"NaN\"  # **改成 NaN 讓 Gnuplot 忽略**\n",
    "        file.write(f\"{i} {train_loss[i]} {val[1]} {lr_list[i]}\\n\")\n",
    "\n",
    "print(f\"✅ 輸出內容已儲存到 {output_file}\")\n",
    "\n",
    "# **修正 Windows 路徑格式**\n",
    "gnuplot_file = output_file.replace(\"\\\\\", \"/\")  # **確保 Gnuplot 能識別路徑**\n",
    "loss_plot_path = os.path.join(output_dir, f'loss_plot_{timestamp}.png').replace(\"\\\\\", \"/\")\n",
    "lr_plot_path = os.path.join(output_dir, f'lr_plot_{timestamp}.png').replace(\"\\\\\", \"/\")\n",
    "\n",
    "# **定義 gnuplot 腳本**\n",
    "gnuplot_script = f\"\"\"\n",
    "set terminal pngcairo enhanced font 'Arial,12' size 800,600\n",
    "set output \"{loss_plot_path}\"\n",
    "\n",
    "set title \"Training Loss & Validation Loss\"\n",
    "set xlabel \"Step\"\n",
    "set ylabel \"Value\"\n",
    "set grid\n",
    "set key outside\n",
    "\n",
    "# **確保 Validation Loss 顯示**\n",
    "plot \"{gnuplot_file}\" using 1:2 with lines title \"Train Loss\" linecolor rgb \"blue\", \\\n",
    "     \"{gnuplot_file}\" using 1:3 with lines title \"Val Loss\" linecolor rgb \"red\"\n",
    "   \n",
    "\n",
    "# **第二張圖：Learning Rate**\n",
    "set output \"{lr_plot_path}\"\n",
    "set title \"Learning Rate\"\n",
    "set xlabel \"Step\"\n",
    "set ylabel \"Learning Rate\"\n",
    "set grid\n",
    "set key outside\n",
    "\n",
    "# **繪製 Learning Rate**\n",
    "plot \"{gnuplot_file}\" using 1:4 with lines title \"Learning Rate\" linecolor rgb \"green\" lw 2\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# **執行 gnuplot**\n",
    "try:\n",
    "    result = subprocess.run([\"gnuplot\"], input=gnuplot_script, text=True, check=True,\n",
    "                            stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    if result.stderr:\n",
    "        print(\"⚠️ Gnuplot Error:\", result.stderr)\n",
    "    else:\n",
    "        print(f\"✅ Plots generated successfully.\\n📂 Loss plot: {loss_plot_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ Error: Gnuplot is not installed or not in PATH.\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"❌ Gnuplot execution failed:\\n{e.stderr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AvUTphUSzqzA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Desktop\\GNN_OPF_electricity_market-main\\GNN_OPF_electricity_market-main\\GNN_OPF_electricity_market-main\\data_gen\\118_data\\FCNN_model\\dnn_02181120.pickle\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "timestamp=datetime.now().strftime('%m%d%H%M')\n",
    "path = 'C:\\\\Users\\\\USER\\\\Desktop\\\\GNN_OPF_electricity_market-main\\\\GNN_OPF_electricity_market-main\\\\GNN_OPF_electricity_market-main\\\\data_gen\\\\118_data\\\\FCNN_model\\\\'\n",
    "\n",
    "path = path+'dnn_%s.pickle'%(timestamp)\n",
    "if feas==False: path.replace('feas','')\n",
    "print(path)\n",
    "torch.save(net.state_dict(),path)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "L3M4jmjkscK_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "val_len = len(val_loss)\n",
    "print(val_len)\n",
    "val_plt = np.zeros((2,val_len))\n",
    "for i in range(val_len):\n",
    "  val_plt[0,i] = val_loss[i][0]\n",
    "  val_plt[1,i] = val_loss[i][1]\n",
    "\n",
    "plt.figure()\n",
    "plot_idx = np.arange(np.size(train_loss))\n",
    "plt.plot(plot_idx[5:-1],train_loss[5:-1],lw=2,label='training loss')\n",
    "plt.plot(val_plt[0,1:],val_plt[1,1:],lw=2,label='validation loss')\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "\n",
    "# 在圖的右上角標示學習率\n",
    "plt.text(0.95, 0.95, f'Learning Rate: {lr:.6f}\\nbatch size: {batch_size}', \n",
    "         transform=plt.gca().transAxes, fontsize=10, ha='right', va='top', color='red')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 儲存圖片到指定資料夾（需提供路徑）\n",
    "timestamp=datetime.now().strftime('%m%d%H%M')\n",
    "output_path = r'C:\\Users\\USER\\Desktop\\GNN_OPF_electricity_market-main\\output_loss_fig'\n",
    "output_path = output_path + '/118ac_loss_fig_%s.png' % (timestamp)\n",
    "plt.savefig(output_path, dpi=300, bbox_inches='tight')  # 儲存為 PNG 格式，解析度 300 dpi\n",
    "\n",
    "\n",
    "plt.show(block=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9uGUm4rsco3"
   },
   "source": [
    "# Evaluate the model w/ validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "4u6001UQ2pN3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset size: torch.Size([2000, 708])\n",
      "Number of validation set:  2000\n",
      "torch.Size([2000, 236])\n"
     ]
    }
   ],
   "source": [
    "n_test = np.size(x_test,0)\n",
    "x_test_feed = torch.from_numpy(x_test).float()\n",
    "x_test_feed = x_test_feed#.transpose(1,2)\n",
    "x_test_feed = x_test_feed.to(device)\n",
    "print('Validation dataset size:',x_test_feed.shape)\n",
    "print('Number of validation set: ',n_test)\n",
    "y_pred = net(x_test_feed)\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnNbSGYoXS3J"
   },
   "source": [
    "* Visualization\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKZQaqwJkn3P"
   },
   "source": [
    " - Visualize errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "7-JCo0KmXwm3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 236) (2000, 236)\n"
     ]
    }
   ],
   "source": [
    "y_pred1 = y_pred.cpu().detach()\n",
    "y_pred1 = torch.squeeze(y_pred1,1).numpy()#.transpose()\n",
    "print(y_test.shape,y_pred1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "NOeXUzrd9h9y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 2, 2000) (2000, 236)\n"
     ]
    }
   ],
   "source": [
    "# x=np.reshape(x,(x.shape[0]*x.shape[1],x.shape[2])) # reshape by samples not dim1\n",
    "# y=np.reshape(y,(y.shape[0]*y.shape[1],y.shape[2]))\n",
    "# print(x_pre.shape,y_pre.shape)\n",
    "\n",
    "y_pred_temp = y_pred1.copy().transpose()\n",
    "# y_pred2=np.reshape(y_pred2,(y_pre.shape[0],y_pre.shape[1],n_test))\n",
    "y_pred2=np.zeros([y.shape[0],y.shape[1],n_test])\n",
    "y_pred2[:,0,:]=y_pred_temp[:n_bus,:]\n",
    "y_pred2[:,1,:]=y_pred_temp[n_bus:,:]\n",
    "print(y_pred2.shape,y_pred1.shape)\n",
    "\n",
    "y_test_temp = y_test.copy().transpose()\n",
    "# y_pred2=np.reshape(y_pred2,(y_pre.shape[0],y_pre.shape[1],n_test))\n",
    "y_test2=np.zeros([y.shape[0],y.shape[1],n_test])\n",
    "y_test2[:,0,:]=y_test_temp[:n_bus,:]\n",
    "y_test2[:,1,:]=y_test_temp[n_bus:,:]\n",
    "y_pred1 = y_pred2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "2eVE-t3nl0Cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 2, 2000) (118, 2, 2000)\n"
     ]
    }
   ],
   "source": [
    "# recover the original p.u. scale\n",
    "# vy_deviation) * vy_scale\n",
    "y_pred1[:,1,:] = y_pred1[:,1,:] / vy_scale + vy_deviation\n",
    "y_test2[:,1,:] = y_test2[:,1,:] / vy_scale + vy_deviation\n",
    "print(y_test2.shape,y_pred1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "FUVl5LXeknC4"
   },
   "outputs": [],
   "source": [
    "n_test = np.size(y_test2,2)\n",
    "err_L2 = np.zeros(n_test)\n",
    "err_Linf = np.zeros(n_test)\n",
    "for i in range(n_test):\n",
    "  err_L2[i] = np.linalg.norm(y_test2[:,0,i] - y_pred1[:,0,i]) / np.linalg.norm(y_test2[:,0,i])\n",
    "  err_Linf[i] = np.max(np.abs(y_test2[:,0,i] - y_pred1[:,0,i])) / np.max(np.abs(y_test2[:,0,i]))\n",
    "\n",
    "err_L2_v = np.zeros(n_test)\n",
    "err_Linf_v = np.zeros(n_test)\n",
    "for i in range(n_test):\n",
    "  err_L2_v[i] = np.linalg.norm(y_test2[:,1,i] - y_pred1[:,1,i]) / np.linalg.norm(y_test2[:,1,i])\n",
    "  err_Linf_v[i] = np.max(np.abs(y_test2[:,1,i] - y_pred1[:,1,i])) / np.max(np.abs(y_test2[:,1,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "79xFCVXklkLb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price L2 mean: 0.0554743482234698 L_inf mean: 0.06359566486968149\n",
      "Voltage L2 mean: 0.005736061124148106 L_inf mean: 0.017060354212589984\n"
     ]
    }
   ],
   "source": [
    "err_L2_mean = np.mean(err_L2)\n",
    "err_Linf_mean = np.mean(err_Linf)\n",
    "print('Price L2 mean:', err_L2_mean,'L_inf mean:', err_Linf_mean )\n",
    "err_L2_mean_v = np.mean(err_L2_v)\n",
    "err_Linf_mean_v = np.mean(err_Linf_v)\n",
    "print('Voltage L2 mean:', err_L2_mean_v,'L_inf mean:', err_Linf_mean_v )\n",
    "\n",
    "# fig2 = plt.figure(figsize=(16, 16))\n",
    "# plt.subplot(2, 2, 1)\n",
    "# # plt.hist(np.abs(ga),bins = 10)\n",
    "# plt.plot(err_L2,'bo',markersize=0.5,label = 'L2 error')\n",
    "# plt.plot(err_Linf,'r^',markersize=0.5,label = 'Linf error')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('sample index')\n",
    "# plt.ylabel('error')\n",
    "# plt.title('Normalized sample error')\n",
    "# plt.grid(True)\n",
    "# # error histogram\n",
    "# plt.subplot(2, 2, 2)\n",
    "# plt.hist(err_L2, bins = 50, facecolor='b', alpha=0.75,label = 'L2 error')\n",
    "# plt.hist(err_Linf, bins = 50, facecolor='g', alpha=0.75,label = 'Linf error')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('sample error value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('Histogram of L_2 error')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)\n",
    "\n",
    "# plt.subplot(2, 2, 3)\n",
    "# # plt.hist(np.abs(ga),bins = 10)\n",
    "# plt.plot(err_L2_v,'bo',markersize=0.5,label = 'L2 error')\n",
    "# plt.plot(err_Linf_v,'r^',markersize=0.5,label = 'Linf error')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('sample index')\n",
    "# plt.ylabel('error')\n",
    "# plt.title('Normalized sample error')\n",
    "# plt.grid(True)\n",
    "# # error histogram\n",
    "# plt.subplot(2, 2, 4)\n",
    "# plt.hist(err_L2_v, bins = 50, facecolor='b', alpha=0.75,label = 'L2 error')\n",
    "# plt.hist(err_Linf_v, bins = 50, facecolor='g', alpha=0.75,label = 'Linf error')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('sample error value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('Histogram of L_2 error')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "6sUddh-uGg_p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 2000) (118, 2000)\n",
      "true range: 1.06 0.94\n",
      "predicted range 1.0878542709350587 0.9313497233390808\n"
     ]
    }
   ],
   "source": [
    "print(y_pred1[:,1,:n_test].shape,y_test2[:,1,:n_test].shape)\n",
    "print('true range:',np.max(y_test2[:,1,:n_test]),np.min(y_test2[:,1,:n_test]))\n",
    "print('predicted range',np.max(y_pred1[:,1,:n_test]),np.min(y_pred1[:,1,:n_test]))\n",
    "\n",
    "# fig3 = plt.figure(figsize=(16, 8))\n",
    "# flat_list1 = list(np.concatenate(y_test2[:,1,:n_test]).flat)\n",
    "# flat_list2 = list(np.concatenate(y_pred1[:,1,:n_test]).flat)\n",
    "# plt.hist(flat_list1,bins = 100,label = 'true')\n",
    "\n",
    "# plt.hist(flat_list2,bins = 100,label = 'pred')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "jhfImaPsjKYq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 6, 10000) 10000\n"
     ]
    }
   ],
   "source": [
    "print(x.shape,n_sample)\n",
    "\n",
    "x_new = np.zeros([x.shape[0],x.shape[1],n_sample])\n",
    "for i in range(x.shape[1]):\n",
    "  x_new[:,i,:] = x_total[n_bus*i:n_bus*(i+1),:]\n",
    "\n",
    "y_new = np.zeros([y.shape[0],y.shape[1],n_sample])\n",
    "for i in range(y.shape[1]):\n",
    "  y_new[:,i,:] = y_total[n_bus*i:n_bus*(i+1),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9x7neMNj7n3"
   },
   "source": [
    "# Predict generation using $\\pi$\n",
    "* Using predicted $\\pi$ and find the active constraints in $p_G(i)$\n",
    "* For inactive $p_G(i)$ consider other methods like power flow balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Kr29K04j2KTN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 10000)\n",
      "<class 'numpy.ndarray'> 118 [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117]\n"
     ]
    }
   ],
   "source": [
    "gen_limit0 = x_new[:,4,:].copy() # lin cost\n",
    "print(gen_limit0.shape)\n",
    "\n",
    "gen_idx = []\n",
    "gen_idx = np.arange(n_bus)\n",
    "# for i in range(n_bus):\n",
    "#   if gen_limit0[i,0] > 0:\n",
    "#     gen_idx.append(i)\n",
    "print(type(gen_idx),len(gen_idx),gen_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "gHmx9lMDXzpM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 2, 10000) (236, 10000)\n"
     ]
    }
   ],
   "source": [
    "n_sample=x_total.shape[-1]\n",
    "x_feed = torch.from_numpy(x_total.T).float()\n",
    "y_pred1=net(x_feed.to(device)).cpu().detach().numpy().T\n",
    "y_pred_temp = y_pred1.copy()\n",
    "y_pred2=np.zeros([y.shape[0],y.shape[1],n_sample])\n",
    "y_pred2[:,0,:]=y_pred_temp[:n_bus,:]\n",
    "y_pred2[:,1,:]=y_pred_temp[n_bus:,:]\n",
    "print(y_pred2.shape,y_pred1.shape)\n",
    "y_pred1 = y_pred2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "GPEHF2L91bGv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0010632171630859943\n",
      "0.784000000000006\n",
      "0.3804287313435779\n",
      "0.24221453285700786\n"
     ]
    }
   ],
   "source": [
    "gen_cost0 = x_new[:,4,:].copy()\n",
    "lmp_data = y_new[:,0,:].copy()\n",
    "quadratic_a = x_new[:,5,:].copy()\n",
    "profit_pred = y_pred1[:,0,:] - gen_cost0\n",
    "print(np.min(np.abs(profit_pred)))\n",
    "profit_true = lmp_data - gen_cost0\n",
    "print(np.min(np.abs(profit_true)))\n",
    "profit_pred=(y_pred1[:,0,:]-gen_cost0)/(quadratic_a+1e-10)/2\n",
    "profit_true=(lmp_data-gen_cost0)/(quadratic_a+1e-10)/2\n",
    "print(np.min(np.abs(profit_pred)))\n",
    "print(np.min(np.abs(profit_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "uiHWtU5OLc1x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 10000) (118, 10000)\n",
      "0.04557398942388041\n"
     ]
    }
   ],
   "source": [
    "print(profit_pred.shape,profit_true.shape)\n",
    "profit_err = profit_true - profit_pred\n",
    "profit_err_l2 = np.zeros([n_sample,1])\n",
    "\n",
    "for i in range(n_sample):\n",
    "  profit_err_l2[i] = np.linalg.norm(profit_err[:,i])/np.linalg.norm(profit_true[:,i])\n",
    "print(np.mean(profit_err_l2))\n",
    "\n",
    "# fig5 = plt.figure(figsize=(16, 8))\n",
    "# # error histogram\n",
    "# plt.hist(profit_err_l2, bins = 50, facecolor='b', alpha=0.75,label = 'L2 error')\n",
    "# # plt.hist(err_linf_new, bins = 50, facecolor='g', alpha=0.75,label = 'Linf error')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('sample error value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('Histogram of L_2 error')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "ZbHchRQd_g8-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1180000,)\n",
      "-954.4922588259392 -2.9391182643149665\n"
     ]
    }
   ],
   "source": [
    "p_pred_sort = np.reshape(profit_pred,n_bus*n_sample)\n",
    "p_true_sort = np.reshape(profit_true,n_bus*n_sample)\n",
    "print(p_pred_sort.shape)\n",
    "print(np.min(p_pred_sort),np.min(p_true_sort))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "TYsSdNGp-OLP"
   },
   "outputs": [],
   "source": [
    "# fig2 = plt.figure(figsize=(8, 8))\n",
    "# plt.hist(p_pred_sort, bins = 50, facecolor='b', alpha=0.75,label = 'pred. profit')\n",
    "# plt.hist(p_true_sort, bins = 50, facecolor='g', alpha=0.75,label = 'true profit')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('profit histogram')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "1J9j5tT9p_f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2843030.1489740764 2764601.835122853\n",
      "2843030.1489740764 43260060.56350001 2843030.1489740764\n"
     ]
    }
   ],
   "source": [
    "# x = [load, gen_cost, gen_lim]\n",
    "binary_thres_true = 1e-5\n",
    "binary_thres = x_new[:,0,:].copy() # upper\n",
    "binary_thres_lo = x_new[:,1,:].copy() # lower\n",
    "gen_pred_binary_full = np.zeros((n_bus,n_sample))\n",
    "gen_true_binary_full = np.zeros((n_bus,n_sample))\n",
    "\n",
    "for i in range(n_sample):\n",
    "  for j in range(len(gen_idx)):\n",
    "    # predicted generator limit\n",
    "    if profit_pred[gen_idx[j],i] > binary_thres[gen_idx[j],i]:\n",
    "      gen_pred_binary_full[gen_idx[j],i] = binary_thres[gen_idx[j],i]\n",
    "    elif profit_pred[gen_idx[j],i] < binary_thres_lo[gen_idx[j],i]:\n",
    "      gen_pred_binary_full[gen_idx[j],i] = binary_thres_lo[gen_idx[j],i]\n",
    "    else:\n",
    "      gen_pred_binary_full[gen_idx[j],i] = profit_pred[gen_idx[j],i]\n",
    "    # true generator limit\n",
    "    if profit_true[gen_idx[j],i] > binary_thres[gen_idx[j],i]:\n",
    "      gen_true_binary_full[gen_idx[j],i] = binary_thres[gen_idx[j],i]\n",
    "    elif profit_true[gen_idx[j],i] < binary_thres_lo[gen_idx[j],i]:\n",
    "      gen_true_binary_full[gen_idx[j],i] = binary_thres_lo[gen_idx[j],i]\n",
    "    else:\n",
    "      gen_true_binary_full[gen_idx[j],i] = profit_true[gen_idx[j],i]\n",
    "\n",
    "gen_inj=gen_pred_binary_full\n",
    "gen_inj_true=gen_true_binary_full\n",
    "# nodal injection\n",
    "load0 = -x_new[:,1,:].copy() # load file\n",
    "p_inj = gen_inj #- load0\n",
    "p_inj_true = gen_inj_true #- load0\n",
    "print(np.sum(p_inj),np.sum(gen_inj_true))\n",
    "print(np.sum(p_inj),np.sum(load0),np.sum(gen_inj))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WAgqdRPjAONm"
   },
   "source": [
    "## injection accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "J46pLAw2AQor"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 10000) (118, 10000)\n",
      "mean p_inj l2 err: 0.10875750815853978\n"
     ]
    }
   ],
   "source": [
    "print(p_inj_true.shape,p_inj.shape)\n",
    "p_inj_true_sort = np.reshape(p_inj_true,n_bus*n_sample)\n",
    "p_inj_sort = np.reshape(p_inj,n_bus*n_sample)\n",
    "p_err = np.zeros(n_sample)\n",
    "for i in range(n_sample):\n",
    "  p_err[i] = np.linalg.norm(p_inj_true[:,i]-p_inj[:,i]) / np.linalg.norm(p_inj_true[:,i])\n",
    "print('mean p_inj l2 err:',np.mean(p_err))\n",
    "# fig3 = plt.figure(figsize=(16, 8))\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.hist(p_inj_sort, bins = 50, facecolor='b', alpha=0.75,label = 'pred. injection')\n",
    "# plt.hist(p_inj_true_sort, bins = 50, facecolor='g', alpha=0.75,label = 'true injection')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('injection histogram')\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.hist(p_err, bins = 50, facecolor='b', alpha=0.75,label = 'injection err')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('error histogram')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)s_binary\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DaN7u4xeGIom"
   },
   "source": [
    "* Calculate flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "YT4sgn_n79MI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(186, 1) (186, 10000) (186, 10000)\n",
      "15916 13334\n",
      "0.008556989247311828 0.007168817204301075\n",
      "186 10000 (186, 10000)\n"
     ]
    }
   ],
   "source": [
    "filename=root+'118ac_fmax.txt'\n",
    "f_max1=pd.read_table(filename,sep=',',header=None).to_numpy() # flow limit\n",
    "\n",
    "n_line = np.size(S_isf,0)\n",
    "flow_est = np.zeros((n_line,n_sample))\n",
    "flow_est0 = np.zeros((n_line,n_sample))\n",
    "\n",
    "f_binary = np.zeros((n_line,n_sample))\n",
    "f_binary0 = np.zeros((n_line,n_sample))\n",
    "\n",
    "# for i in range(n_sample):\n",
    "flow_est = np.dot(S_isf,p_inj)\n",
    "flow_est0 = np.dot(S_isf,p_inj_true)\n",
    "# f_max\n",
    "# f_max_numpy = f_max.cpu().detach().numpy()\n",
    "f_max_numpy = f_max1.copy()\n",
    "f_binary = (np.abs(flow_est)-f_max_numpy > 0)\n",
    "f_binary0 = (np.abs(flow_est0)-f_max_numpy > 0)\n",
    "\n",
    "print(f_max_numpy.shape,flow_est.shape,flow_est0.shape)\n",
    "f_tot_sample = n_line * n_sample\n",
    "print(np.sum(f_binary),np.sum(f_binary0))\n",
    "print(np.sum(f_binary)/f_tot_sample,np.sum(f_binary0)/f_tot_sample)\n",
    "print(n_line,n_sample,flow_est.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "VyvDpKhyQj0M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173.28198880162773 39.64401431411352\n",
      "1.1552132586775181 0.2642934287607568\n"
     ]
    }
   ],
   "source": [
    "# soft threshold\n",
    "f_err_est = np.abs(flow_est)-f_max_numpy\n",
    "f_err_true = np.abs(flow_est0)-f_max_numpy\n",
    "\n",
    "f_err_est = np.maximum(np.abs(flow_est)-f_max_numpy,0) # identify violations\n",
    "f_err_true = np.maximum(np.abs(flow_est0)-f_max_numpy,0)\n",
    "\n",
    "print(np.max(f_err_est),np.max(f_err_true))\n",
    "print(np.max(f_err_est/f_max_numpy),np.max(f_err_true/f_max_numpy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "7iuX7tN2a2Cp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11968 10451\n",
      "0.006434408602150538 0.005618817204301075\n"
     ]
    }
   ],
   "source": [
    "f_binary_soft = (np.abs(flow_est)-f_max_numpy > 0.1*(f_max_numpy))\n",
    "f_binary0_soft = (np.abs(flow_est0)-f_max_numpy > 0.1*(f_max_numpy))\n",
    "print(np.sum(f_binary_soft),np.sum(f_binary0_soft))\n",
    "print(np.sum(f_binary_soft)/f_tot_sample,np.sum(f_binary0_soft)/f_tot_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "SYl9pxnOUUQF"
   },
   "outputs": [],
   "source": [
    "f_pred_sort = np.reshape(f_err_est/f_max_numpy,n_line*n_sample)\n",
    "f_true_sort = np.reshape(f_err_true/f_max_numpy,n_line*n_sample)\n",
    "\n",
    "# fig2 = plt.figure(figsize=(16, 8))\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.hist(f_pred_sort, bins = 10, facecolor='b', alpha=0.75,label = 'pred. f')\n",
    "# plt.hist(f_true_sort, bins = 10, facecolor='g', alpha=0.75,label = 'true f')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('percentage')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('flow violation level histogram')\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.hist(f_true_sort, bins = 10, facecolor='g', alpha=0.75,label = 'true f')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('percentage')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('flow violation level histogram')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "YglgTpWVRLri"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sample pred: 8\n",
      "max line pred: 8377\n",
      "max sample true: 3\n",
      "max line true: 10000\n"
     ]
    }
   ],
   "source": [
    "f_line = np.sum(f_binary,0)\n",
    "f_samp = np.sum(f_binary,1)\n",
    "print('max sample pred:',np.max(f_line))\n",
    "print('max line pred:',np.max(f_samp))\n",
    "\n",
    "f_line0  = np.sum(f_binary0,0)\n",
    "f_samp0 = np.sum(f_binary0,1)\n",
    "print('max sample true:',np.max(f_line0))\n",
    "print('max line true:',np.max(f_samp0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZQnarHmPl35"
   },
   "source": [
    "# Check objective optimality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "BZjhp-CgQaDz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0792481711033575\n"
     ]
    }
   ],
   "source": [
    "gen_cost_pred = np.zeros((n_bus,n_sample))\n",
    "gen_cost_true = np.zeros((n_bus,n_sample))\n",
    "objective_err = np.zeros(n_sample)\n",
    "\n",
    "gen_cost_pred = np.multiply(np.multiply(p_inj,p_inj),quadratic_a) + np.multiply(p_inj,gen_cost0)\n",
    "gen_cost_true = np.multiply(np.multiply(p_inj_true,p_inj_true),quadratic_a) + np.multiply(p_inj_true,gen_cost0)\n",
    "\n",
    "objective_err = np.sum(np.abs(gen_cost_true-gen_cost_pred),axis=0) / np.sum(gen_cost_true,axis=0)\n",
    "print(np.mean(objective_err))\n",
    "\n",
    "# fig6 = plt.figure(figsize=(16, 8))\n",
    "# # error histogram\n",
    "# plt.hist(objective_err, bins = 50, facecolor='b', alpha=0.75,label = 'L2 error')\n",
    "# # plt.hist(err_linf_new, bins = 50, facecolor='g', alpha=0.75,label = 'Linf error')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('sample error value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('Histogram of L_2 error')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdrsAWpYo0-w"
   },
   "source": [
    "## injection accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "mArjSN-So0-x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 10000) (118, 10000)\n",
      "mean p_inj l2 err: 0.10875750815853978\n"
     ]
    }
   ],
   "source": [
    "print(p_inj_true.shape,p_inj.shape)\n",
    "\n",
    "p_inj_true_sort = np.reshape(p_inj_true,n_bus*n_sample)\n",
    "p_inj_sort = np.reshape(p_inj,n_bus*n_sample)\n",
    "\n",
    "p_err = np.zeros(n_sample)\n",
    "for i in range(n_sample):\n",
    "  p_err[i] = np.linalg.norm(p_inj_true[:,i]-p_inj[:,i]) / np.linalg.norm(p_inj_true[:,i])\n",
    "\n",
    "print('mean p_inj l2 err:',np.mean(p_err))\n",
    "# fig3 = plt.figure(figsize=(16, 8))\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.hist(p_inj_sort, bins = 50, facecolor='b', alpha=0.75,label = 'pred. injection')\n",
    "# plt.hist(p_inj_true_sort, bins = 50, facecolor='g', alpha=0.75,label = 'true injection')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('injection histogram')s_binary\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.hist(p_err, bins = 50, facecolor='b', alpha=0.75,label = 'injection err')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('value')\n",
    "# plt.ylabel('frequency')\n",
    "# plt.title('error histogram')\n",
    "# # plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujU84tOSpqsy"
   },
   "source": [
    "# Test AC feasibility\n",
    "* P in actual value, V in p.u.\n",
    "* Use P to recover $\\theta$, or solve $\\theta$ and Q for PF\n",
    "$$ Q_m = V_m \\sum_{n=1}^N V_n \\left(G_{mn}\\sin\\theta_{mn} - B_{mn}\\cos\\theta_{mn} \\right) $$\n",
    "calculate $Q_{mn}$ directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "Of6mEXF4puDN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 118) (118, 118)\n",
      "(117, 117) (118, 10000) (118, 10000)\n",
      "(118, 10000) (118, 10000)\n"
     ]
    }
   ],
   "source": [
    "# Bbus and B_r inverse\n",
    "filename1 = root+'ieee118_Bbus.txt'\n",
    "Bbus=pd.read_table(filename1,sep=',',header=None).to_numpy()\n",
    "B_r = np.delete(Bbus,68,axis=0)\n",
    "B_r = np.delete(B_r,68,axis=1)\n",
    "Br_inv = np.linalg.inv(B_r)\n",
    "\n",
    "# Y = G + jB\n",
    "filename1 = root+'ieee118_Gmat.txt'\n",
    "G_mat=pd.read_table(filename1,sep=',',header=None).to_numpy()\n",
    "filename1 = root+'ieee118_Bmat.txt'\n",
    "B_mat=pd.read_table(filename1,sep=',',header=None).to_numpy()\n",
    "print(G_mat.shape,B_mat.shape)\n",
    "\n",
    "# line parameters\n",
    "filename1 = root+'ieee118_lineloc.txt'\n",
    "line_loc = pd.read_table(filename1,sep=',',header=None).to_numpy()\n",
    "\n",
    "# load line params\n",
    "filename1 = root+'ieee118_lineparams.txt'\n",
    "line_params = pd.read_table(filename1,sep=',',header=None).to_numpy()\n",
    "R_line = line_params[:,0].copy()\n",
    "X_line = line_params[:,1].copy()\n",
    "B_shunt = line_params[:,2].copy()\n",
    "Z_line = R_line + 1j * X_line \n",
    "Y_line = 1 / Z_line\n",
    "G_line = np.real(Y_line)\n",
    "B_line = np.imag(Y_line)\n",
    "# P_inj w/out reference bus in p.u.\n",
    "p_inj_r = np.delete(p_inj,68,axis=0) / 100\n",
    "p_inj_true_r = np.delete(p_inj_true,68,axis=0) / 100\n",
    "p_inj_pu = p_inj / 100\n",
    "p_inj_true_pu = p_inj_true / 100\n",
    "print(Br_inv.shape,p_inj.shape,p_inj_true.shape)#p_inj_true\n",
    "\n",
    "theta0 = np.matmul(Br_inv,p_inj_r)\n",
    "theta_true0 = np.matmul(Br_inv,p_inj_true_r)\n",
    "theta = np.insert(theta0,68,0,axis = 0)\n",
    "theta_true = np.insert(theta_true0,68,0,axis = 0)\n",
    "print(theta.shape,theta_true.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5078442705631396 -0.9885800086286192\n",
      "2.7803011534120627 -9.166735486002148\n"
     ]
    }
   ],
   "source": [
    "print(np.max(theta),np.min(theta))\n",
    "math.sin(math.pi/6)\n",
    "print(G_line[0],B_line[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(186, 10000)\n",
      "1.0931943511962892 0.930349338054657 (118, 10000)\n"
     ]
    }
   ],
   "source": [
    "# Calculate real and reactive flow\n",
    "f_p = np.zeros((n_line,n_sample))\n",
    "f_q = np.zeros((n_line,n_sample))\n",
    "fji_p = np.zeros((n_line,n_sample))\n",
    "fji_q = np.zeros((n_line,n_sample))\n",
    "print(f_q.shape)\n",
    "\n",
    "v_pred = y_pred1[:,1,:].copy()\n",
    "v_pred = v_pred / vy_scale + vy_deviation\n",
    "print(np.max(v_pred),np.min(v_pred),v_pred.shape)\n",
    "\n",
    "theta1 = theta[line_loc[:,0]-1,:]\n",
    "theta2 = theta[line_loc[:,1]-1,:]\n",
    "V1 = v_pred[line_loc[:,0]-1,:]\n",
    "V2 = v_pred[line_loc[:,1]-1,:] \n",
    "f_p=(a*G_line*(V1*V1).T)-a*((V1*V2).T)*(G_line*np.cos(theta1-theta2).T+B_line*np.sin(theta1-theta2).T)\n",
    "f_p=f_p.T\n",
    "f_q=-a*(V1.T)*(a*V1.T)*(B_line+B_shunt/2)+a*((V1*V2).T)*(B_line*np.cos(theta1-theta2).T-G_line*np.sin(theta1-theta2).T)\n",
    "f_q=f_q.T\n",
    "\n",
    "theta1 = theta[line_loc[:,1]-1,:]\n",
    "theta2 = theta[line_loc[:,0]-1,:]\n",
    "V1 = v_pred[line_loc[:,1]-1,:]\n",
    "V2 = v_pred[line_loc[:,0]-1,:]\n",
    "fji_p=(a*G_line*(V1*V1).T)-a*((V1*V2).T)*(G_line*np.cos(theta1-theta2).T+B_line*np.sin(theta1-theta2).T)\n",
    "fji_p=fji_p.T\n",
    "fji_q=-a*(V1.T)*(a*V1.T)*(B_line+B_shunt/2)+a*((V1*V2).T)*(B_line*np.cos(theta1-theta2).T-G_line*np.sin(theta1-theta2).T)\n",
    "fji_q=fji_q.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "gKfcrbTSVMeO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0322166633308605 -1.4095107013721986\n",
      "16.165897399103642 151.0\n"
     ]
    }
   ],
   "source": [
    "s_pred = np.sqrt(f_p*f_p+f_q*f_q)*100\n",
    "sji_pred = np.sqrt(fji_p*fji_p+fji_q*fji_q)*100\n",
    "print(np.max(f_q),np.min(f_q))\n",
    "flow_est.shape\n",
    "print(np.mean(s_pred[0,:]),np.mean(f_max_numpy[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "YO4__LJ2brLu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21083\n",
      "hard violation rate: 0.011334946236559139\n",
      "13730\n",
      "0.0073817204301075265\n"
     ]
    }
   ],
   "source": [
    "sij_binary = (np.abs(s_pred)-f_max_numpy[:n_line] > 0)\n",
    "sji_binary = (np.abs(sji_pred)-f_max_numpy[:n_line] > 0)\n",
    "s_binary = np.maximum(sij_binary,sji_binary)\n",
    "print(np.sum(s_binary))#,np.sum(f_binary0))\n",
    "print('hard violation rate:',np.sum(s_binary)/n_sample/n_line)#,np.sum(f_binary0)/f_tot_sample)\n",
    "s_binary_soft = (np.abs(s_pred)-f_max_numpy[:n_line] > 0.1*(f_max_numpy[:n_line]))\n",
    "print(np.sum(s_binary_soft))#,np.sum(f_binary0_soft))\n",
    "print(np.sum(s_binary_soft)/n_sample/n_line)#,np.sum(f_binary0_soft)/f_tot_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "Ty0WpBdfJwMR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S violation level:\n",
      "hard: 0.01133494623655914\n",
      "mean: 0.002920956607543062\n",
      "median: 0.0\n",
      "max: 1.2475662699100194\n",
      "std: 0.03624520866911846\n",
      "p99: 0.02891875588580139\n",
      "f violation level:\n",
      "hard: 0.008556989247311828 0.007168817204301075\n",
      "mean: 0.0026573611462077547\n",
      "median: 0.0\n",
      "max: 1.1552132586775181\n",
      "std: 0.03646321535888234\n",
      "p99: 0.0\n"
     ]
    }
   ],
   "source": [
    "# violation level\n",
    "sij_violation = np.abs(s_pred)-f_max_numpy[:n_line] #/ f_max_numpy\n",
    "sij_violation_level = np.maximum(sij_violation,0)\n",
    "sji_violation = np.abs(sji_pred)-f_max_numpy[:n_line] #/ f_max_numpy\n",
    "sji_violation_level = np.maximum(sji_violation,0)\n",
    "s_violation_level = np.maximum(sij_violation_level,sji_violation_level)\n",
    "s_violation_level = np.divide(s_violation_level,f_max_numpy[:n_line])\n",
    "s_vio_lvl = np.reshape(s_violation_level,n_line*n_sample)\n",
    "\n",
    "print('S violation level:')\n",
    "print('hard:',np.sum(s_binary)/f_tot_sample)\n",
    "print('mean:',np.mean(s_vio_lvl))\n",
    "print('median:',np.median(s_vio_lvl))\n",
    "print('max:',np.max(s_vio_lvl))\n",
    "print('std:',np.std(s_vio_lvl))\n",
    "print('p99:',np.percentile(s_vio_lvl,99))\n",
    "\n",
    "f_violation = np.abs(flow_est)-f_max_numpy #/ f_max_numpy\n",
    "f_violation_level = np.maximum(f_violation,0)\n",
    "f_violation_level = np.divide(f_violation_level,f_max_numpy)\n",
    "f_vio_lvl = np.reshape(f_violation_level,n_line*n_sample)\n",
    "\n",
    "print('f violation level:')\n",
    "print('hard:',np.sum(f_binary)/f_tot_sample,np.sum(f_binary0)/f_tot_sample)\n",
    "print('mean:',np.mean(f_vio_lvl))\n",
    "print('median:',np.median(f_vio_lvl))\n",
    "print('max:',np.max(f_vio_lvl))\n",
    "print('std:',np.std(f_vio_lvl))\n",
    "print('p99:',np.percentile(f_vio_lvl,99))\n",
    "\n",
    "# fig4 = plt.figure(figsize=(6,4))\n",
    "# plt.hist(s_vio_lvl, bins = 50, facecolor='b', alpha=0.75,label = 's violation')\n",
    "# plt.hist(f_vio_lvl, bins = 50, facecolor='r', alpha=0.75,label = 'f violation')\n",
    "# plt.legend(loc=\"upper right\")\n",
    "# plt.xlabel('violation level')\n",
    "# plt.ylabel('frequency')\n",
    "# # plt.title('injection histogram')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "kWXEpj-ryjbJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price L2 mean: 0.0554743482234698 L_inf mean: 0.06359566486968149\n",
      "std: 0.03836081710264243\n",
      "Voltage L2 mean: 0.005736061124148106 L_inf mean: 0.017060354212589984\n",
      "std: 0.00345892664322895\n"
     ]
    }
   ],
   "source": [
    "# err_L2_mean = np.mean(err_L2)\n",
    "# err_Linf_mean = np.mean(err_Linf)\n",
    "print('Price L2 mean:', err_L2_mean,'L_inf mean:', err_Linf_mean )\n",
    "print('std:',np.std(err_L2))\n",
    "# err_L2_mean_v = np.mean(err_L2_v)\n",
    "# err_Linf_mean_v = np.mean(err_Linf_v)\n",
    "print('Voltage L2 mean:', err_L2_mean_v,'L_inf mean:', err_Linf_mean_v )\n",
    "print('std:',np.std(err_L2_v))\n",
    "\n",
    "params = (sum(temp.numel() for temp in net.parameters() if temp.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "輸出內容已儲存到 C:\\Users\\USER\\Desktop\\GNN_OPF_electricity_market-main\\output_loss_fig/118ac_output_02181120.txt\n"
     ]
    }
   ],
   "source": [
    "import os   \n",
    "# 建立輸出內容字串\n",
    "output_content = f\"\"\"\n",
    "date: {timestamp}\n",
    "final_epoch: {final_epoch}\n",
    "training time: {t1 - t0:.4e}s\n",
    "\n",
    "factor: {factor}\n",
    "patience: {patience}\n",
    "\n",
    "start learning rate: {lr:.4e}\n",
    "batch size: {batch_size}\n",
    "params number: {params:.4e}\n",
    "\n",
    "Price L2 mean: {err_L2_mean:.4e} \n",
    "Price std: {np.std(err_L2):.4e}\n",
    "\n",
    "Voltage L2 mean: {err_L2_mean_v:.4e} \n",
    "Voltage std: {np.std(err_L2_v):.4e}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# 設定儲存路徑和檔名\n",
    "output_dir = r'C:\\Users\\USER\\Desktop\\GNN_OPF_electricity_market-main\\output_loss_fig'\n",
    "timestamp = datetime.now().strftime('%m%d%H%M')\n",
    "output_file = f'{output_dir}/118ac_output_{timestamp}.txt'\n",
    "\n",
    "# 確保目錄存在\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 將內容寫入 txt 檔案\n",
    "with open(output_file, 'w') as file:\n",
    "    file.write(output_content)\n",
    "\n",
    "print(f\"輸出內容已儲存到 {output_file}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "118ac_feasdnn0417.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "AC_OPF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
